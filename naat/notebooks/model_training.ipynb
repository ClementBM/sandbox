{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-27 16:31:12.806466: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-27 16:31:13.508808: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64${LD_LIBRARY_PATH:+:}\n",
      "2023-02-27 16:31:13.508903: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64${LD_LIBRARY_PATH:+:}\n",
      "2023-02-27 16:31:13.508910: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertForMaskedLM,\n",
    "    BertModel,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    RobertaTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from naat.data import list_files, OUTPUT_PATH, get_file_languages\n",
    "from naat.data_files import ROOT\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LEN = 512  # maximum length of embeddings\n",
    "TOKENIZER_FILES = None  # files of raw text\n",
    "VOCAB_SIZE = 32000  # vocabulary size for tokenizer\n",
    "MIN_FREQ = 2  # minimum term frequency for the vocabulary\n",
    "\n",
    "MODEL_PATH = None  # path to save the pretrained model\n",
    "\n",
    "HIDDEN_LAYERS = 12  # number of hidden layers of BERT model\n",
    "HIDDEN_SIZE = 768  # hidden size of BERT model\n",
    "ATTENTION_HEADS = 12  # number of attention heads of BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegalDataset(Dataset):\n",
    "    def __init__(self, text):\n",
    "        self.encodings = text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = {\"input_ids\": torch.tensor(self.encodings.iloc[index])}\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(files, vocab_size, min_freq, max_len, save_path: Path):\n",
    "    tokenizer = ByteLevelBPETokenizer()\n",
    "    tokenizer.train(\n",
    "        files=files,\n",
    "        vocab_size=vocab_size,\n",
    "        min_frequency=min_freq,\n",
    "        special_tokens=[\n",
    "            \"<s>\",\n",
    "            \"<pad>\",\n",
    "            \"</s>\",\n",
    "            \"<unk>\",\n",
    "            \"<mask>\",\n",
    "        ],\n",
    "    )\n",
    "    tokenizer.save_model(str(save_path))\n",
    "    tokenizer = ByteLevelBPETokenizer(\n",
    "        str(save_path / \"vocab.json\"),\n",
    "        str(save_path / \"merges.txt\"),\n",
    "    )\n",
    "    tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "        (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "        (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "    )\n",
    "    tokenizer.enable_truncation(max_length=max_len)\n",
    "\n",
    "    tokenizer.save(str(save_path / \"tokenizer.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(filenames, map_tokenize, encoding):\n",
    "    texts = []\n",
    "    for filename in filenames:\n",
    "        with open(filename, \"r\", encoding=encoding) as file:\n",
    "            text = file.read()\n",
    "            \n",
    "        corpus = PlaintextCorpusReader(filename.parent, filename.name)\n",
    "        texts += corpus.sents()\n",
    "\n",
    "    texts = pd.Series(texts)\n",
    "    tqdm.pandas(desc=\"Tokenizing\")\n",
    "    texts = texts.progress_map(map_tokenize)\n",
    "    dataset = LegalDataset(texts)\n",
    "    texts = None\n",
    "    \n",
    "    torch.save(dataset, \"dataset.pt\")\n",
    "    return \"dataset.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_files = list(list_files(str(OUTPUT_PATH), verbose=False, extension=\"*\"))\n",
    "fr_text_files = []\n",
    "\n",
    "for text_file in text_files:\n",
    "    language = get_file_languages(text_file)\n",
    "    if language == \"fr\":\n",
    "        fr_text_files.append(text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Calendrier des newsletters - Affaires Climatiques_.txt'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toto.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Newsletter', '15', '/', '01', '/', '2019', 'CALENDRIER', 'DES', 'NEWSLETTERS', 'ET', 'THEMATIQUES', 'ABORDEES', 'AVANT', 'TOUT', ':', 'à', 'qui', 'on', 'a', 'envie', 'd', '’', 'envoyer', 'cette', 'newsletter', '?'], ['Etablir', 'une', 'liste', 'de', 'contacts', 'qui', 'pourraient', 'être', 'intéressé', '-', 'es', ':', 'groupe', 'juristes', '+', 'académiques', '+', 'qui', 'd', '’', 'autre', '?'], ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toto = fr_text_files[0]\n",
    "coco = PlaintextCorpusReader(str(toto.parent), toto.name)\n",
    "coco.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tokenizer(\n",
    "    [str(text_file) for text_file in fr_text_files], VOCAB_SIZE, MIN_FREQ, SEQUENCE_LEN, ROOT.parent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(str(ROOT.parent), max_len=SEQUENCE_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_tokenize(text):\n",
    "    return tokenizer.encode(text, max_length=SEQUENCE_LEN, truncation=True)\n",
    "\n",
    "\n",
    "dataset_path = process_text(fr_text_files, map_tokenize, \"utf8\")\n",
    "dataset = torch.load(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_prob = 0.15 # mlm masking probability\n",
    "# Create Masked Language Model\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=mlm_prob\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config = BertConfig(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    max_position_embeddings=SEQUENCE_LEN,\n",
    "    num_hidden_layers=HIDDEN_LAYERS,  # L\n",
    "    hidden_size=HIDDEN_SIZE,  # H\n",
    "    num_attention_heads=ATTENTION_HEADS,  # A\n",
    "    type_vocab_size=1,\n",
    ")\n",
    "model = BertForMaskedLM(config=bert_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=str(ROOT.parent / \"checkpoint\"),\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=5,\n",
    "    prediction_loss_only=True,\n",
    "    max_steps=0, # number of training steps, overwrites epochs\n",
    "    learning_rate=1e-4,\n",
    "    adam_beta1=0.9, # adam beta1 parameter\n",
    "    adam_beta2=0.99, # adam beta2 parameter\n",
    "    weight_decay=0.01, # weight decay\n",
    "    lr_scheduler_type=\"linear\", # learning rate scheduler type\n",
    "    warmup_steps=10_000, # warmup steps\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "# Save model\n",
    "trainer.save_model(MODEL_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe15505adf06195d2a047b121845922c926e1a5f5ad0a100fa42450eb4a147fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
