{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.classic import connect_four_v3\n",
    "import ray.rllib.algorithms.ppo as ppo\n",
    "from ray.rllib.algorithms.pg import (\n",
    "    PG,\n",
    "    PGConfig,\n",
    ")\n",
    "import ray\n",
    "from ray import tune, air\n",
    "from ray.tune import CLIReporter, register_env\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from bnbot.wrappers.connect4wrapper import Connect4Env\n",
    "from bnbot.models.connect4model import Connect4MaskModel\n",
    "from bnbot.models.connect4model import SacConnect4MaskModel\n",
    "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "import imageio\n",
    "import numpy as np\n",
    "import sys\n",
    "from ray.rllib.algorithms import sac\n",
    "import random\n",
    "from ray.rllib.algorithms.algorithm_config import AlgorithmConfig\n",
    "from bnbot.policies.dummy_policy import AlwaysSameHeuristic, BeatLastHeuristic, RandomHeuristic\n",
    "\n",
    "torch, nn = try_import_torch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()\n",
    "# ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define how to make the environment. This way takes an optional environment config, num_floors\n",
    "env_creator = lambda config: connect_four_v3.env(render_mode=\"rgb_array\")\n",
    "\n",
    "# register that way to make the environment under an rllib name\n",
    "register_env(\"connect4\", lambda config: Connect4Env(env_creator(config)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_add_policy_cb(policy_id, checkpoint):\n",
    "    class AddPolicyCallback(DefaultCallbacks):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def on_algorithm_init(self, *, algorithm, **kwargs):\n",
    "            policy = Policy.from_checkpoint(checkpoint)\n",
    "\n",
    "            # Add restored policy to trainer.\n",
    "            # Note that this policy doesn't have to be trained with the same algorithm\n",
    "            # of the training stack. You can even mix up TF policies with a Torch stack.\n",
    "            algorithm.add_policy(\n",
    "                policy_id=\"opponent\",\n",
    "                policy=policy[policy_id],\n",
    "                evaluation_workers=True,\n",
    "            )\n",
    "    return AddPolicyCallback\n",
    "\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "    # main policy plays against opponent policy.\n",
    "    return \"main\" if episode.episode_id % 2 == int(agent_id[-1:]) else \"opponent\"\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_user_for_action(obs, player_id):\n",
    "    \"\"\"\n",
    "    Asks the user for a valid action on the command line and returns it.\n",
    "    \"\"\"\n",
    "    legal_moves = obs[player_id][\"action_mask\"]\n",
    "    legal_moves = np.arange(7)[legal_moves == 1]\n",
    "\n",
    "    choice = -1\n",
    "    while choice not in legal_moves:\n",
    "        # print(\"Choose an action from {}:\".format(legal_moves))\n",
    "        sys.stdout.flush()\n",
    "        choice_str = input()\n",
    "        try:\n",
    "            choice = int(choice_str)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return choice"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = (\n",
    "    ppo.PPOConfig()\n",
    "    .environment(\"connect4\")\n",
    "    .framework(\"torch\")\n",
    "    .training(model={\"custom_model\": Connect4MaskModel})\n",
    "    .multi_agent(\n",
    "        policies={\"policy_0\", \"policy_1\"},\n",
    "        policy_mapping_fn=(\n",
    "            lambda agent_id, episode, worker, **kw: (\n",
    "                \"policy_0\" if agent_id == \"player_0\" else \"policy_1\"\n",
    "            )\n",
    "        ),\n",
    "        policies_to_train=[\n",
    "            \"policy_0\",\n",
    "            \"policy_1\",\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "\n",
    "stop = {\n",
    "    \"timesteps_total\": 100000,\n",
    "    \"training_iteration\": 10,\n",
    "}\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    \"PPO\",\n",
    "    param_space=config.to_dict(),\n",
    "    run_config=air.RunConfig(stop=stop),\n",
    ")\n",
    "\n",
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result = results.get_best_result()\n",
    "algo = Algorithm.from_checkpoint(best_result.checkpoint)\n",
    "algo.restore(best_result.checkpoint)\n",
    "# Perform inference (action computations) based on given env observations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PG Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = (\n",
    "    PGConfig()\n",
    "    .environment(\"connect4\")\n",
    "    .framework(\"torch\")\n",
    "    .training(model={\"custom_model\": Connect4MaskModel})\n",
    "    .multi_agent(\n",
    "        policies={\"policy_0\", \"policy_1\"},\n",
    "        policy_mapping_fn=(\n",
    "            lambda agent_id, episode, worker, **kw: (\n",
    "                \"policy_0\" if agent_id == \"player_0\" else \"policy_1\"\n",
    "            )\n",
    "        ),\n",
    "        policies_to_train=[\n",
    "            \"policy_0\",\n",
    "            \"policy_1\",\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "\n",
    "stop = {\n",
    "    \"timesteps_total\": 100000,\n",
    "    \"training_iteration\": 20,\n",
    "}\n",
    "\n",
    "results = tune.Tuner(\n",
    "    \"PG\", \n",
    "    param_space=config, \n",
    "    run_config=air.RunConfig(stop=stop)\n",
    ").fit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heuristic and Random policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfPlayCallback(DefaultCallbacks):\n",
    "    win_rate_threshold = 0.95\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 0=RandomPolicy\n",
    "        # always_same\n",
    "        # beat_last\n",
    "        # 3=1st learned policy snapshot,\n",
    "        # 4=2nd learned policy snapshot, etc..\n",
    "        self.current_opponent = 0\n",
    "\n",
    "    def on_train_result(self, *, algorithm, result, **kwargs):\n",
    "        # Get the win rate for the train batch.\n",
    "        # Note that normally, one should set up a proper evaluation config,\n",
    "        # such that evaluation always happens on the already updated policy,\n",
    "        # instead of on the already used train_batch.\n",
    "        main_rew = result[\"hist_stats\"].pop(\"policy_learned_reward\")\n",
    "        opponent_rew = result[\"hist_stats\"].pop(\"episode_reward\")\n",
    "        \n",
    "        if len(main_rew) != len(opponent_rew):\n",
    "            raise Exception(\"len(main_rew) != len(opponent_rew)\", len(main_rew), len(opponent_rew), result[\"hist_stats\"].keys(), \"episode len\", len(opponent_rew))\n",
    "        \n",
    "        won = 0\n",
    "        for r_main, r_opponent in zip(main_rew, opponent_rew):\n",
    "            if r_main > r_opponent:\n",
    "                won += 1\n",
    "        win_rate = won / len(main_rew)\n",
    "        \n",
    "        result[\"win_rate\"] = win_rate\n",
    "        print(f\"Iter={algorithm.iteration} win-rate={win_rate} -> \", end=\"\")\n",
    "\n",
    "        # If win rate is good -> Snapshot current policy and play against\n",
    "        # it next, keeping the snapshot fixed and only improving the \"learned\"\n",
    "        # policy.\n",
    "        if win_rate > self.win_rate_threshold:            \n",
    "            self.current_opponent += 1\n",
    "            new_pol_id = f\"learned_v{self.current_opponent}\"\n",
    "            print(f\"Iter={algorithm.iteration} ### Adding new opponent to the mix ({new_pol_id}).\")\n",
    "\n",
    "            # Re-define the mapping function, such that \"learned\" is forced\n",
    "            # to play against any of the previously played policies\n",
    "            # (excluding \"random\").\n",
    "            def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "                # agent_id = [0|1] -> policy depends on episode ID\n",
    "                # This way, we make sure that both policies sometimes play\n",
    "                # (start player) and sometimes agent1 (player to move 2nd).\n",
    "                return (\n",
    "                    \"learned\"\n",
    "                    if episode.episode_id % 2 == int(agent_id[-1:])\n",
    "                    else random.choice([\"always_same\", \"beat_last\", \"random\"] + [f\"learned_v{i}\" for i in range(1, self.current_opponent + 1)])\n",
    "                )\n",
    "\n",
    "            new_policy = algorithm.add_policy(\n",
    "                policy_id=new_pol_id,\n",
    "                policy_cls=type(algorithm.get_policy(\"learned\")),\n",
    "                policy_mapping_fn=policy_mapping_fn,\n",
    "            )\n",
    "\n",
    "            # Set the weights of the new policy to the learned policy.\n",
    "            # We'll keep training the learned policy, whereas `new_pol_id` will\n",
    "            # remain fixed.\n",
    "            learned_state = algorithm.get_policy(\"learned\").get_state()\n",
    "            new_policy.set_state(learned_state)\n",
    "            # We need to sync the just copied local weights (from learned policy)\n",
    "            # to all the remote workers as well.\n",
    "            algorithm.workers.sync_weights()\n",
    "        else:\n",
    "            print(\"not good enough; will keep learning ...\")\n",
    "\n",
    "        # +4 = learned + random + ...\n",
    "        result[\"league_size\"] = self.current_opponent + 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_policy(agent_id, episode, **kwargs):\n",
    "    if episode.episode_id % 2 == int(agent_id[-1:]):\n",
    "        return \"learned\"\n",
    "    else:\n",
    "        return random.choice([\"always_same\", \"beat_last\", \"random\"])\n",
    "\n",
    "config = (\n",
    "    ppo.PPOConfig()\n",
    "    .environment(\"connect4\")\n",
    "    .framework(\"torch\")\n",
    "    .training(model={\"custom_model\": Connect4MaskModel})\n",
    "    .callbacks(SelfPlayCallback)\n",
    "    .rollouts(\n",
    "        num_rollout_workers=0,\n",
    "        num_envs_per_worker=4,\n",
    "    )\n",
    "    .multi_agent(\n",
    "        policies={\n",
    "            \"learned\": PolicySpec(),\n",
    "            \"always_same\": PolicySpec(policy_class=AlwaysSameHeuristic),\n",
    "            \"beat_last\": PolicySpec(policy_class=BeatLastHeuristic),\n",
    "            \"random\": PolicySpec(policy_class=RandomHeuristic),\n",
    "        },\n",
    "        policy_mapping_fn=select_policy,\n",
    "        policies_to_train=[\"learned\"],\n",
    "    )\n",
    "    .resources(num_gpus=1, num_cpus_per_worker=6)\n",
    ")\n",
    "\n",
    "stop = {\n",
    "    \"timesteps_total\": 10000000,\n",
    "    \"training_iteration\": 200,\n",
    "}\n",
    "\n",
    "results = tune.Tuner(\n",
    "    \"PPO\",\n",
    "    param_space=config,\n",
    "    run_config=air.RunConfig(\n",
    "        stop=stop,\n",
    "        verbose=2,\n",
    "        progress_reporter=CLIReporter(\n",
    "            metric_columns={\n",
    "                \"training_iteration\": \"iter\",\n",
    "                \"time_total_s\": \"time_total_s\",\n",
    "                \"timesteps_total\": \"ts\",\n",
    "                \"episodes_this_iter\": \"train_episodes\",\n",
    "                \"policy_reward_mean/learned\": \"reward\",\n",
    "                \"win_rate\": \"win_rate\",\n",
    "                \"league_size\": \"league_size\",\n",
    "            },\n",
    "            sort_by_metric=True,\n",
    "        ),\n",
    "        checkpoint_config=air.CheckpointConfig(\n",
    "            checkpoint_at_end=True,\n",
    "            checkpoint_frequency=10,\n",
    "        ),\n",
    "    ),\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result = results.get_best_result()\n",
    "best_result.checkpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a video record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = connect_four_v3.env(render_mode=\"rgb_array\")\n",
    "env = Connect4Env(env)\n",
    "\n",
    "done = False\n",
    "obs, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"testtesttest.mp4\"\n",
    "with imageio.get_writer(filename, fps=30) as video:\n",
    "    while not done:\n",
    "        player_id = list(obs.keys())[0]\n",
    "        action = algo.compute_single_action(obs[player_id], policy_id=\"policy_0\" if player_id == \"player_0\" else \"policy_1\")\n",
    "        \n",
    "        if action not in np.arange(7)[obs[player_id][\"action_mask\"] == 1]:\n",
    "            action = np.random.choice(7)\n",
    "\n",
    "        player_actions = {player_id: action}\n",
    "        \n",
    "        obs, rew, terminated, truncated, info = env.step(player_actions)\n",
    "        done = terminated[\"__all__\"] or truncated[\"__all__\"]\n",
    "        video.append_data(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AddPolicyCallback = create_add_policy_cb(\"learned_v5\", results.get_best_result().checkpoint)\n",
    "\n",
    "config = (\n",
    "    ppo.PPOConfig()\n",
    "    .environment(\"connect4\")\n",
    "    .framework(\"torch\")\n",
    "    .callbacks(AddPolicyCallback)\n",
    "    .training(model={\"custom_model\": Connect4MaskModel})\n",
    "    .multi_agent(\n",
    "        # Initial policy map: Random and PPO. This will be expanded\n",
    "        # to more policy snapshots taken from \"main\" against which \"main\"\n",
    "        # will then play (instead of \"random\"). This is done in the\n",
    "        # custom callback defined above (`SelfPlayCallback`).\n",
    "        # Note: We will add the \"opponent\" policy with callback.\n",
    "        policies={\"main\"},  # Our main policy, we'd like to optimize.\n",
    "        # Assign agent 0 and 1 randomly to the \"main\" policy or\n",
    "        # to the opponent (\"random\" at first). Make sure (via episode_id)\n",
    "        # that \"main\" always plays against \"random\" (and not against\n",
    "        # another \"main\").\n",
    "        policy_mapping_fn=policy_mapping_fn,\n",
    "        # Always just train the \"main\" policy.\n",
    "        policies_to_train=[\"main\"],\n",
    "    )\n",
    ")\n",
    "stop = {\n",
    "    \"timesteps_total\": 10000000,\n",
    "    \"training_iteration\": 50,\n",
    "}\n",
    "self_results = tune.Tuner(\n",
    "    \"PPO\",\n",
    "    param_space=config.to_dict(),\n",
    "    run_config=air.RunConfig(\n",
    "        stop=stop,\n",
    "        verbose=2,\n",
    "        progress_reporter=CLIReporter(\n",
    "            metric_columns={\n",
    "                \"training_iteration\": \"iter\",\n",
    "                \"time_total_s\": \"time_total_s\",\n",
    "                \"timesteps_total\": \"ts\",\n",
    "                \"episodes_this_iter\": \"train_episodes\",\n",
    "                \"policy_reward_mean/main\": \"reward\",\n",
    "                \"win_rate\": \"win_rate\",\n",
    "                \"league_size\": \"league_size\",\n",
    "            },\n",
    "            sort_by_metric=True,\n",
    "        ),\n",
    "        checkpoint_config=air.CheckpointConfig(\n",
    "            checkpoint_at_end=True,\n",
    "            checkpoint_frequency=10,\n",
    "        ),\n",
    "    ),\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_self_result = results.get_best_result()\n",
    "algo = Algorithm.from_checkpoint(best_self_result.checkpoint)\n",
    "algo.restore(best_self_result.checkpoint)\n",
    "# Perform inference (action computations) based on given env observations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human vs Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore trained trainer (set to non-explore behavior) and play against\n",
    "# human on command line.\n",
    "best_result_checkpt = results.get_best_result().checkpoint\n",
    "config.explore = False\n",
    "algo = config.build()\n",
    "algo.restore(best_result_checkpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = connect_four_v3.env(render_mode=\"human\")\n",
    "env = Connect4Env(env)\n",
    "\n",
    "done = False\n",
    "obs, info = env.reset()\n",
    "\n",
    "human_player = \"player_1\"\n",
    "\n",
    "print(\"You play as {}\".format(\"o\" if human_player else \"x\"))\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while not done:\n",
    "    player_id = list(obs.keys())[0]\n",
    "    if player_id == human_player:\n",
    "        action = ask_user_for_action(obs, player_id)\n",
    "    else:\n",
    "        action = algo.compute_single_action(obs[player_id], policy_id=\"main\")\n",
    "        \n",
    "        legal_moves = obs[player_id][\"action_mask\"]\n",
    "        if action not in np.arange(7)[legal_moves == 1]:\n",
    "            action = np.random.choice(7)\n",
    "\n",
    "    player_actions = {player_id: action}\n",
    "    \n",
    "    obs, rew, terminated, truncated, info = env.step(player_actions)\n",
    "    done = terminated[\"__all__\"] or truncated[\"__all__\"]\n",
    "\n",
    "print(\"End of game!\")\n",
    "if rew[human_player] > 0:\n",
    "    print(\"You win\")\n",
    "elif rew[human_player] < 0:\n",
    "    print(\"You lose\")\n",
    "else:\n",
    "    print(\"Draw\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAC algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ray.rllib.algorithms.sac.sac_torch_model import SACTorchModel\n",
    "from gymnasium.spaces import Dict\n",
    "from ray.rllib.models.torch.fcnet import FullyConnectedNetwork as TorchFC\n",
    "from ray.rllib.utils import override\n",
    "from ray.rllib.utils.torch_utils import FLOAT_MIN\n",
    "\n",
    "class SacConnect4MaskModel1(SACTorchModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_space,\n",
    "        action_space,\n",
    "        num_outputs,\n",
    "        model_config,\n",
    "        name: str,\n",
    "        policy_model_config=None,\n",
    "        q_model_config=None,\n",
    "        twin_q=False,\n",
    "        initial_alpha=1.0,\n",
    "        target_entropy=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        orig_space = getattr(obs_space, \"original_space\", obs_space)\n",
    "\n",
    "        assert isinstance(orig_space, Dict)\n",
    "        assert \"action_mask\" in orig_space.spaces\n",
    "        assert \"observation\" in orig_space.spaces\n",
    "\n",
    "        super().__init__(\n",
    "            obs_space,\n",
    "            action_space,\n",
    "            num_outputs,\n",
    "            model_config,\n",
    "            policy_model_config,\n",
    "            q_model_config,\n",
    "            twin_q,\n",
    "            initial_alpha,\n",
    "            target_entropy,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self.internal_model = TorchFC(\n",
    "            orig_space[\"observation\"],\n",
    "            action_space,\n",
    "            num_outputs,\n",
    "            model_config,\n",
    "            name + \"_internal\",\n",
    "        )\n",
    "\n",
    "    @override(SACTorchModel)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        # Extract the available actions tensor from the observation.\n",
    "        action_mask = input_dict[\"obs\"][\"action_mask\"]\n",
    "\n",
    "        # Compute the unmasked logits.\n",
    "        logits, _ = self.internal_model({\"obs\": input_dict[\"obs\"][\"observation\"]})\n",
    "\n",
    "        # Convert action_mask into a [0.0 || -inf]-type mask.\n",
    "        inf_mask = torch.clamp(torch.log(action_mask), min=FLOAT_MIN)\n",
    "        masked_logits = logits + inf_mask\n",
    "\n",
    "        # Return masked logits.\n",
    "        return masked_logits, state\n",
    "\n",
    "    def value_function(self):\n",
    "        return self.internal_model.value_function()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AddPolicyCallback = create_add_policy_cb(\"policy_0\", best_result.checkpoint)\n",
    "\n",
    "config = (\n",
    "    sac.SACConfig()\n",
    "    .environment(\"connect4\")\n",
    "    .framework(\"torch\")\n",
    "    .callbacks(AddPolicyCallback)\n",
    "    .training(\n",
    "        # model={\n",
    "        #     \"custom_model\": SacConnect4MaskModel1,\n",
    "        # },\n",
    "        policy_model_config={\n",
    "            \"custom_model\": SacConnect4MaskModel1,\n",
    "            # \"conv_filters\": [[2, [6, 7], 1]],\n",
    "        },\n",
    "        q_model_config={\n",
    "            \"custom_model\": SacConnect4MaskModel1,\n",
    "            # \"conv_filters\": [[2, [6, 7], 1]],\n",
    "        })\n",
    "    .multi_agent(\n",
    "        # Initial policy map: Random and PPO. This will be expanded\n",
    "        # to more policy snapshots taken from \"main\" against which \"main\"\n",
    "        # will then play (instead of \"random\"). This is done in the\n",
    "        # custom callback defined above (`SelfPlayCallback`).\n",
    "        # Note: We will add the \"opponent\" policy with callback.\n",
    "        policies={\"main\"},  # Our main policy, we'd like to optimize.\n",
    "        # Assign agent 0 and 1 randomly to the \"main\" policy or\n",
    "        # to the opponent (\"random\" at first). Make sure (via episode_id)\n",
    "        # that \"main\" always plays against \"random\" (and not against\n",
    "        # another \"main\").\n",
    "        policy_mapping_fn=policy_mapping_fn,\n",
    "        # Always just train the \"main\" policy.\n",
    "        policies_to_train=[\"main\"],\n",
    "    )\n",
    ")\n",
    "stop = {\n",
    "    \"timesteps_total\": 10000000,\n",
    "    \"training_iteration\": 500,\n",
    "}\n",
    "self_results = tune.Tuner(\n",
    "    \"SAC\",\n",
    "    param_space=config.to_dict(),\n",
    "    run_config=air.RunConfig(\n",
    "        stop=stop,\n",
    "        verbose=2,\n",
    "        progress_reporter=CLIReporter(\n",
    "            metric_columns={\n",
    "                \"training_iteration\": \"iter\",\n",
    "                \"time_total_s\": \"time_total_s\",\n",
    "                \"timesteps_total\": \"ts\",\n",
    "                \"episodes_this_iter\": \"train_episodes\",\n",
    "                \"policy_reward_mean/main\": \"reward\",\n",
    "                \"win_rate\": \"win_rate\",\n",
    "                \"league_size\": \"league_size\",\n",
    "            },\n",
    "            sort_by_metric=True,\n",
    "        ),\n",
    "        checkpoint_config=air.CheckpointConfig(\n",
    "            checkpoint_at_end=True,\n",
    "            checkpoint_frequency=10,\n",
    "        ),\n",
    "    ),\n",
    ").fit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir /home/clem/ray_results/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
