{"cells":[{"cell_type":"markdown","source":["Installing torchmetrics which is used for metrics calculations"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"aa832112-1d14-4058-9527-a8081ea9d4b8","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["pip install torchmetrics"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f0aee70b-6217-4c91-962b-f4db2625f32d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Python interpreter will be restarted.\nCollecting torchmetrics\n  Using cached torchmetrics-0.10.2-py3-none-any.whl (529 kB)\nRequirement already satisfied: numpy>=1.17.2 in /databricks/python3/lib/python3.9/site-packages (from torchmetrics) (1.20.3)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.9/site-packages (from torchmetrics) (21.0)\nRequirement already satisfied: torch>=1.3.1 in /databricks/python3/lib/python3.9/site-packages (from torchmetrics) (1.11.0+cpu)\nRequirement already satisfied: typing-extensions in /databricks/python3/lib/python3.9/site-packages (from torch>=1.3.1->torchmetrics) (3.10.0.2)\nRequirement already satisfied: pyparsing>=2.0.2 in /databricks/python3/lib/python3.9/site-packages (from packaging->torchmetrics) (3.0.4)\nInstalling collected packages: torchmetrics\nSuccessfully installed torchmetrics-0.10.2\nPython interpreter will be restarted.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Python interpreter will be restarted.\nCollecting torchmetrics\n  Using cached torchmetrics-0.10.2-py3-none-any.whl (529 kB)\nRequirement already satisfied: numpy>=1.17.2 in /databricks/python3/lib/python3.9/site-packages (from torchmetrics) (1.20.3)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.9/site-packages (from torchmetrics) (21.0)\nRequirement already satisfied: torch>=1.3.1 in /databricks/python3/lib/python3.9/site-packages (from torchmetrics) (1.11.0+cpu)\nRequirement already satisfied: typing-extensions in /databricks/python3/lib/python3.9/site-packages (from torch>=1.3.1->torchmetrics) (3.10.0.2)\nRequirement already satisfied: pyparsing>=2.0.2 in /databricks/python3/lib/python3.9/site-packages (from packaging->torchmetrics) (3.0.4)\nInstalling collected packages: torchmetrics\nSuccessfully installed torchmetrics-0.10.2\nPython interpreter will be restarted.\n"]}}],"execution_count":0},{"cell_type":"code","source":["import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport os\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision.datasets import FashionMNIST\nfrom torchmetrics.functional import accuracy"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0310e25e-1962-436e-b072-a7159023cc99","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Setting up checkpoint location\nThe next cell creates a directory for saved checkpoint models. Databricks recommends saving training data under dbfs:/ml, which maps to file:/dbfs/ml on driver and worker nodes."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"86c4f9fb-3778-471c-abfd-9a456d346a9c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["PYTORCH_DIR = '/dbfs/ml/horovod_pytorch'"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3c220f8b-c6a0-4d3b-9bba-bbe69842ed4e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["https://docs.databricks.com/_static/notebooks/deep-learning/mnist-pytorch.html\n\nNote that We are using same model configuration as demo-04 but here we will be testing the model too.Optimiser is also same as demo-04"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1b3f70ac-1d3f-4ee1-bdde-f9a32dc0a1b9","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n        \n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d869bfe3-4d89-41af-99cf-cb62f58059f2","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Here We are configuring for single node training."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0d5c9831-482c-41a9-86b2-530ac693294f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["batch_size = 32\nnum_epochs = 20\nlog_interval = 100\n\ndef train_one_epoch(model, device, data_loader, optimizer, epoch):\n    model.train()\n\n    for batch_idx, (data, target) in enumerate(data_loader):\n        data, target = data.to(device), target.to(device)\n        \n        optimizer.zero_grad()\n        \n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        \n        optimizer.step()\n        \n        if batch_idx % log_interval == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(data_loader) * len(data),\n                100. * batch_idx / len(data_loader), loss.item()))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"58be5f98-b5e6-4d5f-b7be-bc0b80d30a1a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Creating methods for saving and loading model checkpoints"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"280b0173-026f-4ce7-bcdf-39bf20241063","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def save_checkpoint(log_dir, model, optimizer, epoch):\n    filepath = log_dir + '/checkpoint-{epoch}.pth.tar'.format(epoch = epoch)\n    \n    state = {\n      'model': model.state_dict(),\n      'optimizer': optimizer.state_dict(),\n    }\n    \n    torch.save(state, filepath)\n    \ndef load_checkpoint(log_dir, epoch = num_epochs):\n    filepath = log_dir + '/checkpoint-{epoch}.pth.tar'.format(epoch = epoch)\n    \n    return torch.load(filepath)\n \ndef create_log_dir():\n    log_dir = os.path.join(PYTORCH_DIR, str(time()), 'FashionMNISTDemo')\n    os.makedirs(log_dir)\n    \n    return log_dir\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e2fb0f49-11a6-46ac-b8fb-ab36d985b60b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Running single-node training with PyTorch"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bfef4b36-a840-4e0b-ab96-45bfa6b95c47","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from torchvision import datasets, transforms\nfrom time import time\n \nsingle_node_log_dir = create_log_dir()\nprint('Log directory:', single_node_log_dir)\n \ndef train(learning_rate):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n \n    train_dataset = datasets.FashionMNIST(\n      'data', \n      train = True,\n      download = True,\n      transform = transforms.Compose([transforms.ToTensor()])\n    )\n\n    data_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n \n    model = Net().to(device)\n \n    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n \n    for epoch in range(1, num_epochs + 1):\n        train_one_epoch(model, device, data_loader, optimizer, epoch)\n        save_checkpoint(single_node_log_dir, model, optimizer, epoch)\n \n    \ndef test(log_dir):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    loaded_model = Net().to(device)\n  \n    checkpoint = load_checkpoint(log_dir)\n    loaded_model.load_state_dict(checkpoint['model'])\n    loaded_model.eval()\n \n    test_dataset = datasets.FashionMNIST(\n        'data', \n        train = False,\n        download = True,\n        transform = transforms.Compose([transforms.ToTensor()])\n    )\n    \n    data_loader = torch.utils.data.DataLoader(test_dataset)\n    \n    correct = 0\n    total = 0\n    test_loss = 0\n    for data, target in data_loader:\n        data, target = data.to(device), target.to(device)\n        output = loaded_model(data)\n        test_loss += F.nll_loss(output, target)\n        _, predicted = torch.max(output.data, 1)\n        total += target.size(0)\n        correct += (predicted == target).sum().item()\n\n    test_loss /= len(data_loader.dataset)\n\n    print('Average test loss: {}'.format(test_loss.item()))\n    print('Accuracy of the network on the test images: ', 100 * correct // total, '%')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"53bb62c4-3e1b-4581-93fa-5df9c2521394","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Log directory: /dbfs/ml/horovod_pytorch/1668000143.9331226/FashionMNISTDemo\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Log directory: /dbfs/ml/horovod_pytorch/1668000143.9331226/FashionMNISTDemo\n"]}}],"execution_count":0},{"cell_type":"code","source":["train(learning_rate = 0.02)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"437acc4b-2a06-47fc-b8f8-9026f063b9f0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":{"text/plain":"  0%|          | 0/26421880 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0c0aba1a0944904b9039f1784b18490"}},"removedWidgets":[],"addedWidgets":{},"metadata":{"kernelSessionId":"df347cec-bb7c2551b1ce8e6424877c67"},"type":"mimeBundle","arguments":{}}},"output_type":"display_data","data":{"text/plain":"  0%|          | 0/26421880 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0c0aba1a0944904b9039f1784b18490"}}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":{"text/plain":"  0%|          | 0/29515 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2631a898a1c44283bec4e4ee6b7c047a"}},"removedWidgets":[],"addedWidgets":{},"metadata":{"kernelSessionId":"df347cec-bb7c2551b1ce8e6424877c67"},"type":"mimeBundle","arguments":{}}},"output_type":"display_data","data":{"text/plain":"  0%|          | 0/29515 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2631a898a1c44283bec4e4ee6b7c047a"}}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":{"text/plain":"  0%|          | 0/4422102 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b9d64628ebc491f966a49cadd020692"}},"removedWidgets":[],"addedWidgets":{},"metadata":{"kernelSessionId":"df347cec-bb7c2551b1ce8e6424877c67"},"type":"mimeBundle","arguments":{}}},"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4422102 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b9d64628ebc491f966a49cadd020692"}}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":{"text/plain":"  0%|          | 0/5148 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1836f807c1fc4f5395820752c421a9ea"}},"removedWidgets":[],"addedWidgets":{},"metadata":{"kernelSessionId":"df347cec-bb7c2551b1ce8e6424877c67"},"type":"mimeBundle","arguments":{}}},"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5148 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1836f807c1fc4f5395820752c421a9ea"}}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n\nTrain Epoch: 1 [0/60000 (0%)]\tLoss: 2.273260\nTrain Epoch: 1 [3200/60000 (5%)]\tLoss: 1.599953\nTrain Epoch: 1 [6400/60000 (11%)]\tLoss: 1.027429\nTrain Epoch: 1 [9600/60000 (16%)]\tLoss: 1.090250\nTrain Epoch: 1 [12800/60000 (21%)]\tLoss: 0.815612\nTrain Epoch: 1 [16000/60000 (27%)]\tLoss: 1.124819\nTrain Epoch: 1 [19200/60000 (32%)]\tLoss: 0.543982\nTrain Epoch: 1 [22400/60000 (37%)]\tLoss: 1.075094\nTrain Epoch: 1 [25600/60000 (43%)]\tLoss: 0.531899\nTrain Epoch: 1 [28800/60000 (48%)]\tLoss: 0.817578\nTrain Epoch: 1 [32000/60000 (53%)]\tLoss: 1.031522\nTrain Epoch: 1 [35200/60000 (59%)]\tLoss: 0.855289\nTrain Epoch: 1 [38400/60000 (64%)]\tLoss: 1.144775\nTrain Epoch: 1 [41600/60000 (69%)]\tLoss: 1.247397\nTrain Epoch: 1 [44800/60000 (75%)]\tLoss: 0.717510\nTrain Epoch: 1 [48000/60000 (80%)]\tLoss: 0.803581\nTrain Epoch: 1 [51200/60000 (85%)]\tLoss: 1.120059\nTrain Epoch: 1 [54400/60000 (91%)]\tLoss: 0.829369\nTrain Epoch: 1 [57600/60000 (96%)]\tLoss: 0.721658\nTrain Epoch: 2 [0/60000 (0%)]\tLoss: 0.775172\nTrain Epoch: 2 [3200/60000 (5%)]\tLoss: 0.975825\nTrain Epoch: 2 [6400/60000 (11%)]\tLoss: 0.993350\nTrain Epoch: 2 [9600/60000 (16%)]\tLoss: 1.405437\nTrain Epoch: 2 [12800/60000 (21%)]\tLoss: 0.899158\nTrain Epoch: 2 [16000/60000 (27%)]\tLoss: 1.151344\nTrain Epoch: 2 [19200/60000 (32%)]\tLoss: 0.902632\nTrain Epoch: 2 [22400/60000 (37%)]\tLoss: 1.821064\nTrain Epoch: 2 [25600/60000 (43%)]\tLoss: 0.962480\nTrain Epoch: 2 [28800/60000 (48%)]\tLoss: 1.061507\nTrain Epoch: 2 [32000/60000 (53%)]\tLoss: 1.278113\nTrain Epoch: 2 [35200/60000 (59%)]\tLoss: 0.927010\nTrain Epoch: 2 [38400/60000 (64%)]\tLoss: 0.974235\nTrain Epoch: 2 [41600/60000 (69%)]\tLoss: 1.053996\nTrain Epoch: 2 [44800/60000 (75%)]\tLoss: 0.739277\nTrain Epoch: 2 [48000/60000 (80%)]\tLoss: 0.750613\nTrain Epoch: 2 [51200/60000 (85%)]\tLoss: 0.859195\nTrain Epoch: 2 [54400/60000 (91%)]\tLoss: 1.211353\nTrain Epoch: 2 [57600/60000 (96%)]\tLoss: 0.795715\nTrain Epoch: 3 [0/60000 (0%)]\tLoss: 1.205979\nTrain Epoch: 3 [3200/60000 (5%)]\tLoss: 1.047980\nTrain Epoch: 3 [6400/60000 (11%)]\tLoss: 0.933089\nTrain Epoch: 3 [9600/60000 (16%)]\tLoss: 1.482590\nTrain Epoch: 3 [12800/60000 (21%)]\tLoss: 1.011938\nTrain Epoch: 3 [16000/60000 (27%)]\tLoss: 0.775776\nTrain Epoch: 3 [19200/60000 (32%)]\tLoss: 1.211690\nTrain Epoch: 3 [22400/60000 (37%)]\tLoss: 0.904257\nTrain Epoch: 3 [25600/60000 (43%)]\tLoss: 1.102352\nTrain Epoch: 3 [28800/60000 (48%)]\tLoss: 0.882785\nTrain Epoch: 3 [32000/60000 (53%)]\tLoss: 0.875881\nTrain Epoch: 3 [35200/60000 (59%)]\tLoss: 1.071694\nTrain Epoch: 3 [38400/60000 (64%)]\tLoss: 1.096462\nTrain Epoch: 3 [41600/60000 (69%)]\tLoss: 1.184636\nTrain Epoch: 3 [44800/60000 (75%)]\tLoss: 1.031978\nTrain Epoch: 3 [48000/60000 (80%)]\tLoss: 1.317308\nTrain Epoch: 3 [51200/60000 (85%)]\tLoss: 0.943205\nTrain Epoch: 3 [54400/60000 (91%)]\tLoss: 1.230762\nTrain Epoch: 3 [57600/60000 (96%)]\tLoss: 0.822100\nTrain Epoch: 4 [0/60000 (0%)]\tLoss: 1.077190\nTrain Epoch: 4 [3200/60000 (5%)]\tLoss: 0.936503\nTrain Epoch: 4 [6400/60000 (11%)]\tLoss: 0.821748\nTrain Epoch: 4 [9600/60000 (16%)]\tLoss: 1.356460\nTrain Epoch: 4 [12800/60000 (21%)]\tLoss: 1.075151\nTrain Epoch: 4 [16000/60000 (27%)]\tLoss: 1.063204\nTrain Epoch: 4 [19200/60000 (32%)]\tLoss: 1.716724\nTrain Epoch: 4 [22400/60000 (37%)]\tLoss: 1.470576\nTrain Epoch: 4 [25600/60000 (43%)]\tLoss: 1.234031\nTrain Epoch: 4 [28800/60000 (48%)]\tLoss: 1.089860\nTrain Epoch: 4 [32000/60000 (53%)]\tLoss: 1.251610\nTrain Epoch: 4 [35200/60000 (59%)]\tLoss: 0.791799\nTrain Epoch: 4 [38400/60000 (64%)]\tLoss: 0.745393\nTrain Epoch: 4 [41600/60000 (69%)]\tLoss: 0.932722\nTrain Epoch: 4 [44800/60000 (75%)]\tLoss: 1.002915\nTrain Epoch: 4 [48000/60000 (80%)]\tLoss: 1.055039\nTrain Epoch: 4 [51200/60000 (85%)]\tLoss: 1.005410\nTrain Epoch: 4 [54400/60000 (91%)]\tLoss: 0.966119\nTrain Epoch: 4 [57600/60000 (96%)]\tLoss: 0.832923\nTrain Epoch: 5 [0/60000 (0%)]\tLoss: 0.992388\nTrain Epoch: 5 [3200/60000 (5%)]\tLoss: 1.112072\nTrain Epoch: 5 [6400/60000 (11%)]\tLoss: 1.136824\nTrain Epoch: 5 [9600/60000 (16%)]\tLoss: 1.395138\nTrain Epoch: 5 [12800/60000 (21%)]\tLoss: 0.888680\nTrain Epoch: 5 [16000/60000 (27%)]\tLoss: 0.913624\nTrain Epoch: 5 [19200/60000 (32%)]\tLoss: 0.850774\nTrain Epoch: 5 [22400/60000 (37%)]\tLoss: 1.113031\nTrain Epoch: 5 [25600/60000 (43%)]\tLoss: 0.912433\nTrain Epoch: 5 [28800/60000 (48%)]\tLoss: 0.881838\nTrain Epoch: 5 [32000/60000 (53%)]\tLoss: 1.361618\nTrain Epoch: 5 [35200/60000 (59%)]\tLoss: 0.820254\nTrain Epoch: 5 [38400/60000 (64%)]\tLoss: 1.100415\nTrain Epoch: 5 [41600/60000 (69%)]\tLoss: 0.617996\nTrain Epoch: 5 [44800/60000 (75%)]\tLoss: 1.243987\nTrain Epoch: 5 [48000/60000 (80%)]\tLoss: 0.954995\nTrain Epoch: 5 [51200/60000 (85%)]\tLoss: 0.818335\nTrain Epoch: 5 [54400/60000 (91%)]\tLoss: 0.988921\nTrain Epoch: 5 [57600/60000 (96%)]\tLoss: 1.138756\nTrain Epoch: 6 [0/60000 (0%)]\tLoss: 1.403925\nTrain Epoch: 6 [3200/60000 (5%)]\tLoss: 0.771945\nTrain Epoch: 6 [6400/60000 (11%)]\tLoss: 0.871269\nTrain Epoch: 6 [9600/60000 (16%)]\tLoss: 0.819527\nTrain Epoch: 6 [12800/60000 (21%)]\tLoss: 0.720687\nTrain Epoch: 6 [16000/60000 (27%)]\tLoss: 0.805354\nTrain Epoch: 6 [19200/60000 (32%)]\tLoss: 1.215445\nTrain Epoch: 6 [22400/60000 (37%)]\tLoss: 0.924986\nTrain Epoch: 6 [25600/60000 (43%)]\tLoss: 0.813246\nTrain Epoch: 6 [28800/60000 (48%)]\tLoss: 1.029393\nTrain Epoch: 6 [32000/60000 (53%)]\tLoss: 0.807215\nTrain Epoch: 6 [35200/60000 (59%)]\tLoss: 1.182888\nTrain Epoch: 6 [38400/60000 (64%)]\tLoss: 1.515429\nTrain Epoch: 6 [41600/60000 (69%)]\tLoss: 0.655966\nTrain Epoch: 6 [44800/60000 (75%)]\tLoss: 0.913739\nTrain Epoch: 6 [48000/60000 (80%)]\tLoss: 0.578605\nTrain Epoch: 6 [51200/60000 (85%)]\tLoss: 0.910598\nTrain Epoch: 6 [54400/60000 (91%)]\tLoss: 1.286528\nTrain Epoch: 6 [57600/60000 (96%)]\tLoss: 0.690669\nTrain Epoch: 7 [0/60000 (0%)]\tLoss: 0.770200\nTrain Epoch: 7 [3200/60000 (5%)]\tLoss: 0.844138\nTrain Epoch: 7 [6400/60000 (11%)]\tLoss: 0.761577\nTrain Epoch: 7 [9600/60000 (16%)]\tLoss: 1.680434\nTrain Epoch: 7 [12800/60000 (21%)]\tLoss: 0.950974\nTrain Epoch: 7 [16000/60000 (27%)]\tLoss: 0.794620\nTrain Epoch: 7 [19200/60000 (32%)]\tLoss: 1.179558\nTrain Epoch: 7 [22400/60000 (37%)]\tLoss: 0.885666\nTrain Epoch: 7 [25600/60000 (43%)]\tLoss: 0.740741\nTrain Epoch: 7 [28800/60000 (48%)]\tLoss: 0.816225\nTrain Epoch: 7 [32000/60000 (53%)]\tLoss: 0.940559\nTrain Epoch: 7 [35200/60000 (59%)]\tLoss: 1.385345\nTrain Epoch: 7 [38400/60000 (64%)]\tLoss: 1.250580\nTrain Epoch: 7 [41600/60000 (69%)]\tLoss: 0.785304\nTrain Epoch: 7 [44800/60000 (75%)]\tLoss: 1.152100\nTrain Epoch: 7 [48000/60000 (80%)]\tLoss: 0.712901\nTrain Epoch: 7 [51200/60000 (85%)]\tLoss: 0.622097\nTrain Epoch: 7 [54400/60000 (91%)]\tLoss: 1.178175\nTrain Epoch: 7 [57600/60000 (96%)]\tLoss: 1.378611\nTrain Epoch: 8 [0/60000 (0%)]\tLoss: 0.983257\nTrain Epoch: 8 [3200/60000 (5%)]\tLoss: 0.792742\nTrain Epoch: 8 [6400/60000 (11%)]\tLoss: 0.769253\nTrain Epoch: 8 [9600/60000 (16%)]\tLoss: 1.310156\nTrain Epoch: 8 [12800/60000 (21%)]\tLoss: 0.865200\nTrain Epoch: 8 [16000/60000 (27%)]\tLoss: 0.710945\nTrain Epoch: 8 [19200/60000 (32%)]\tLoss: 1.161991\nTrain Epoch: 8 [22400/60000 (37%)]\tLoss: 1.044387\nTrain Epoch: 8 [25600/60000 (43%)]\tLoss: 1.397429\nTrain Epoch: 8 [28800/60000 (48%)]\tLoss: 1.063408\nTrain Epoch: 8 [32000/60000 (53%)]\tLoss: 1.130605\nTrain Epoch: 8 [35200/60000 (59%)]\tLoss: 0.856220\nTrain Epoch: 8 [38400/60000 (64%)]\tLoss: 0.938741\nTrain Epoch: 8 [41600/60000 (69%)]\tLoss: 2.088273\nTrain Epoch: 8 [44800/60000 (75%)]\tLoss: 1.320642\nTrain Epoch: 8 [48000/60000 (80%)]\tLoss: 0.673828\nTrain Epoch: 8 [51200/60000 (85%)]\tLoss: 0.758273\nTrain Epoch: 8 [54400/60000 (91%)]\tLoss: 0.882033\nTrain Epoch: 8 [57600/60000 (96%)]\tLoss: 1.207307\nTrain Epoch: 9 [0/60000 (0%)]\tLoss: 1.250788\nTrain Epoch: 9 [3200/60000 (5%)]\tLoss: 1.160486\nTrain Epoch: 9 [6400/60000 (11%)]\tLoss: 0.785669\nTrain Epoch: 9 [9600/60000 (16%)]\tLoss: 0.804436\nTrain Epoch: 9 [12800/60000 (21%)]\tLoss: 1.037995\nTrain Epoch: 9 [16000/60000 (27%)]\tLoss: 0.805331\nTrain Epoch: 9 [19200/60000 (32%)]\tLoss: 1.015645\nTrain Epoch: 9 [22400/60000 (37%)]\tLoss: 0.773032\nTrain Epoch: 9 [25600/60000 (43%)]\tLoss: 0.615527\nTrain Epoch: 9 [28800/60000 (48%)]\tLoss: 0.811075\nTrain Epoch: 9 [32000/60000 (53%)]\tLoss: 1.051100\nTrain Epoch: 9 [35200/60000 (59%)]\tLoss: 0.860456\nTrain Epoch: 9 [38400/60000 (64%)]\tLoss: 0.384371\nTrain Epoch: 9 [41600/60000 (69%)]\tLoss: 1.043506\nTrain Epoch: 9 [44800/60000 (75%)]\tLoss: 1.163232\nTrain Epoch: 9 [48000/60000 (80%)]\tLoss: 1.223855\nTrain Epoch: 9 [51200/60000 (85%)]\tLoss: 0.943369\nTrain Epoch: 9 [54400/60000 (91%)]\tLoss: 1.158034\nTrain Epoch: 9 [57600/60000 (96%)]\tLoss: 1.224279\nTrain Epoch: 10 [0/60000 (0%)]\tLoss: 0.955542\nTrain Epoch: 10 [3200/60000 (5%)]\tLoss: 1.389231\nTrain Epoch: 10 [6400/60000 (11%)]\tLoss: 0.947471\nTrain Epoch: 10 [9600/60000 (16%)]\tLoss: 0.832508\nTrain Epoch: 10 [12800/60000 (21%)]\tLoss: 1.732301\nTrain Epoch: 10 [16000/60000 (27%)]\tLoss: 0.993642\nTrain Epoch: 10 [19200/60000 (32%)]\tLoss: 0.845107\nTrain Epoch: 10 [22400/60000 (37%)]\tLoss: 0.893238\nTrain Epoch: 10 [25600/60000 (43%)]\tLoss: 0.900986\nTrain Epoch: 10 [28800/60000 (48%)]\tLoss: 0.834590\nTrain Epoch: 10 [32000/60000 (53%)]\tLoss: 0.763298\nTrain Epoch: 10 [35200/60000 (59%)]\tLoss: 0.898613\nTrain Epoch: 10 [38400/60000 (64%)]\tLoss: 0.961223\nTrain Epoch: 10 [41600/60000 (69%)]\tLoss: 1.007339\nTrain Epoch: 10 [44800/60000 (75%)]\tLoss: 1.119481\nTrain Epoch: 10 [48000/60000 (80%)]\tLoss: 0.745059\nTrain Epoch: 10 [51200/60000 (85%)]\tLoss: 0.830305\nTrain Epoch: 10 [54400/60000 (91%)]\tLoss: 1.386109\nTrain Epoch: 10 [57600/60000 (96%)]\tLoss: 1.086295\nTrain Epoch: 11 [0/60000 (0%)]\tLoss: 0.634647\nTrain Epoch: 11 [3200/60000 (5%)]\tLoss: 0.836453\nTrain Epoch: 11 [6400/60000 (11%)]\tLoss: 0.923910\nTrain Epoch: 11 [9600/60000 (16%)]\tLoss: 1.007622\nTrain Epoch: 11 [12800/60000 (21%)]\tLoss: 0.995869\nTrain Epoch: 11 [16000/60000 (27%)]\tLoss: 1.110815\nTrain Epoch: 11 [19200/60000 (32%)]\tLoss: 0.718137\nTrain Epoch: 11 [22400/60000 (37%)]\tLoss: 1.180480\nTrain Epoch: 11 [25600/60000 (43%)]\tLoss: 1.081848\nTrain Epoch: 11 [28800/60000 (48%)]\tLoss: 0.792830\nTrain Epoch: 11 [32000/60000 (53%)]\tLoss: 0.644760\nTrain Epoch: 11 [35200/60000 (59%)]\tLoss: 0.560990\nTrain Epoch: 11 [38400/60000 (64%)]\tLoss: 0.689144\nTrain Epoch: 11 [41600/60000 (69%)]\tLoss: 1.134218\nTrain Epoch: 11 [44800/60000 (75%)]\tLoss: 0.610044\nTrain Epoch: 11 [48000/60000 (80%)]\tLoss: 1.715760\nTrain Epoch: 11 [51200/60000 (85%)]\tLoss: 0.781585\nTrain Epoch: 11 [54400/60000 (91%)]\tLoss: 0.999184\nTrain Epoch: 11 [57600/60000 (96%)]\tLoss: 0.905494\nTrain Epoch: 12 [0/60000 (0%)]\tLoss: 0.969722\nTrain Epoch: 12 [3200/60000 (5%)]\tLoss: 0.920955\nTrain Epoch: 12 [6400/60000 (11%)]\tLoss: 0.772392\nTrain Epoch: 12 [9600/60000 (16%)]\tLoss: 1.264042\nTrain Epoch: 12 [12800/60000 (21%)]\tLoss: 0.954187\nTrain Epoch: 12 [16000/60000 (27%)]\tLoss: 0.591757\nTrain Epoch: 12 [19200/60000 (32%)]\tLoss: 0.855580\nTrain Epoch: 12 [22400/60000 (37%)]\tLoss: 1.002626\nTrain Epoch: 12 [25600/60000 (43%)]\tLoss: 1.231830\nTrain Epoch: 12 [28800/60000 (48%)]\tLoss: 1.243482\nTrain Epoch: 12 [32000/60000 (53%)]\tLoss: 0.700764\nTrain Epoch: 12 [35200/60000 (59%)]\tLoss: 0.891650\nTrain Epoch: 12 [38400/60000 (64%)]\tLoss: 0.706239\nTrain Epoch: 12 [41600/60000 (69%)]\tLoss: 1.434680\nTrain Epoch: 12 [44800/60000 (75%)]\tLoss: 0.888867\nTrain Epoch: 12 [48000/60000 (80%)]\tLoss: 0.917048\nTrain Epoch: 12 [51200/60000 (85%)]\tLoss: 1.222978\nTrain Epoch: 12 [54400/60000 (91%)]\tLoss: 1.137347\nTrain Epoch: 12 [57600/60000 (96%)]\tLoss: 1.450256\nTrain Epoch: 13 [0/60000 (0%)]\tLoss: 0.967024\nTrain Epoch: 13 [3200/60000 (5%)]\tLoss: 1.030789\nTrain Epoch: 13 [6400/60000 (11%)]\tLoss: 1.295430\nTrain Epoch: 13 [9600/60000 (16%)]\tLoss: 0.871987\nTrain Epoch: 13 [12800/60000 (21%)]\tLoss: 1.047137\nTrain Epoch: 13 [16000/60000 (27%)]\tLoss: 0.986406\nTrain Epoch: 13 [19200/60000 (32%)]\tLoss: 0.814248\nTrain Epoch: 13 [22400/60000 (37%)]\tLoss: 0.969713\nTrain Epoch: 13 [25600/60000 (43%)]\tLoss: 0.729157\nTrain Epoch: 13 [28800/60000 (48%)]\tLoss: 1.168123\nTrain Epoch: 13 [32000/60000 (53%)]\tLoss: 1.074293\nTrain Epoch: 13 [35200/60000 (59%)]\tLoss: 1.508831\nTrain Epoch: 13 [38400/60000 (64%)]\tLoss: 1.126747\nTrain Epoch: 13 [41600/60000 (69%)]\tLoss: 0.927935\nTrain Epoch: 13 [44800/60000 (75%)]\tLoss: 1.103164\nTrain Epoch: 13 [48000/60000 (80%)]\tLoss: 0.693556\nTrain Epoch: 13 [51200/60000 (85%)]\tLoss: 1.071212\nTrain Epoch: 13 [54400/60000 (91%)]\tLoss: 1.252457\nTrain Epoch: 13 [57600/60000 (96%)]\tLoss: 0.744302\nTrain Epoch: 14 [0/60000 (0%)]\tLoss: 1.379637\nTrain Epoch: 14 [3200/60000 (5%)]\tLoss: 0.792607\nTrain Epoch: 14 [6400/60000 (11%)]\tLoss: 1.012426\nTrain Epoch: 14 [9600/60000 (16%)]\tLoss: 0.947124\nTrain Epoch: 14 [12800/60000 (21%)]\tLoss: 1.102205\nTrain Epoch: 14 [16000/60000 (27%)]\tLoss: 0.722943\nTrain Epoch: 14 [19200/60000 (32%)]\tLoss: 0.992637\nTrain Epoch: 14 [22400/60000 (37%)]\tLoss: 1.336712\nTrain Epoch: 14 [25600/60000 (43%)]\tLoss: 0.848735\nTrain Epoch: 14 [28800/60000 (48%)]\tLoss: 0.857499\nTrain Epoch: 14 [32000/60000 (53%)]\tLoss: 0.855187\nTrain Epoch: 14 [35200/60000 (59%)]\tLoss: 1.014216\nTrain Epoch: 14 [38400/60000 (64%)]\tLoss: 0.945315\nTrain Epoch: 14 [41600/60000 (69%)]\tLoss: 0.650733\nTrain Epoch: 14 [44800/60000 (75%)]\tLoss: 1.229133\nTrain Epoch: 14 [48000/60000 (80%)]\tLoss: 1.364087\nTrain Epoch: 14 [51200/60000 (85%)]\tLoss: 0.921818\nTrain Epoch: 14 [54400/60000 (91%)]\tLoss: 1.030675\nTrain Epoch: 14 [57600/60000 (96%)]\tLoss: 0.794336\nTrain Epoch: 15 [0/60000 (0%)]\tLoss: 0.840845\nTrain Epoch: 15 [3200/60000 (5%)]\tLoss: 1.686122\nTrain Epoch: 15 [6400/60000 (11%)]\tLoss: 1.372050\nTrain Epoch: 15 [9600/60000 (16%)]\tLoss: 0.975734\nTrain Epoch: 15 [12800/60000 (21%)]\tLoss: 0.989867\nTrain Epoch: 15 [16000/60000 (27%)]\tLoss: 1.339431\nTrain Epoch: 15 [19200/60000 (32%)]\tLoss: 0.914139\nTrain Epoch: 15 [22400/60000 (37%)]\tLoss: 1.107859\nTrain Epoch: 15 [25600/60000 (43%)]\tLoss: 0.967759\nTrain Epoch: 15 [28800/60000 (48%)]\tLoss: 1.080097\nTrain Epoch: 15 [32000/60000 (53%)]\tLoss: 0.869940\nTrain Epoch: 15 [35200/60000 (59%)]\tLoss: 1.002007\nTrain Epoch: 15 [38400/60000 (64%)]\tLoss: 0.903447\nTrain Epoch: 15 [41600/60000 (69%)]\tLoss: 0.903836\nTrain Epoch: 15 [44800/60000 (75%)]\tLoss: 0.692108\nTrain Epoch: 15 [48000/60000 (80%)]\tLoss: 1.322002\nTrain Epoch: 15 [51200/60000 (85%)]\tLoss: 0.998504\nTrain Epoch: 15 [54400/60000 (91%)]\tLoss: 0.743107\nTrain Epoch: 15 [57600/60000 (96%)]\tLoss: 0.859381\nTrain Epoch: 16 [0/60000 (0%)]\tLoss: 0.793166\nTrain Epoch: 16 [3200/60000 (5%)]\tLoss: 1.267776\nTrain Epoch: 16 [6400/60000 (11%)]\tLoss: 0.881496\nTrain Epoch: 16 [9600/60000 (16%)]\tLoss: 0.630358\nTrain Epoch: 16 [12800/60000 (21%)]\tLoss: 0.985188\nTrain Epoch: 16 [16000/60000 (27%)]\tLoss: 0.760136\nTrain Epoch: 16 [19200/60000 (32%)]\tLoss: 0.747839\nTrain Epoch: 16 [22400/60000 (37%)]\tLoss: 0.748285\nTrain Epoch: 16 [25600/60000 (43%)]\tLoss: 0.746582\nTrain Epoch: 16 [28800/60000 (48%)]\tLoss: 0.987383\nTrain Epoch: 16 [32000/60000 (53%)]\tLoss: 1.176063\nTrain Epoch: 16 [35200/60000 (59%)]\tLoss: 1.097143\nTrain Epoch: 16 [38400/60000 (64%)]\tLoss: 1.021680\nTrain Epoch: 16 [41600/60000 (69%)]\tLoss: 0.700033\nTrain Epoch: 16 [44800/60000 (75%)]\tLoss: 0.825742\nTrain Epoch: 16 [48000/60000 (80%)]\tLoss: 1.294810\nTrain Epoch: 16 [51200/60000 (85%)]\tLoss: 0.855803\nTrain Epoch: 16 [54400/60000 (91%)]\tLoss: 0.652467\nTrain Epoch: 16 [57600/60000 (96%)]\tLoss: 1.294417\nTrain Epoch: 17 [0/60000 (0%)]\tLoss: 0.886119\nTrain Epoch: 17 [3200/60000 (5%)]\tLoss: 1.764886\nTrain Epoch: 17 [6400/60000 (11%)]\tLoss: 0.811992\nTrain Epoch: 17 [9600/60000 (16%)]\tLoss: 1.156330\nTrain Epoch: 17 [12800/60000 (21%)]\tLoss: 1.283693\nTrain Epoch: 17 [16000/60000 (27%)]\tLoss: 0.946574\nTrain Epoch: 17 [19200/60000 (32%)]\tLoss: 1.079442\nTrain Epoch: 17 [22400/60000 (37%)]\tLoss: 0.719215\nTrain Epoch: 17 [25600/60000 (43%)]\tLoss: 0.879068\nTrain Epoch: 17 [28800/60000 (48%)]\tLoss: 0.999480\nTrain Epoch: 17 [32000/60000 (53%)]\tLoss: 0.967303\nTrain Epoch: 17 [35200/60000 (59%)]\tLoss: 0.668322\nTrain Epoch: 17 [38400/60000 (64%)]\tLoss: 0.750904\nTrain Epoch: 17 [41600/60000 (69%)]\tLoss: 0.829480\nTrain Epoch: 17 [44800/60000 (75%)]\tLoss: 0.768210\nTrain Epoch: 17 [48000/60000 (80%)]\tLoss: 1.119552\nTrain Epoch: 17 [51200/60000 (85%)]\tLoss: 0.669064\nTrain Epoch: 17 [54400/60000 (91%)]\tLoss: 1.124795\nTrain Epoch: 17 [57600/60000 (96%)]\tLoss: 0.689165\nTrain Epoch: 18 [0/60000 (0%)]\tLoss: 1.168346\nTrain Epoch: 18 [3200/60000 (5%)]\tLoss: 0.889254\nTrain Epoch: 18 [6400/60000 (11%)]\tLoss: 0.894667\nTrain Epoch: 18 [9600/60000 (16%)]\tLoss: 0.993654\nTrain Epoch: 18 [12800/60000 (21%)]\tLoss: 1.246198\nTrain Epoch: 18 [16000/60000 (27%)]\tLoss: 0.742207\nTrain Epoch: 18 [19200/60000 (32%)]\tLoss: 1.022499\nTrain Epoch: 18 [22400/60000 (37%)]\tLoss: 0.953169\nTrain Epoch: 18 [25600/60000 (43%)]\tLoss: 0.812642\nTrain Epoch: 18 [28800/60000 (48%)]\tLoss: 0.828113\nTrain Epoch: 18 [32000/60000 (53%)]\tLoss: 0.833560\nTrain Epoch: 18 [35200/60000 (59%)]\tLoss: 1.110223\nTrain Epoch: 18 [38400/60000 (64%)]\tLoss: 1.140273\nTrain Epoch: 18 [41600/60000 (69%)]\tLoss: 0.872436\nTrain Epoch: 18 [44800/60000 (75%)]\tLoss: 0.890928\nTrain Epoch: 18 [48000/60000 (80%)]\tLoss: 0.509080\nTrain Epoch: 18 [51200/60000 (85%)]\tLoss: 1.271792\nTrain Epoch: 18 [54400/60000 (91%)]\tLoss: 1.140503\nTrain Epoch: 18 [57600/60000 (96%)]\tLoss: 0.986190\nTrain Epoch: 19 [0/60000 (0%)]\tLoss: 1.450598\nTrain Epoch: 19 [3200/60000 (5%)]\tLoss: 0.794300\nTrain Epoch: 19 [6400/60000 (11%)]\tLoss: 1.328353\nTrain Epoch: 19 [9600/60000 (16%)]\tLoss: 0.841129\nTrain Epoch: 19 [12800/60000 (21%)]\tLoss: 0.930606\nTrain Epoch: 19 [16000/60000 (27%)]\tLoss: 1.543601\nTrain Epoch: 19 [19200/60000 (32%)]\tLoss: 1.487835\nTrain Epoch: 19 [22400/60000 (37%)]\tLoss: 1.019675\nTrain Epoch: 19 [25600/60000 (43%)]\tLoss: 1.362598\nTrain Epoch: 19 [28800/60000 (48%)]\tLoss: 0.998519\nTrain Epoch: 19 [32000/60000 (53%)]\tLoss: 0.868672\nTrain Epoch: 19 [35200/60000 (59%)]\tLoss: 0.724700\nTrain Epoch: 19 [38400/60000 (64%)]\tLoss: 0.817852\nTrain Epoch: 19 [41600/60000 (69%)]\tLoss: 0.676260\nTrain Epoch: 19 [44800/60000 (75%)]\tLoss: 0.682136\nTrain Epoch: 19 [48000/60000 (80%)]\tLoss: 1.061193\nTrain Epoch: 19 [51200/60000 (85%)]\tLoss: 0.710633\nTrain Epoch: 19 [54400/60000 (91%)]\tLoss: 0.661032\nTrain Epoch: 19 [57600/60000 (96%)]\tLoss: 0.764810\nTrain Epoch: 20 [0/60000 (0%)]\tLoss: 1.104778\nTrain Epoch: 20 [3200/60000 (5%)]\tLoss: 0.791256\nTrain Epoch: 20 [6400/60000 (11%)]\tLoss: 1.109865\nTrain Epoch: 20 [9600/60000 (16%)]\tLoss: 1.196493\nTrain Epoch: 20 [12800/60000 (21%)]\tLoss: 1.130605\nTrain Epoch: 20 [16000/60000 (27%)]\tLoss: 1.301185\nTrain Epoch: 20 [19200/60000 (32%)]\tLoss: 0.611422\nTrain Epoch: 20 [22400/60000 (37%)]\tLoss: 0.875233\nTrain Epoch: 20 [25600/60000 (43%)]\tLoss: 0.766816\nTrain Epoch: 20 [28800/60000 (48%)]\tLoss: 0.729236\nTrain Epoch: 20 [32000/60000 (53%)]\tLoss: 1.429911\nTrain Epoch: 20 [35200/60000 (59%)]\tLoss: 1.290803\nTrain Epoch: 20 [38400/60000 (64%)]\tLoss: 0.883053\nTrain Epoch: 20 [41600/60000 (69%)]\tLoss: 0.799179\nTrain Epoch: 20 [44800/60000 (75%)]\tLoss: 1.045050\nTrain Epoch: 20 [48000/60000 (80%)]\tLoss: 1.015107\nTrain Epoch: 20 [51200/60000 (85%)]\tLoss: 0.954014\nTrain Epoch: 20 [54400/60000 (91%)]\tLoss: 1.033333\nTrain Epoch: 20 [57600/60000 (96%)]\tLoss: 0.780848\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n\nTrain Epoch: 1 [0/60000 (0%)]\tLoss: 2.273260\nTrain Epoch: 1 [3200/60000 (5%)]\tLoss: 1.599953\nTrain Epoch: 1 [6400/60000 (11%)]\tLoss: 1.027429\nTrain Epoch: 1 [9600/60000 (16%)]\tLoss: 1.090250\nTrain Epoch: 1 [12800/60000 (21%)]\tLoss: 0.815612\nTrain Epoch: 1 [16000/60000 (27%)]\tLoss: 1.124819\nTrain Epoch: 1 [19200/60000 (32%)]\tLoss: 0.543982\nTrain Epoch: 1 [22400/60000 (37%)]\tLoss: 1.075094\nTrain Epoch: 1 [25600/60000 (43%)]\tLoss: 0.531899\nTrain Epoch: 1 [28800/60000 (48%)]\tLoss: 0.817578\nTrain Epoch: 1 [32000/60000 (53%)]\tLoss: 1.031522\nTrain Epoch: 1 [35200/60000 (59%)]\tLoss: 0.855289\nTrain Epoch: 1 [38400/60000 (64%)]\tLoss: 1.144775\nTrain Epoch: 1 [41600/60000 (69%)]\tLoss: 1.247397\nTrain Epoch: 1 [44800/60000 (75%)]\tLoss: 0.717510\nTrain Epoch: 1 [48000/60000 (80%)]\tLoss: 0.803581\nTrain Epoch: 1 [51200/60000 (85%)]\tLoss: 1.120059\nTrain Epoch: 1 [54400/60000 (91%)]\tLoss: 0.829369\nTrain Epoch: 1 [57600/60000 (96%)]\tLoss: 0.721658\nTrain Epoch: 2 [0/60000 (0%)]\tLoss: 0.775172\nTrain Epoch: 2 [3200/60000 (5%)]\tLoss: 0.975825\nTrain Epoch: 2 [6400/60000 (11%)]\tLoss: 0.993350\nTrain Epoch: 2 [9600/60000 (16%)]\tLoss: 1.405437\nTrain Epoch: 2 [12800/60000 (21%)]\tLoss: 0.899158\nTrain Epoch: 2 [16000/60000 (27%)]\tLoss: 1.151344\nTrain Epoch: 2 [19200/60000 (32%)]\tLoss: 0.902632\nTrain Epoch: 2 [22400/60000 (37%)]\tLoss: 1.821064\nTrain Epoch: 2 [25600/60000 (43%)]\tLoss: 0.962480\nTrain Epoch: 2 [28800/60000 (48%)]\tLoss: 1.061507\nTrain Epoch: 2 [32000/60000 (53%)]\tLoss: 1.278113\nTrain Epoch: 2 [35200/60000 (59%)]\tLoss: 0.927010\nTrain Epoch: 2 [38400/60000 (64%)]\tLoss: 0.974235\nTrain Epoch: 2 [41600/60000 (69%)]\tLoss: 1.053996\nTrain Epoch: 2 [44800/60000 (75%)]\tLoss: 0.739277\nTrain Epoch: 2 [48000/60000 (80%)]\tLoss: 0.750613\nTrain Epoch: 2 [51200/60000 (85%)]\tLoss: 0.859195\nTrain Epoch: 2 [54400/60000 (91%)]\tLoss: 1.211353\nTrain Epoch: 2 [57600/60000 (96%)]\tLoss: 0.795715\nTrain Epoch: 3 [0/60000 (0%)]\tLoss: 1.205979\nTrain Epoch: 3 [3200/60000 (5%)]\tLoss: 1.047980\nTrain Epoch: 3 [6400/60000 (11%)]\tLoss: 0.933089\nTrain Epoch: 3 [9600/60000 (16%)]\tLoss: 1.482590\nTrain Epoch: 3 [12800/60000 (21%)]\tLoss: 1.011938\nTrain Epoch: 3 [16000/60000 (27%)]\tLoss: 0.775776\nTrain Epoch: 3 [19200/60000 (32%)]\tLoss: 1.211690\nTrain Epoch: 3 [22400/60000 (37%)]\tLoss: 0.904257\nTrain Epoch: 3 [25600/60000 (43%)]\tLoss: 1.102352\nTrain Epoch: 3 [28800/60000 (48%)]\tLoss: 0.882785\nTrain Epoch: 3 [32000/60000 (53%)]\tLoss: 0.875881\nTrain Epoch: 3 [35200/60000 (59%)]\tLoss: 1.071694\nTrain Epoch: 3 [38400/60000 (64%)]\tLoss: 1.096462\nTrain Epoch: 3 [41600/60000 (69%)]\tLoss: 1.184636\nTrain Epoch: 3 [44800/60000 (75%)]\tLoss: 1.031978\nTrain Epoch: 3 [48000/60000 (80%)]\tLoss: 1.317308\nTrain Epoch: 3 [51200/60000 (85%)]\tLoss: 0.943205\nTrain Epoch: 3 [54400/60000 (91%)]\tLoss: 1.230762\nTrain Epoch: 3 [57600/60000 (96%)]\tLoss: 0.822100\nTrain Epoch: 4 [0/60000 (0%)]\tLoss: 1.077190\nTrain Epoch: 4 [3200/60000 (5%)]\tLoss: 0.936503\nTrain Epoch: 4 [6400/60000 (11%)]\tLoss: 0.821748\nTrain Epoch: 4 [9600/60000 (16%)]\tLoss: 1.356460\nTrain Epoch: 4 [12800/60000 (21%)]\tLoss: 1.075151\nTrain Epoch: 4 [16000/60000 (27%)]\tLoss: 1.063204\nTrain Epoch: 4 [19200/60000 (32%)]\tLoss: 1.716724\nTrain Epoch: 4 [22400/60000 (37%)]\tLoss: 1.470576\nTrain Epoch: 4 [25600/60000 (43%)]\tLoss: 1.234031\nTrain Epoch: 4 [28800/60000 (48%)]\tLoss: 1.089860\nTrain Epoch: 4 [32000/60000 (53%)]\tLoss: 1.251610\nTrain Epoch: 4 [35200/60000 (59%)]\tLoss: 0.791799\nTrain Epoch: 4 [38400/60000 (64%)]\tLoss: 0.745393\nTrain Epoch: 4 [41600/60000 (69%)]\tLoss: 0.932722\nTrain Epoch: 4 [44800/60000 (75%)]\tLoss: 1.002915\nTrain Epoch: 4 [48000/60000 (80%)]\tLoss: 1.055039\nTrain Epoch: 4 [51200/60000 (85%)]\tLoss: 1.005410\nTrain Epoch: 4 [54400/60000 (91%)]\tLoss: 0.966119\nTrain Epoch: 4 [57600/60000 (96%)]\tLoss: 0.832923\nTrain Epoch: 5 [0/60000 (0%)]\tLoss: 0.992388\nTrain Epoch: 5 [3200/60000 (5%)]\tLoss: 1.112072\nTrain Epoch: 5 [6400/60000 (11%)]\tLoss: 1.136824\nTrain Epoch: 5 [9600/60000 (16%)]\tLoss: 1.395138\nTrain Epoch: 5 [12800/60000 (21%)]\tLoss: 0.888680\nTrain Epoch: 5 [16000/60000 (27%)]\tLoss: 0.913624\nTrain Epoch: 5 [19200/60000 (32%)]\tLoss: 0.850774\nTrain Epoch: 5 [22400/60000 (37%)]\tLoss: 1.113031\nTrain Epoch: 5 [25600/60000 (43%)]\tLoss: 0.912433\nTrain Epoch: 5 [28800/60000 (48%)]\tLoss: 0.881838\nTrain Epoch: 5 [32000/60000 (53%)]\tLoss: 1.361618\nTrain Epoch: 5 [35200/60000 (59%)]\tLoss: 0.820254\nTrain Epoch: 5 [38400/60000 (64%)]\tLoss: 1.100415\nTrain Epoch: 5 [41600/60000 (69%)]\tLoss: 0.617996\nTrain Epoch: 5 [44800/60000 (75%)]\tLoss: 1.243987\nTrain Epoch: 5 [48000/60000 (80%)]\tLoss: 0.954995\nTrain Epoch: 5 [51200/60000 (85%)]\tLoss: 0.818335\nTrain Epoch: 5 [54400/60000 (91%)]\tLoss: 0.988921\nTrain Epoch: 5 [57600/60000 (96%)]\tLoss: 1.138756\nTrain Epoch: 6 [0/60000 (0%)]\tLoss: 1.403925\nTrain Epoch: 6 [3200/60000 (5%)]\tLoss: 0.771945\nTrain Epoch: 6 [6400/60000 (11%)]\tLoss: 0.871269\nTrain Epoch: 6 [9600/60000 (16%)]\tLoss: 0.819527\nTrain Epoch: 6 [12800/60000 (21%)]\tLoss: 0.720687\nTrain Epoch: 6 [16000/60000 (27%)]\tLoss: 0.805354\nTrain Epoch: 6 [19200/60000 (32%)]\tLoss: 1.215445\nTrain Epoch: 6 [22400/60000 (37%)]\tLoss: 0.924986\nTrain Epoch: 6 [25600/60000 (43%)]\tLoss: 0.813246\nTrain Epoch: 6 [28800/60000 (48%)]\tLoss: 1.029393\nTrain Epoch: 6 [32000/60000 (53%)]\tLoss: 0.807215\nTrain Epoch: 6 [35200/60000 (59%)]\tLoss: 1.182888\nTrain Epoch: 6 [38400/60000 (64%)]\tLoss: 1.515429\nTrain Epoch: 6 [41600/60000 (69%)]\tLoss: 0.655966\nTrain Epoch: 6 [44800/60000 (75%)]\tLoss: 0.913739\nTrain Epoch: 6 [48000/60000 (80%)]\tLoss: 0.578605\nTrain Epoch: 6 [51200/60000 (85%)]\tLoss: 0.910598\nTrain Epoch: 6 [54400/60000 (91%)]\tLoss: 1.286528\nTrain Epoch: 6 [57600/60000 (96%)]\tLoss: 0.690669\nTrain Epoch: 7 [0/60000 (0%)]\tLoss: 0.770200\nTrain Epoch: 7 [3200/60000 (5%)]\tLoss: 0.844138\nTrain Epoch: 7 [6400/60000 (11%)]\tLoss: 0.761577\nTrain Epoch: 7 [9600/60000 (16%)]\tLoss: 1.680434\nTrain Epoch: 7 [12800/60000 (21%)]\tLoss: 0.950974\nTrain Epoch: 7 [16000/60000 (27%)]\tLoss: 0.794620\nTrain Epoch: 7 [19200/60000 (32%)]\tLoss: 1.179558\nTrain Epoch: 7 [22400/60000 (37%)]\tLoss: 0.885666\nTrain Epoch: 7 [25600/60000 (43%)]\tLoss: 0.740741\nTrain Epoch: 7 [28800/60000 (48%)]\tLoss: 0.816225\nTrain Epoch: 7 [32000/60000 (53%)]\tLoss: 0.940559\nTrain Epoch: 7 [35200/60000 (59%)]\tLoss: 1.385345\nTrain Epoch: 7 [38400/60000 (64%)]\tLoss: 1.250580\nTrain Epoch: 7 [41600/60000 (69%)]\tLoss: 0.785304\nTrain Epoch: 7 [44800/60000 (75%)]\tLoss: 1.152100\nTrain Epoch: 7 [48000/60000 (80%)]\tLoss: 0.712901\nTrain Epoch: 7 [51200/60000 (85%)]\tLoss: 0.622097\nTrain Epoch: 7 [54400/60000 (91%)]\tLoss: 1.178175\nTrain Epoch: 7 [57600/60000 (96%)]\tLoss: 1.378611\nTrain Epoch: 8 [0/60000 (0%)]\tLoss: 0.983257\nTrain Epoch: 8 [3200/60000 (5%)]\tLoss: 0.792742\nTrain Epoch: 8 [6400/60000 (11%)]\tLoss: 0.769253\nTrain Epoch: 8 [9600/60000 (16%)]\tLoss: 1.310156\nTrain Epoch: 8 [12800/60000 (21%)]\tLoss: 0.865200\nTrain Epoch: 8 [16000/60000 (27%)]\tLoss: 0.710945\nTrain Epoch: 8 [19200/60000 (32%)]\tLoss: 1.161991\nTrain Epoch: 8 [22400/60000 (37%)]\tLoss: 1.044387\nTrain Epoch: 8 [25600/60000 (43%)]\tLoss: 1.397429\nTrain Epoch: 8 [28800/60000 (48%)]\tLoss: 1.063408\nTrain Epoch: 8 [32000/60000 (53%)]\tLoss: 1.130605\nTrain Epoch: 8 [35200/60000 (59%)]\tLoss: 0.856220\nTrain Epoch: 8 [38400/60000 (64%)]\tLoss: 0.938741\nTrain Epoch: 8 [41600/60000 (69%)]\tLoss: 2.088273\nTrain Epoch: 8 [44800/60000 (75%)]\tLoss: 1.320642\nTrain Epoch: 8 [48000/60000 (80%)]\tLoss: 0.673828\nTrain Epoch: 8 [51200/60000 (85%)]\tLoss: 0.758273\nTrain Epoch: 8 [54400/60000 (91%)]\tLoss: 0.882033\nTrain Epoch: 8 [57600/60000 (96%)]\tLoss: 1.207307\nTrain Epoch: 9 [0/60000 (0%)]\tLoss: 1.250788\nTrain Epoch: 9 [3200/60000 (5%)]\tLoss: 1.160486\nTrain Epoch: 9 [6400/60000 (11%)]\tLoss: 0.785669\nTrain Epoch: 9 [9600/60000 (16%)]\tLoss: 0.804436\nTrain Epoch: 9 [12800/60000 (21%)]\tLoss: 1.037995\nTrain Epoch: 9 [16000/60000 (27%)]\tLoss: 0.805331\nTrain Epoch: 9 [19200/60000 (32%)]\tLoss: 1.015645\nTrain Epoch: 9 [22400/60000 (37%)]\tLoss: 0.773032\nTrain Epoch: 9 [25600/60000 (43%)]\tLoss: 0.615527\nTrain Epoch: 9 [28800/60000 (48%)]\tLoss: 0.811075\nTrain Epoch: 9 [32000/60000 (53%)]\tLoss: 1.051100\nTrain Epoch: 9 [35200/60000 (59%)]\tLoss: 0.860456\nTrain Epoch: 9 [38400/60000 (64%)]\tLoss: 0.384371\nTrain Epoch: 9 [41600/60000 (69%)]\tLoss: 1.043506\nTrain Epoch: 9 [44800/60000 (75%)]\tLoss: 1.163232\nTrain Epoch: 9 [48000/60000 (80%)]\tLoss: 1.223855\nTrain Epoch: 9 [51200/60000 (85%)]\tLoss: 0.943369\nTrain Epoch: 9 [54400/60000 (91%)]\tLoss: 1.158034\nTrain Epoch: 9 [57600/60000 (96%)]\tLoss: 1.224279\nTrain Epoch: 10 [0/60000 (0%)]\tLoss: 0.955542\nTrain Epoch: 10 [3200/60000 (5%)]\tLoss: 1.389231\nTrain Epoch: 10 [6400/60000 (11%)]\tLoss: 0.947471\nTrain Epoch: 10 [9600/60000 (16%)]\tLoss: 0.832508\nTrain Epoch: 10 [12800/60000 (21%)]\tLoss: 1.732301\nTrain Epoch: 10 [16000/60000 (27%)]\tLoss: 0.993642\nTrain Epoch: 10 [19200/60000 (32%)]\tLoss: 0.845107\nTrain Epoch: 10 [22400/60000 (37%)]\tLoss: 0.893238\nTrain Epoch: 10 [25600/60000 (43%)]\tLoss: 0.900986\nTrain Epoch: 10 [28800/60000 (48%)]\tLoss: 0.834590\nTrain Epoch: 10 [32000/60000 (53%)]\tLoss: 0.763298\nTrain Epoch: 10 [35200/60000 (59%)]\tLoss: 0.898613\nTrain Epoch: 10 [38400/60000 (64%)]\tLoss: 0.961223\nTrain Epoch: 10 [41600/60000 (69%)]\tLoss: 1.007339\nTrain Epoch: 10 [44800/60000 (75%)]\tLoss: 1.119481\nTrain Epoch: 10 [48000/60000 (80%)]\tLoss: 0.745059\nTrain Epoch: 10 [51200/60000 (85%)]\tLoss: 0.830305\nTrain Epoch: 10 [54400/60000 (91%)]\tLoss: 1.386109\nTrain Epoch: 10 [57600/60000 (96%)]\tLoss: 1.086295\nTrain Epoch: 11 [0/60000 (0%)]\tLoss: 0.634647\nTrain Epoch: 11 [3200/60000 (5%)]\tLoss: 0.836453\nTrain Epoch: 11 [6400/60000 (11%)]\tLoss: 0.923910\nTrain Epoch: 11 [9600/60000 (16%)]\tLoss: 1.007622\nTrain Epoch: 11 [12800/60000 (21%)]\tLoss: 0.995869\nTrain Epoch: 11 [16000/60000 (27%)]\tLoss: 1.110815\nTrain Epoch: 11 [19200/60000 (32%)]\tLoss: 0.718137\nTrain Epoch: 11 [22400/60000 (37%)]\tLoss: 1.180480\nTrain Epoch: 11 [25600/60000 (43%)]\tLoss: 1.081848\nTrain Epoch: 11 [28800/60000 (48%)]\tLoss: 0.792830\nTrain Epoch: 11 [32000/60000 (53%)]\tLoss: 0.644760\nTrain Epoch: 11 [35200/60000 (59%)]\tLoss: 0.560990\nTrain Epoch: 11 [38400/60000 (64%)]\tLoss: 0.689144\nTrain Epoch: 11 [41600/60000 (69%)]\tLoss: 1.134218\nTrain Epoch: 11 [44800/60000 (75%)]\tLoss: 0.610044\nTrain Epoch: 11 [48000/60000 (80%)]\tLoss: 1.715760\nTrain Epoch: 11 [51200/60000 (85%)]\tLoss: 0.781585\nTrain Epoch: 11 [54400/60000 (91%)]\tLoss: 0.999184\nTrain Epoch: 11 [57600/60000 (96%)]\tLoss: 0.905494\nTrain Epoch: 12 [0/60000 (0%)]\tLoss: 0.969722\nTrain Epoch: 12 [3200/60000 (5%)]\tLoss: 0.920955\nTrain Epoch: 12 [6400/60000 (11%)]\tLoss: 0.772392\nTrain Epoch: 12 [9600/60000 (16%)]\tLoss: 1.264042\nTrain Epoch: 12 [12800/60000 (21%)]\tLoss: 0.954187\nTrain Epoch: 12 [16000/60000 (27%)]\tLoss: 0.591757\nTrain Epoch: 12 [19200/60000 (32%)]\tLoss: 0.855580\nTrain Epoch: 12 [22400/60000 (37%)]\tLoss: 1.002626\nTrain Epoch: 12 [25600/60000 (43%)]\tLoss: 1.231830\nTrain Epoch: 12 [28800/60000 (48%)]\tLoss: 1.243482\nTrain Epoch: 12 [32000/60000 (53%)]\tLoss: 0.700764\nTrain Epoch: 12 [35200/60000 (59%)]\tLoss: 0.891650\nTrain Epoch: 12 [38400/60000 (64%)]\tLoss: 0.706239\nTrain Epoch: 12 [41600/60000 (69%)]\tLoss: 1.434680\nTrain Epoch: 12 [44800/60000 (75%)]\tLoss: 0.888867\nTrain Epoch: 12 [48000/60000 (80%)]\tLoss: 0.917048\nTrain Epoch: 12 [51200/60000 (85%)]\tLoss: 1.222978\nTrain Epoch: 12 [54400/60000 (91%)]\tLoss: 1.137347\nTrain Epoch: 12 [57600/60000 (96%)]\tLoss: 1.450256\nTrain Epoch: 13 [0/60000 (0%)]\tLoss: 0.967024\nTrain Epoch: 13 [3200/60000 (5%)]\tLoss: 1.030789\nTrain Epoch: 13 [6400/60000 (11%)]\tLoss: 1.295430\nTrain Epoch: 13 [9600/60000 (16%)]\tLoss: 0.871987\nTrain Epoch: 13 [12800/60000 (21%)]\tLoss: 1.047137\nTrain Epoch: 13 [16000/60000 (27%)]\tLoss: 0.986406\nTrain Epoch: 13 [19200/60000 (32%)]\tLoss: 0.814248\nTrain Epoch: 13 [22400/60000 (37%)]\tLoss: 0.969713\nTrain Epoch: 13 [25600/60000 (43%)]\tLoss: 0.729157\nTrain Epoch: 13 [28800/60000 (48%)]\tLoss: 1.168123\nTrain Epoch: 13 [32000/60000 (53%)]\tLoss: 1.074293\nTrain Epoch: 13 [35200/60000 (59%)]\tLoss: 1.508831\nTrain Epoch: 13 [38400/60000 (64%)]\tLoss: 1.126747\nTrain Epoch: 13 [41600/60000 (69%)]\tLoss: 0.927935\nTrain Epoch: 13 [44800/60000 (75%)]\tLoss: 1.103164\nTrain Epoch: 13 [48000/60000 (80%)]\tLoss: 0.693556\nTrain Epoch: 13 [51200/60000 (85%)]\tLoss: 1.071212\nTrain Epoch: 13 [54400/60000 (91%)]\tLoss: 1.252457\nTrain Epoch: 13 [57600/60000 (96%)]\tLoss: 0.744302\nTrain Epoch: 14 [0/60000 (0%)]\tLoss: 1.379637\nTrain Epoch: 14 [3200/60000 (5%)]\tLoss: 0.792607\nTrain Epoch: 14 [6400/60000 (11%)]\tLoss: 1.012426\nTrain Epoch: 14 [9600/60000 (16%)]\tLoss: 0.947124\nTrain Epoch: 14 [12800/60000 (21%)]\tLoss: 1.102205\nTrain Epoch: 14 [16000/60000 (27%)]\tLoss: 0.722943\nTrain Epoch: 14 [19200/60000 (32%)]\tLoss: 0.992637\nTrain Epoch: 14 [22400/60000 (37%)]\tLoss: 1.336712\nTrain Epoch: 14 [25600/60000 (43%)]\tLoss: 0.848735\nTrain Epoch: 14 [28800/60000 (48%)]\tLoss: 0.857499\nTrain Epoch: 14 [32000/60000 (53%)]\tLoss: 0.855187\nTrain Epoch: 14 [35200/60000 (59%)]\tLoss: 1.014216\nTrain Epoch: 14 [38400/60000 (64%)]\tLoss: 0.945315\nTrain Epoch: 14 [41600/60000 (69%)]\tLoss: 0.650733\nTrain Epoch: 14 [44800/60000 (75%)]\tLoss: 1.229133\nTrain Epoch: 14 [48000/60000 (80%)]\tLoss: 1.364087\nTrain Epoch: 14 [51200/60000 (85%)]\tLoss: 0.921818\nTrain Epoch: 14 [54400/60000 (91%)]\tLoss: 1.030675\nTrain Epoch: 14 [57600/60000 (96%)]\tLoss: 0.794336\nTrain Epoch: 15 [0/60000 (0%)]\tLoss: 0.840845\nTrain Epoch: 15 [3200/60000 (5%)]\tLoss: 1.686122\nTrain Epoch: 15 [6400/60000 (11%)]\tLoss: 1.372050\nTrain Epoch: 15 [9600/60000 (16%)]\tLoss: 0.975734\nTrain Epoch: 15 [12800/60000 (21%)]\tLoss: 0.989867\nTrain Epoch: 15 [16000/60000 (27%)]\tLoss: 1.339431\nTrain Epoch: 15 [19200/60000 (32%)]\tLoss: 0.914139\nTrain Epoch: 15 [22400/60000 (37%)]\tLoss: 1.107859\nTrain Epoch: 15 [25600/60000 (43%)]\tLoss: 0.967759\nTrain Epoch: 15 [28800/60000 (48%)]\tLoss: 1.080097\nTrain Epoch: 15 [32000/60000 (53%)]\tLoss: 0.869940\nTrain Epoch: 15 [35200/60000 (59%)]\tLoss: 1.002007\nTrain Epoch: 15 [38400/60000 (64%)]\tLoss: 0.903447\nTrain Epoch: 15 [41600/60000 (69%)]\tLoss: 0.903836\nTrain Epoch: 15 [44800/60000 (75%)]\tLoss: 0.692108\nTrain Epoch: 15 [48000/60000 (80%)]\tLoss: 1.322002\nTrain Epoch: 15 [51200/60000 (85%)]\tLoss: 0.998504\nTrain Epoch: 15 [54400/60000 (91%)]\tLoss: 0.743107\nTrain Epoch: 15 [57600/60000 (96%)]\tLoss: 0.859381\nTrain Epoch: 16 [0/60000 (0%)]\tLoss: 0.793166\nTrain Epoch: 16 [3200/60000 (5%)]\tLoss: 1.267776\nTrain Epoch: 16 [6400/60000 (11%)]\tLoss: 0.881496\nTrain Epoch: 16 [9600/60000 (16%)]\tLoss: 0.630358\nTrain Epoch: 16 [12800/60000 (21%)]\tLoss: 0.985188\nTrain Epoch: 16 [16000/60000 (27%)]\tLoss: 0.760136\nTrain Epoch: 16 [19200/60000 (32%)]\tLoss: 0.747839\nTrain Epoch: 16 [22400/60000 (37%)]\tLoss: 0.748285\nTrain Epoch: 16 [25600/60000 (43%)]\tLoss: 0.746582\nTrain Epoch: 16 [28800/60000 (48%)]\tLoss: 0.987383\nTrain Epoch: 16 [32000/60000 (53%)]\tLoss: 1.176063\nTrain Epoch: 16 [35200/60000 (59%)]\tLoss: 1.097143\nTrain Epoch: 16 [38400/60000 (64%)]\tLoss: 1.021680\nTrain Epoch: 16 [41600/60000 (69%)]\tLoss: 0.700033\nTrain Epoch: 16 [44800/60000 (75%)]\tLoss: 0.825742\nTrain Epoch: 16 [48000/60000 (80%)]\tLoss: 1.294810\nTrain Epoch: 16 [51200/60000 (85%)]\tLoss: 0.855803\nTrain Epoch: 16 [54400/60000 (91%)]\tLoss: 0.652467\nTrain Epoch: 16 [57600/60000 (96%)]\tLoss: 1.294417\nTrain Epoch: 17 [0/60000 (0%)]\tLoss: 0.886119\nTrain Epoch: 17 [3200/60000 (5%)]\tLoss: 1.764886\nTrain Epoch: 17 [6400/60000 (11%)]\tLoss: 0.811992\nTrain Epoch: 17 [9600/60000 (16%)]\tLoss: 1.156330\nTrain Epoch: 17 [12800/60000 (21%)]\tLoss: 1.283693\nTrain Epoch: 17 [16000/60000 (27%)]\tLoss: 0.946574\nTrain Epoch: 17 [19200/60000 (32%)]\tLoss: 1.079442\nTrain Epoch: 17 [22400/60000 (37%)]\tLoss: 0.719215\nTrain Epoch: 17 [25600/60000 (43%)]\tLoss: 0.879068\nTrain Epoch: 17 [28800/60000 (48%)]\tLoss: 0.999480\nTrain Epoch: 17 [32000/60000 (53%)]\tLoss: 0.967303\nTrain Epoch: 17 [35200/60000 (59%)]\tLoss: 0.668322\nTrain Epoch: 17 [38400/60000 (64%)]\tLoss: 0.750904\nTrain Epoch: 17 [41600/60000 (69%)]\tLoss: 0.829480\nTrain Epoch: 17 [44800/60000 (75%)]\tLoss: 0.768210\nTrain Epoch: 17 [48000/60000 (80%)]\tLoss: 1.119552\nTrain Epoch: 17 [51200/60000 (85%)]\tLoss: 0.669064\nTrain Epoch: 17 [54400/60000 (91%)]\tLoss: 1.124795\nTrain Epoch: 17 [57600/60000 (96%)]\tLoss: 0.689165\nTrain Epoch: 18 [0/60000 (0%)]\tLoss: 1.168346\nTrain Epoch: 18 [3200/60000 (5%)]\tLoss: 0.889254\nTrain Epoch: 18 [6400/60000 (11%)]\tLoss: 0.894667\nTrain Epoch: 18 [9600/60000 (16%)]\tLoss: 0.993654\nTrain Epoch: 18 [12800/60000 (21%)]\tLoss: 1.246198\nTrain Epoch: 18 [16000/60000 (27%)]\tLoss: 0.742207\nTrain Epoch: 18 [19200/60000 (32%)]\tLoss: 1.022499\nTrain Epoch: 18 [22400/60000 (37%)]\tLoss: 0.953169\nTrain Epoch: 18 [25600/60000 (43%)]\tLoss: 0.812642\nTrain Epoch: 18 [28800/60000 (48%)]\tLoss: 0.828113\nTrain Epoch: 18 [32000/60000 (53%)]\tLoss: 0.833560\nTrain Epoch: 18 [35200/60000 (59%)]\tLoss: 1.110223\nTrain Epoch: 18 [38400/60000 (64%)]\tLoss: 1.140273\nTrain Epoch: 18 [41600/60000 (69%)]\tLoss: 0.872436\nTrain Epoch: 18 [44800/60000 (75%)]\tLoss: 0.890928\nTrain Epoch: 18 [48000/60000 (80%)]\tLoss: 0.509080\nTrain Epoch: 18 [51200/60000 (85%)]\tLoss: 1.271792\nTrain Epoch: 18 [54400/60000 (91%)]\tLoss: 1.140503\nTrain Epoch: 18 [57600/60000 (96%)]\tLoss: 0.986190\nTrain Epoch: 19 [0/60000 (0%)]\tLoss: 1.450598\nTrain Epoch: 19 [3200/60000 (5%)]\tLoss: 0.794300\nTrain Epoch: 19 [6400/60000 (11%)]\tLoss: 1.328353\nTrain Epoch: 19 [9600/60000 (16%)]\tLoss: 0.841129\nTrain Epoch: 19 [12800/60000 (21%)]\tLoss: 0.930606\nTrain Epoch: 19 [16000/60000 (27%)]\tLoss: 1.543601\nTrain Epoch: 19 [19200/60000 (32%)]\tLoss: 1.487835\nTrain Epoch: 19 [22400/60000 (37%)]\tLoss: 1.019675\nTrain Epoch: 19 [25600/60000 (43%)]\tLoss: 1.362598\nTrain Epoch: 19 [28800/60000 (48%)]\tLoss: 0.998519\nTrain Epoch: 19 [32000/60000 (53%)]\tLoss: 0.868672\nTrain Epoch: 19 [35200/60000 (59%)]\tLoss: 0.724700\nTrain Epoch: 19 [38400/60000 (64%)]\tLoss: 0.817852\nTrain Epoch: 19 [41600/60000 (69%)]\tLoss: 0.676260\nTrain Epoch: 19 [44800/60000 (75%)]\tLoss: 0.682136\nTrain Epoch: 19 [48000/60000 (80%)]\tLoss: 1.061193\nTrain Epoch: 19 [51200/60000 (85%)]\tLoss: 0.710633\nTrain Epoch: 19 [54400/60000 (91%)]\tLoss: 0.661032\nTrain Epoch: 19 [57600/60000 (96%)]\tLoss: 0.764810\nTrain Epoch: 20 [0/60000 (0%)]\tLoss: 1.104778\nTrain Epoch: 20 [3200/60000 (5%)]\tLoss: 0.791256\nTrain Epoch: 20 [6400/60000 (11%)]\tLoss: 1.109865\nTrain Epoch: 20 [9600/60000 (16%)]\tLoss: 1.196493\nTrain Epoch: 20 [12800/60000 (21%)]\tLoss: 1.130605\nTrain Epoch: 20 [16000/60000 (27%)]\tLoss: 1.301185\nTrain Epoch: 20 [19200/60000 (32%)]\tLoss: 0.611422\nTrain Epoch: 20 [22400/60000 (37%)]\tLoss: 0.875233\nTrain Epoch: 20 [25600/60000 (43%)]\tLoss: 0.766816\nTrain Epoch: 20 [28800/60000 (48%)]\tLoss: 0.729236\nTrain Epoch: 20 [32000/60000 (53%)]\tLoss: 1.429911\nTrain Epoch: 20 [35200/60000 (59%)]\tLoss: 1.290803\nTrain Epoch: 20 [38400/60000 (64%)]\tLoss: 0.883053\nTrain Epoch: 20 [41600/60000 (69%)]\tLoss: 0.799179\nTrain Epoch: 20 [44800/60000 (75%)]\tLoss: 1.045050\nTrain Epoch: 20 [48000/60000 (80%)]\tLoss: 1.015107\nTrain Epoch: 20 [51200/60000 (85%)]\tLoss: 0.954014\nTrain Epoch: 20 [54400/60000 (91%)]\tLoss: 1.033333\nTrain Epoch: 20 [57600/60000 (96%)]\tLoss: 0.780848\n"]}}],"execution_count":0},{"cell_type":"code","source":["test(single_node_log_dir)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f0099b21-90e1-4af4-8743-7bc3a84ea583","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Average test loss: 0.7281309366226196\nAccuracy of the network on the test images:  71 %\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Average test loss: 0.7281309366226196\nAccuracy of the network on the test images:  71 %\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Migrating to HorovodRunner\nHorovodRunner takes a Python method that contains deep learning training code with Horovod hooks. HorovodRunner pickles the method on the driver and distributes it to Spark workers. A Horovod MPI job is embedded as a Spark job using barrier execution mode."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"73431f87-7152-4b23-8aa4-597ecfd33103","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import horovod.torch as hvd\nfrom sparkdl import HorovodRunner"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"56544b18-e484-4f79-9b80-b5dfe43aa63e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["hvd_log_dir = create_log_dir()\nprint('Log directory:', hvd_log_dir)\n \ndef train_hvd(learning_rate):\n  \n    # Initialize Horovod\n    hvd.init()  \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n  \n    if device.type == 'cuda':\n       # Pin GPU to local rank\n       torch.cuda.set_device(hvd.local_rank())\n \n    train_dataset = datasets.FashionMNIST(\n        # Use different root directory for each worker to avoid conflicts\n        root = 'data-%d'% hvd.rank(),  \n        train = True, \n        download = True,\n        transform = transforms.Compose([transforms.ToTensor()])\n    )\n \n    from torch.utils.data.distributed import DistributedSampler\n  \n    # Configure the sampler so that each worker gets a distinct sample of the input dataset\n    train_sampler = DistributedSampler(train_dataset, num_replicas = hvd.size(), rank = hvd.rank())\n    \n    # Use train_sampler to load a different sample of data on each worker\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, sampler = train_sampler)\n \n    model = Net().to(device)\n    optimizer = optim.Adam(model.parameters(), lr = 0.02)\n \n    # Wrap the local optimizer with hvd.DistributedOptimizer so that Horovod handles the distributed optimization\n    optimizer = hvd.DistributedOptimizer(optimizer, named_parameters = model.named_parameters())\n  \n    # Broadcast initial parameters so all workers start with the same parameters\n    hvd.broadcast_parameters(model.state_dict(), root_rank = 0)\n \n    for epoch in range(1, num_epochs + 1):\n        \n        train_one_epoch(model, device, train_loader, optimizer, epoch)\n        \n        # Save checkpoints only on worker 0 to prevent conflicts between workers\n        if hvd.rank() == 0:\n            save_checkpoint(hvd_log_dir, model, optimizer, epoch)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d1e2b287-d9f4-4625-a868-e1b4825e590a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Log directory: /dbfs/ml/horovod_pytorch/1668000582.9829545/FashionMNISTDemo\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Log directory: /dbfs/ml/horovod_pytorch/1668000582.9829545/FashionMNISTDemo\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Now that you have defined a training function with Horovod, you can use HorovodRunner to distribute the work of training the model.\n\nThe HorovodRunner parameter np sets the number of processes. This example uses a cluster with two workers, each with a single GPU, so set np=2. (If you use np=-1, HorovodRunner trains using a single process on the driver node.)\nNote that it takes nearly half(2.6 mins) of the time that of  single node training(4.9 mins)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c3bbc35e-6eff-4c44-b169-e5db2d72263d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["hr = HorovodRunner(np = 2)\n\nhr.run(train_hvd, learning_rate = 0.02)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3dacdce9-52f4-405f-81a1-605ed196ea86","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"HorovodRunner will only stream logs generated by :func:`sparkdl.horovod.log_to_driver` or\n:class:`sparkdl.horovod.tensorflow.keras.LogCallback` to notebook cell output. If want to stream all\nlogs to driver for debugging, you can set driver_log_verbosity to 'all', like `HorovodRunner(np=2,\ndriver_log_verbosity='all')`.\nThe global names read or written to by the pickled function are {'hvd': None, 'torch': None, 'datasets': None, 'transforms': None, 'batch_size': None, 'Net': None, 'optim': None, 'range': None, 'num_epochs': None, 'train_one_epoch': None, 'save_checkpoint': None, 'hvd_log_dir': None}.\nThe pickled object size is 4895 bytes.\n\n### How to enable Horovod Timeline? ###\nHorovodRunner has the ability to record the timeline of its activity with Horovod  Timeline. To\nrecord a Horovod Timeline, set the `HOROVOD_TIMELINE` environment variable  to the location of the\ntimeline file to be created. You can then open the timeline file  using the chrome://tracing\nfacility of the Chrome browser.\n\n/databricks/spark/python/pyspark/sql/context.py:117: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\nStart training.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["HorovodRunner will only stream logs generated by :func:`sparkdl.horovod.log_to_driver` or\n:class:`sparkdl.horovod.tensorflow.keras.LogCallback` to notebook cell output. If want to stream all\nlogs to driver for debugging, you can set driver_log_verbosity to 'all', like `HorovodRunner(np=2,\ndriver_log_verbosity='all')`.\nThe global names read or written to by the pickled function are {'hvd': None, 'torch': None, 'datasets': None, 'transforms': None, 'batch_size': None, 'Net': None, 'optim': None, 'range': None, 'num_epochs': None, 'train_one_epoch': None, 'save_checkpoint': None, 'hvd_log_dir': None}.\nThe pickled object size is 4895 bytes.\n\n### How to enable Horovod Timeline? ###\nHorovodRunner has the ability to record the timeline of its activity with Horovod  Timeline. To\nrecord a Horovod Timeline, set the `HOROVOD_TIMELINE` environment variable  to the location of the\ntimeline file to be created. You can then open the timeline file  using the chrome://tracing\nfacility of the Chrome browser.\n\n/databricks/spark/python/pyspark/sql/context.py:117: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\nStart training.\n"]}}],"execution_count":0},{"cell_type":"code","source":["test(hvd_log_dir)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"68a56087-119f-4e97-808e-bee3e47b46ac","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Average test loss: 0.6396099925041199\nAccuracy of the network on the test images:  74 %\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Average test loss: 0.6396099925041199\nAccuracy of the network on the test images:  74 %\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Under the hood, HorovodRunner takes a Python method that contains deep learning training code with Horovod hooks. HorovodRunner pickles the method on the driver and distributes it to Spark workers. A Horovod MPI job is embedded as a Spark job using the barrier execution mode. The first executor collects the IP addresses of all task executors using BarrierTaskContext and triggers a Horovod job using mpirun. Each Python MPI process loads the pickled user program, deserializes it, and runs it."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"191c2943-931f-4807-bb95-dd6969c943f6","inputWidgets":{},"title":""}}},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"89344936-fa7b-4242-9180-adc5a88d2bd6","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c6956a70-685f-4bb9-b057-9d06cac21a87","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b3fbe7ee-2c73-40ab-af2f-94eaf43a240e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a97eeb5a-0e7a-4e6c-8a13-9ecb2b38d86e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2d0802ab-962a-460b-9934-a5ee943e67a2","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"86958903-c0ff-48d0-b2a1-dfed6549ac87","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.8.8","nbconvert_exporter":"python","file_extension":".py"},"application/vnd.databricks.v1+notebook":{"notebookName":"demo-05-DistributedTrainingInPyTorchForImageClassification","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1400016144011884}},"nbformat":4,"nbformat_minor":0}
