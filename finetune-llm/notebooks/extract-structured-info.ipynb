{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "import re\n",
    "from artefacts import DATASETS_PATH, GLOBAL_CLIMATE_CHANGE_LITIGATION_PATH\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import create_extraction_chain\n",
    "from langchain.llms import LlamaCpp\n",
    "\n",
    "from langchain.chat_models import ChatAnthropic, ChatOllama\n",
    "\n",
    "# from huggingface_hub import login\n",
    "# login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "\n",
    "MAX_CONTEXT_LENGTH = 4096\n",
    "\n",
    "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "# llm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Llama-2-7b-Chat-GGUF\", model_file=\"llama-2-7b-chat.q4_K_M.gguf\", model_type=\"llama\", gpu_layers=30, context_length=MAX_CONTEXT_LENGTH)\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/home/clem/models/TheBloke/llama-2-7b-chat.q4_K_M.gguf\",\n",
    "    n_ctx=MAX_CONTEXT_LENGTH,\n",
    "    n_gpu_layers=1,\n",
    "    n_batch=512, # Number of tokens to process in parallel Should be a number between 1 and n_ctx\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "llm(\"Question: In bash, how do I list all the text files in the current directory that have been modified in the last month? Answer:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Llama-2-7b-Chat-GPTQ](https://huggingface.co/TheBloke/Llama-2-7b-Chat-GPTQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\n",
    "\n",
    "# Use a [fast Rust-based tokenizer](https://huggingface.co/docs/tokenizers/index) if it is supported for\n",
    "# a given model. If a fast tokenizer is not available for a given model, a normal Python-based tokenizer\n",
    "# is returned instead.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, revision=\"main\", trust_remote_code=False, device_map=\"auto\")\n",
    "\n",
    "pipe_llm = pipeline(\n",
    "    \"text-generation\", # text2text-generation,  text-generation, summarization\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama = ChatOllama(model=\"llama2:7b-chat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ########## \n",
      "<class 'langchain.schema.output.ChatGeneration'>\n"
     ]
    },
    {
     "ename": "OutputParserException",
     "evalue": "Could not parse function call: 'function_call'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Source/sandbox/finetune-llm/.venv/lib/python3.10/site-packages/langchain/output_parsers/openai_functions.py:33\u001b[0m, in \u001b[0;36mOutputFunctionsParser.parse_result\u001b[0;34m(self, result)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     func_call \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(message\u001b[39m.\u001b[39;49madditional_kwargs[\u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m     34\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'function_call'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m/home/clem/Source/sandbox/finetune-llm/notebooks/extract-structured-info.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/clem/Source/sandbox/finetune-llm/notebooks/extract-structured-info.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m inp \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\u001b[39mAlex is 5 feet tall. Claudia is 1 feet taller Alex and jumps higher than him. Claudia is a brunette and Alex is blonde.\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/clem/Source/sandbox/finetune-llm/notebooks/extract-structured-info.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m chain \u001b[39m=\u001b[39m create_extraction_chain(schema, ollama)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/clem/Source/sandbox/finetune-llm/notebooks/extract-structured-info.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m chain\u001b[39m.\u001b[39;49mrun(inp)\n",
      "File \u001b[0;32m~/Source/sandbox/finetune-llm/.venv/lib/python3.10/site-packages/langchain/chains/base.py:487\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    486\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 487\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m], callbacks\u001b[39m=\u001b[39;49mcallbacks, tags\u001b[39m=\u001b[39;49mtags, metadata\u001b[39m=\u001b[39;49mmetadata)[\n\u001b[1;32m    488\u001b[0m         _output_key\n\u001b[1;32m    489\u001b[0m     ]\n\u001b[1;32m    491\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    492\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs, callbacks\u001b[39m=\u001b[39mcallbacks, tags\u001b[39m=\u001b[39mtags, metadata\u001b[39m=\u001b[39mmetadata)[\n\u001b[1;32m    493\u001b[0m         _output_key\n\u001b[1;32m    494\u001b[0m     ]\n",
      "File \u001b[0;32m~/Source/sandbox/finetune-llm/.venv/lib/python3.10/site-packages/langchain/chains/base.py:292\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    291\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 292\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    293\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    294\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    295\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    296\u001b[0m )\n",
      "File \u001b[0;32m~/Source/sandbox/finetune-llm/.venv/lib/python3.10/site-packages/langchain/chains/base.py:286\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    279\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    280\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    281\u001b[0m     inputs,\n\u001b[1;32m    282\u001b[0m     name\u001b[39m=\u001b[39mrun_name,\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    284\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    285\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 286\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    287\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    288\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    289\u001b[0m     )\n\u001b[1;32m    290\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    291\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/Source/sandbox/finetune-llm/.venv/lib/python3.10/site-packages/langchain/chains/llm.py:94\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[1;32m     89\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     90\u001b[0m     inputs: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[1;32m     91\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     92\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m     93\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate([inputs], run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[0;32m---> 94\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_outputs(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Source/sandbox/finetune-llm/.venv/lib/python3.10/site-packages/langchain/chains/llm.py:222\u001b[0m, in \u001b[0;36mLLMChain.create_outputs\u001b[0;34m(self, llm_result)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_outputs\u001b[39m(\u001b[39mself\u001b[39m, llm_result: LLMResult) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Dict[\u001b[39mstr\u001b[39m, Any]]:\n\u001b[1;32m    221\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Create outputs from response.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m     result \u001b[39m=\u001b[39m [\n\u001b[1;32m    223\u001b[0m         \u001b[39m# Get the text of the top generated string.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m         {\n\u001b[1;32m    225\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_parser\u001b[39m.\u001b[39mparse_result(generation),\n\u001b[1;32m    226\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mfull_generation\u001b[39m\u001b[39m\"\u001b[39m: generation,\n\u001b[1;32m    227\u001b[0m         }\n\u001b[1;32m    228\u001b[0m         \u001b[39mfor\u001b[39;00m generation \u001b[39min\u001b[39;00m llm_result\u001b[39m.\u001b[39mgenerations\n\u001b[1;32m    229\u001b[0m     ]\n\u001b[1;32m    230\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_final_only:\n\u001b[1;32m    231\u001b[0m         result \u001b[39m=\u001b[39m [{\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key: r[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key]} \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m result]\n",
      "File \u001b[0;32m~/Source/sandbox/finetune-llm/.venv/lib/python3.10/site-packages/langchain/chains/llm.py:225\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_outputs\u001b[39m(\u001b[39mself\u001b[39m, llm_result: LLMResult) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Dict[\u001b[39mstr\u001b[39m, Any]]:\n\u001b[1;32m    221\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Create outputs from response.\"\"\"\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     result \u001b[39m=\u001b[39m [\n\u001b[1;32m    223\u001b[0m         \u001b[39m# Get the text of the top generated string.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m         {\n\u001b[0;32m--> 225\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_parser\u001b[39m.\u001b[39;49mparse_result(generation),\n\u001b[1;32m    226\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mfull_generation\u001b[39m\u001b[39m\"\u001b[39m: generation,\n\u001b[1;32m    227\u001b[0m         }\n\u001b[1;32m    228\u001b[0m         \u001b[39mfor\u001b[39;00m generation \u001b[39min\u001b[39;00m llm_result\u001b[39m.\u001b[39mgenerations\n\u001b[1;32m    229\u001b[0m     ]\n\u001b[1;32m    230\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_final_only:\n\u001b[1;32m    231\u001b[0m         result \u001b[39m=\u001b[39m [{\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key: r[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key]} \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m result]\n",
      "File \u001b[0;32m~/Source/sandbox/finetune-llm/.venv/lib/python3.10/site-packages/langchain/output_parsers/openai_functions.py:81\u001b[0m, in \u001b[0;36mJsonKeyOutputFunctionsParser.parse_result\u001b[0;34m(self, result)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse_result\u001b[39m(\u001b[39mself\u001b[39m, result: List[Generation]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m---> 81\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mparse_result(result)\n\u001b[1;32m     82\u001b[0m     \u001b[39mreturn\u001b[39;00m res[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_name]\n",
      "File \u001b[0;32m~/Source/sandbox/finetune-llm/.venv/lib/python3.10/site-packages/langchain/output_parsers/openai_functions.py:54\u001b[0m, in \u001b[0;36mJsonOutputFunctionsParser.parse_result\u001b[0;34m(self, result)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse_result\u001b[39m(\u001b[39mself\u001b[39m, result: List[Generation]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m---> 54\u001b[0m     function_call_info \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mparse_result(result)\n\u001b[1;32m     55\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs_only:\n\u001b[1;32m     56\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Source/sandbox/finetune-llm/.venv/lib/python3.10/site-packages/langchain/output_parsers/openai_functions.py:35\u001b[0m, in \u001b[0;36mOutputFunctionsParser.parse_result\u001b[0;34m(self, result)\u001b[0m\n\u001b[1;32m     33\u001b[0m     func_call \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(message\u001b[39m.\u001b[39madditional_kwargs[\u001b[39m\"\u001b[39m\u001b[39mfunction_call\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     34\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m---> 35\u001b[0m     \u001b[39mraise\u001b[39;00m OutputParserException(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not parse function call: \u001b[39m\u001b[39m{\u001b[39;00mexc\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs_only:\n\u001b[1;32m     38\u001b[0m     \u001b[39mreturn\u001b[39;00m func_call[\u001b[39m\"\u001b[39m\u001b[39marguments\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Could not parse function call: 'function_call'"
     ]
    }
   ],
   "source": [
    "schema = {\n",
    "    \"properties\": {\n",
    "        \"name\": {\"type\": \"string\"},\n",
    "        \"height\": {\"type\": \"integer\"},\n",
    "        \"hair_color\": {\"type\": \"string\"},\n",
    "    },\n",
    "    \"required\": [\"name\", \"height\"],\n",
    "}\n",
    "\n",
    "# Input \n",
    "inp = \"\"\"Alex is 5 feet tall. Claudia is 1 feet taller Alex and jumps higher than him. Claudia is a brunette and Alex is blonde.\"\"\"\n",
    "\n",
    "chain = create_extraction_chain(schema, ollama)\n",
    "\n",
    "chain.run(inp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
