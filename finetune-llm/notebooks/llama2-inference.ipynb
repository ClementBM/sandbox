{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BetterTransformer\n",
    "\n",
    "BetterTransformer converts ðŸ¤— Transformers models to use the PyTorch-native fastpath execution, which calls optimized kernels like Flash Attention under the hood.\n",
    "\n",
    "BetterTransformer is also supported for faster inference on single and multi-GPU for text, image, and audio models.\n",
    "\n",
    "# Quantization\n",
    "\n",
    "Using FP4 quantization you can expect to reduce up to 8x the model size compared to its native full precision version\n",
    "\n",
    "# Resources\n",
    "* https://huggingface.co/docs/transformers/perf_infer_gpu_one#requirements-for-fp4-mixedprecision-inference\n",
    "* [Efficient Fine-Tuning for Llama-v2-7b on a Single GPU](https://www.youtube.com/live/g68qlo9Izf0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import re\n",
    "from artefacts import DATASETS_PATH\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "from huggingface_hub import login\n",
    "# login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method LLM.__call__ of <ctransformers.llm.LLM object at 0x7f13f5b4a950>>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "\n",
    "MAX_CONTEXT_LENGTH = 4096\n",
    "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "llm = AutoModelForCausalLM.from_pretrained(\"/home/clem/models/TheBloke/llama-2-7b-chat.q4_K_M.gguf\", model_type=\"llama\", gpu_layers=30, context_length=MAX_CONTEXT_LENGTH)\n",
    "\n",
    "llm.__call__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ']\n",
      "[' ', ' A']\n",
      "[' ', ' A', ' wat']\n",
      "[' ', ' A', ' wat', 'erm']\n",
      "[' ', ' A', ' wat', 'erm', 'el']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and', ' is']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and', ' is', ' known']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and', ' is', ' known', ' for']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and', ' is', ' known', ' for', ' its']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and', ' is', ' known', ' for', ' its', ' high']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and', ' is', ' known', ' for', ' its', ' high', ' water']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and', ' is', ' known', ' for', ' its', ' high', ' water', ' content']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and', ' is', ' known', ' for', ' its', ' high', ' water', ' content', ',']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and', ' is', ' known', ' for', ' its', ' high', ' water', ' content', ',', ' which']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and', ' is', ' known', ' for', ' its', ' high', ' water', ' content', ',', ' which', ' makes']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and', ' is', ' known', ' for', ' its', ' high', ' water', ' content', ',', ' which', ' makes', ' it']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and', ' is', ' known', ' for', ' its', ' high', ' water', ' content', ',', ' which', ' makes', ' it', ' ref']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and', ' is', ' known', ' for', ' its', ' high', ' water', ' content', ',', ' which', ' makes', ' it', ' ref', 'res']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and', ' is', ' known', ' for', ' its', ' high', ' water', ' content', ',', ' which', ' makes', ' it', ' ref', 'res', 'hing']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and', ' is', ' known', ' for', ' its', ' high', ' water', ' content', ',', ' which', ' makes', ' it', ' ref', 'res', 'hing', ' and']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and', ' is', ' known', ' for', ' its', ' high', ' water', ' content', ',', ' which', ' makes', ' it', ' ref', 'res', 'hing', ' and', ' hyd']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and', ' is', ' known', ' for', ' its', ' high', ' water', ' content', ',', ' which', ' makes', ' it', ' ref', 'res', 'hing', ' and', ' hyd', 'rating']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and', ' is', ' known', ' for', ' its', ' high', ' water', ' content', ',', ' which', ' makes', ' it', ' ref', 'res', 'hing', ' and', ' hyd', 'rating', '.']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and', ' is', ' known', ' for', ' its', ' high', ' water', ' content', ',', ' which', ' makes', ' it', ' ref', 'res', 'hing', ' and', ' hyd', 'rating', '.', ' Wat']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and', ' is', ' known', ' for', ' its', ' high', ' water', ' content', ',', ' which', ' makes', ' it', ' ref', 'res', 'hing', ' and', ' hyd', 'rating', '.', ' Wat', 'erm']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and', ' is', ' known', ' for', ' its', ' high', ' water', ' content', ',', ' which', ' makes', ' it', ' ref', 'res', 'hing', ' and', ' hyd', 'rating', '.', ' Wat', 'erm', 'el']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and', ' is', ' known', ' for', ' its', ' high', ' water', ' content', ',', ' which', ' makes', ' it', ' ref', 'res', 'hing', ' and', ' hyd', 'rating', '.', ' Wat', 'erm', 'el', 'ons']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and', ' is', ' known', ' for', ' its', ' high', ' water', ' content', ',', ' which', ' makes', ' it', ' ref', 'res', 'hing', ' and', ' hyd', 'rating', '.', ' Wat', 'erm', 'el', 'ons', ' are']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and', ' is', ' known', ' for', ' its', ' high', ' water', ' content', ',', ' which', ' makes', ' it', ' ref', 'res', 'hing', ' and', ' hyd', 'rating', '.', ' Wat', 'erm', 'el', 'ons', ' are', ' often']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and', ' is', ' known', ' for', ' its', ' high', ' water', ' content', ',', ' which', ' makes', ' it', ' ref', 'res', 'hing', ' and', ' hyd', 'rating', '.', ' Wat', 'erm', 'el', 'ons', ' are', ' often', ' e']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and', ' is', ' known', ' for', ' its', ' high', ' water', ' content', ',', ' which', ' makes', ' it', ' ref', 'res', 'hing', ' and', ' hyd', 'rating', '.', ' Wat', 'erm', 'el', 'ons', ' are', ' often', ' e', 'aten']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and', ' is', ' known', ' for', ' its', ' high', ' water', ' content', ',', ' which', ' makes', ' it', ' ref', 'res', 'hing', ' and', ' hyd', 'rating', '.', ' Wat', 'erm', 'el', 'ons', ' are', ' often', ' e', 'aten', ' fresh']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and', ' is', ' known', ' for', ' its', ' high', ' water', ' content', ',', ' which', ' makes', ' it', ' ref', 'res', 'hing', ' and', ' hyd', 'rating', '.', ' Wat', 'erm', 'el', 'ons', ' are', ' often', ' e', 'aten', ' fresh', ' or']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and', ' is', ' known', ' for', ' its', ' high', ' water', ' content', ',', ' which', ' makes', ' it', ' ref', 'res', 'hing', ' and', ' hyd', 'rating', '.', ' Wat', 'erm', 'el', 'ons', ' are', ' often', ' e', 'aten', ' fresh', ' or', ' used']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and', ' is', ' known', ' for', ' its', ' high', ' water', ' content', ',', ' which', ' makes', ' it', ' ref', 'res', 'hing', ' and', ' hyd', 'rating', '.', ' Wat', 'erm', 'el', 'ons', ' are', ' often', ' e', 'aten', ' fresh', ' or', ' used', ' in']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and', ' is', ' known', ' for', ' its', ' high', ' water', ' content', ',', ' which', ' makes', ' it', ' ref', 'res', 'hing', ' and', ' hyd', 'rating', '.', ' Wat', 'erm', 'el', 'ons', ' are', ' often', ' e', 'aten', ' fresh', ' or', ' used', ' in', ' smooth']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and', ' is', ' known', ' for', ' its', ' high', ' water', ' content', ',', ' which', ' makes', ' it', ' ref', 'res', 'hing', ' and', ' hyd', 'rating', '.', ' Wat', 'erm', 'el', 'ons', ' are', ' often', ' e', 'aten', ' fresh', ' or', ' used', ' in', ' smooth', 'ies']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and', ' is', ' known', ' for', ' its', ' high', ' water', ' content', ',', ' which', ' makes', ' it', ' ref', 'res', 'hing', ' and', ' hyd', 'rating', '.', ' Wat', 'erm', 'el', 'ons', ' are', ' often', ' e', 'aten', ' fresh', ' or', ' used', ' in', ' smooth', 'ies', ',']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and', ' is', ' known', ' for', ' its', ' high', ' water', ' content', ',', ' which', ' makes', ' it', ' ref', 'res', 'hing', ' and', ' hyd', 'rating', '.', ' Wat', 'erm', 'el', 'ons', ' are', ' often', ' e', 'aten', ' fresh', ' or', ' used', ' in', ' smooth', 'ies', ',', ' sal']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and', ' is', ' known', ' for', ' its', ' high', ' water', ' content', ',', ' which', ' makes', ' it', ' ref', 'res', 'hing', ' and', ' hyd', 'rating', '.', ' Wat', 'erm', 'el', 'ons', ' are', ' often', ' e', 'aten', ' fresh', ' or', ' used', ' in', ' smooth', 'ies', ',', ' sal', 'ads']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and', ' is', ' known', ' for', ' its', ' high', ' water', ' content', ',', ' which', ' makes', ' it', ' ref', 'res', 'hing', ' and', ' hyd', 'rating', '.', ' Wat', 'erm', 'el', 'ons', ' are', ' often', ' e', 'aten', ' fresh', ' or', ' used', ' in', ' smooth', 'ies', ',', ' sal', 'ads', ',']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and', ' is', ' known', ' for', ' its', ' high', ' water', ' content', ',', ' which', ' makes', ' it', ' ref', 'res', 'hing', ' and', ' hyd', 'rating', '.', ' Wat', 'erm', 'el', 'ons', ' are', ' often', ' e', 'aten', ' fresh', ' or', ' used', ' in', ' smooth', 'ies', ',', ' sal', 'ads', ',', ' and']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and', ' is', ' known', ' for', ' its', ' high', ' water', ' content', ',', ' which', ' makes', ' it', ' ref', 'res', 'hing', ' and', ' hyd', 'rating', '.', ' Wat', 'erm', 'el', 'ons', ' are', ' often', ' e', 'aten', ' fresh', ' or', ' used', ' in', ' smooth', 'ies', ',', ' sal', 'ads', ',', ' and', ' other']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and', ' is', ' known', ' for', ' its', ' high', ' water', ' content', ',', ' which', ' makes', ' it', ' ref', 'res', 'hing', ' and', ' hyd', 'rating', '.', ' Wat', 'erm', 'el', 'ons', ' are', ' often', ' e', 'aten', ' fresh', ' or', ' used', ' in', ' smooth', 'ies', ',', ' sal', 'ads', ',', ' and', ' other', ' d']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and', ' is', ' known', ' for', ' its', ' high', ' water', ' content', ',', ' which', ' makes', ' it', ' ref', 'res', 'hing', ' and', ' hyd', 'rating', '.', ' Wat', 'erm', 'el', 'ons', ' are', ' often', ' e', 'aten', ' fresh', ' or', ' used', ' in', ' smooth', 'ies', ',', ' sal', 'ads', ',', ' and', ' other', ' d', 'ishes']\n",
      "[' ', ' A', ' wat', 'erm', 'el', 'on', ' is', ' a', ' type', ' of', ' fruit', ' that', ' is', ' typically', ' round', ' or', ' o', 'val', ' in', ' shape', ' and', ' has', ' a', ' green', ' skin', ' with', ' a', ' sweet', ',', ' ju', 'icy', ' flesh', ' inside', '.', ' It', ' is', ' a', ' member', ' of', ' the', ' c', 'uc', 'umber', ' family', ' and', ' is', ' known', ' for', ' its', ' high', ' water', ' content', ',', ' which', ' makes', ' it', ' ref', 'res', 'hing', ' and', ' hyd', 'rating', '.', ' Wat', 'erm', 'el', 'ons', ' are', ' often', ' e', 'aten', ' fresh', ' or', ' used', ' in', ' smooth', 'ies', ',', ' sal', 'ads', ',', ' and', ' other', ' d', 'ishes', '.']\n"
     ]
    }
   ],
   "source": [
    "def get_prompt(message: str, contexts: list[str],\n",
    "               system_prompt: str) -> str:\n",
    "    texts = [f'<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>> [/INST]</s>\\n\\n']\n",
    "\n",
    "    for context in contexts:\n",
    "        texts.append(f'<s> {context.strip()} </s>\\n')\n",
    "\n",
    "    texts.append(f'[INST] {message.strip()} \\nShort answer in english:[/INST]')\n",
    "    \n",
    "    return ''.join(texts)\n",
    "\n",
    "def prompt_llm(prompt, result):\n",
    "    response = llm(prompt=prompt,\n",
    "        stream=True,\n",
    "        max_new_tokens=512,\n",
    "        top_p=0.95,\n",
    "        top_k=50,\n",
    "        temperature=0.8)\n",
    "    \n",
    "    for r in response:\n",
    "        result.append(r)\n",
    "\n",
    "result = []\n",
    "generate_kwargs = dict(\n",
    "    prompt=get_prompt(\"What is a watermelon?\", \"\", \"\"),\n",
    "    result=result\n",
    ")\n",
    "\n",
    "from threading import Thread\n",
    "t = Thread(target=prompt_llm, kwargs=generate_kwargs)\n",
    "t.start()\n",
    "\n",
    "t.join()\n",
    "outputs = []\n",
    "for text in result:\n",
    "    outputs.append(text)\n",
    "    print(list(outputs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Llama-2-7b-Chat-GPTQ](https://huggingface.co/TheBloke/Llama-2-7b-Chat-GPTQ)\n",
    "\n",
    "* Bits: The bit size of the quantised model.\n",
    "* GS: GPTQ group size. Higher numbers use less VRAM, but have lower quantisation accuracy. \"None\" is the lowest possible value.\n",
    "* Act Order: True or False. Also known as desc_act. True results in better quantisation accuracy. Some GPTQ clients have had issues with models that use Act Order plus Group Size, but this is generally resolved now.\n",
    "* Damp %: A GPTQ parameter that affects how samples are processed for quantisation. 0.01 is default, but 0.1 results in slightly better accuracy.\n",
    "* GPTQ dataset: The dataset used for quantisation. Using a dataset more appropriate to the model's training can improve quantisation accuracy. Note that the GPTQ dataset is not the same as the dataset used to train the model - please refer to the original model repo for details of the training dataset(s).\n",
    "* Sequence Length: The length of the dataset sequences used for quantisation. Ideally this is the same as the model sequence length. For some very long sequence models (16+K), a lower sequence length may have to be used. Note that a lower sequence length does not limit the sequence length of the quantised model. It only impacts the quantisation accuracy on longer inference sequences.\n",
    "* ExLlama Compatibility: Whether this file can be loaded with ExLlama, which currently only supports Llama models in 4-bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a wrapper class about all possible attributes and features\n",
    "# that you can play with a model that has been loaded using `bitsandbytes`\n",
    "\n",
    "# Currently only supports quantization\n",
    "# * `LLM.int8()`\n",
    "# * `FP4`\n",
    "# * `NF4`\n",
    "# If more methods are added to `bitsandbytes`, then more arguments will be added to this class.\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # enable 4-bit quantization by replacing the Linear layers with FP4/NF4 layers from `bitsandbytes`\n",
    "    bnb_4bit_quant_type='nf4', # sets the quantization data type in the bnb.nn.Linear4Bit layers, options are `fp4` or `nf4` data types\n",
    "    bnb_4bit_use_double_quant=True, # used for nested quantization where the quantization constants from the first quantization are quantized again\n",
    "    bnb_4bit_compute_dtype=torch.float16 # sets the computational type which might be different than the input time. For example, inputs might be fp32, but computation can be set to bf16 for speedups.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\n",
    "\n",
    "# Use a [fast Rust-based tokenizer](https://huggingface.co/docs/tokenizers/index) if it is supported for\n",
    "# a given model. If a fast tokenizer is not available for a given model, a normal Python-based tokenizer\n",
    "# is returned instead.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, revision=\"main\", trust_remote_code=False, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_TEMPLATE=\"[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>> [/INST]\\n\\n{context}\\n\\n[INST] {user_prompt}\\nPlease make a list in english:[/INST]\"\n",
    "\n",
    "entity_prompts = {\n",
    "    \"defendants\":\"What are the defendants of this case?\",\n",
    "    \"plaintiffs\":\"What are the plaintiffs of this case?\",\n",
    "    \"jurisdictions\":\"What are the jurisdictions at stake in this case?\"\n",
    "}\n",
    "\n",
    "def get_prompt(context, user_prompt=\"\", system_prompt=DEFAULT_SYSTEM_PROMPT):\n",
    "    return PROMPT_TEMPLATE.format(**{\n",
    "        \"system_prompt\": system_prompt,\n",
    "        \"context\": context,\n",
    "        \"user_prompt\": user_prompt,\n",
    "    })\n",
    "\n",
    "\n",
    "MAX_CONTEXT_LENGTH = 3500 # 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Title', 'ID', 'Case Permalink', 'Case Categories', 'Jurisdictions',\n",
       "       'Principal Laws', 'Summary', 'Reporter Info or Case Number',\n",
       "       'Filing Year', 'Status', 'Core Object'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = DATASETS_PATH / \"global_climate_change_litigation.csv\"\n",
    "new_file_path = DATASETS_PATH / \"saved_global_climate_change_litigation.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt for entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3046"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(case_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clem/Source/sandbox/finetune-llm/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1417: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n",
      "0it [00:23, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 5.78 GiB total capacity; 4.95 GiB already allocated; 51.50 MiB free; 5.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/clem/Source/sandbox/finetune-llm/notebooks/llama2-inference.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/clem/Source/sandbox/finetune-llm/notebooks/llama2-inference.ipynb#X15sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m prompt \u001b[39m=\u001b[39m get_prompt(context \u001b[39m=\u001b[39m truncated_summary, user_prompt\u001b[39m=\u001b[39mprompt, system_prompt\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/clem/Source/sandbox/finetune-llm/notebooks/llama2-inference.ipynb#X15sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer(prompt, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39minput_ids\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/clem/Source/sandbox/finetune-llm/notebooks/llama2-inference.ipynb#X15sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(inputs\u001b[39m=\u001b[39;49minput_ids, temperature\u001b[39m=\u001b[39;49m\u001b[39m0.7\u001b[39;49m, do_sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, top_p\u001b[39m=\u001b[39;49m\u001b[39m0.95\u001b[39;49m, top_k\u001b[39m=\u001b[39;49m\u001b[39m40\u001b[39;49m, max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/clem/Source/sandbox/finetune-llm/notebooks/llama2-inference.ipynb#X15sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m llm_response \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(output[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/clem/Source/sandbox/finetune-llm/notebooks/llama2-inference.ipynb#X15sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m llm_list \u001b[39m=\u001b[39m llm_response\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m[/INST]\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/Source/sandbox/finetune-llm/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Source/sandbox/finetune-llm/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1648\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1640\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1641\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1642\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1643\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1644\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1645\u001b[0m     )\n\u001b[1;32m   1647\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1648\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m   1649\u001b[0m         input_ids,\n\u001b[1;32m   1650\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1651\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[1;32m   1652\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1653\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1654\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1655\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1656\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1657\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1658\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1659\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1660\u001b[0m     )\n\u001b[1;32m   1662\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1663\u001b[0m     \u001b[39m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1664\u001b[0m     beam_scorer \u001b[39m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1665\u001b[0m         batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1666\u001b[0m         num_beams\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1671\u001b[0m         max_length\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mmax_length,\n\u001b[1;32m   1672\u001b[0m     )\n",
      "File \u001b[0;32m~/Source/sandbox/finetune-llm/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:2730\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2727\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2729\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2730\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2731\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2732\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2733\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2734\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2735\u001b[0m )\n\u001b[1;32m   2737\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2738\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/Source/sandbox/finetune-llm/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Source/sandbox/finetune-llm/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:820\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    817\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m    819\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 820\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m    821\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    822\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    823\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    824\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    825\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    826\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    827\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    828\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    829\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    830\u001b[0m )\n\u001b[1;32m    832\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    833\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpretraining_tp \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/Source/sandbox/finetune-llm/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Source/sandbox/finetune-llm/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:708\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    701\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    702\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    703\u001b[0m         hidden_states,\n\u001b[1;32m    704\u001b[0m         attention_mask,\n\u001b[1;32m    705\u001b[0m         position_ids,\n\u001b[1;32m    706\u001b[0m     )\n\u001b[1;32m    707\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 708\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    709\u001b[0m         hidden_states,\n\u001b[1;32m    710\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    711\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    712\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    713\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    714\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    715\u001b[0m     )\n\u001b[1;32m    717\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    719\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/Source/sandbox/finetune-llm/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Source/sandbox/finetune-llm/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:424\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    421\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    423\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 424\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[1;32m    425\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    426\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    427\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    428\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    429\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    430\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    431\u001b[0m )\n\u001b[1;32m    432\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    434\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/Source/sandbox/finetune-llm/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Source/sandbox/finetune-llm/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:338\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    336\u001b[0m     \u001b[39m# reuse k, v, self_attention\u001b[39;00m\n\u001b[1;32m    337\u001b[0m     key_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([past_key_value[\u001b[39m0\u001b[39m], key_states], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m--> 338\u001b[0m     value_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat([past_key_value[\u001b[39m1\u001b[39;49m], value_states], dim\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m    340\u001b[0m past_key_value \u001b[39m=\u001b[39m (key_states, value_states) \u001b[39mif\u001b[39;00m use_cache \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[39m# repeat k/v heads if n_kv_heads < n_heads\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 5.78 GiB total capacity; 4.95 GiB already allocated; 51.50 MiB free; 5.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "empty_prompt = get_prompt(context = \"\", user_prompt=\"\", system_prompt=\"\")\n",
    "df[\"Clean_Summary\"] = df[\"Summary\"].fillna(\"\")\n",
    "\n",
    "saved_df = pd.read_csv(new_file_path)\n",
    "\n",
    "for new_column in entity_prompts.keys():\n",
    "    df[new_column] = [None] * len(df)\n",
    "\n",
    "for i, row in tqdm(df[152:].iterrows()):\n",
    "    for entity, prompt in entity_prompts.items():\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        case_summary = row[\"Clean_Summary\"]\n",
    "        truncated_summary = case_summary[:MAX_CONTEXT_LENGTH - len(empty_prompt) - len(prompt)]\n",
    "\n",
    "        prompt = get_prompt(context = truncated_summary, user_prompt=prompt, system_prompt=\"\")\n",
    "\n",
    "        input_ids = tokenizer(prompt, return_tensors='pt').input_ids.cuda()\n",
    "        output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "        llm_response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "        llm_list = llm_response.split(\"[/INST]\")[-1]\n",
    "\n",
    "        try:\n",
    "            llm_list = [match.group(\"item\") for match in re.finditer(r\"(\\d{1,2}\\.{1}|\\*)\\s{1}(?P<item>[^\\n]+)\", llm_response.split(\"[/INST]\")[-1])]\n",
    "        except:\n",
    "            llm_list = [llm_list]\n",
    "     \n",
    "        saved_df.loc[i, entity] = \"<br>\".join(llm_list)\n",
    "    \n",
    "\n",
    "    saved_df[[\"Title\", \"Case Permalink\"] + list(entity_prompts.keys())].to_csv(new_file_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "llm_response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "case_keywords = re.findall(r\"\\d{1,2}\\.{1}\\s{1}([^\\n]+)\", llm_response)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt_template, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs) # , max_new_tokens=40\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] <<SYS>>\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
      "<</SYS>>\n",
      "Tell me about AI[/INST]\n",
      "\n",
      "Hello! I'm here to help you with any questions you may have about AI. AI, or Artificial Intelligence, refers to the development of computer systems that can perform tasks that typically require human intelligence, such as visual perception, speech recognition, and decision-making. AI technology has been rapidly advancing in recent years and has the potential to revolutionize many industries and aspects of our lives.\n",
      "There are several types of AI, including:\n",
      "1. Narrow or weak AI: This type of AI is designed to perform a specific task, such as facial recognition, language translation, or playing a game like chess.\n",
      "2. General or strong AI: This type of AI is designed to perform any intellectual task that a human can, such as reasoning, problem-solving, and learning.\n",
      "3. Superintelligence: This type of AI is significantly more intelligent than the best human minds.\n",
      "4. Artificial general intelligence (AGI): This type of AI is designed to perform any intellectual task that a human can, and is considered the holy grail of AI research.\n",
      "5. Cognitive computing: This type of AI is designed to mimic human thought processes, such as reasoning, problem-solving, and decision-making.\n",
      "6. Machine learning: This type of AI is designed to learn from data and improve its performance over time.\n",
      "7. Deep learning: This type of AI is a subset of machine learning that uses neural networks to analyze data.\n",
      "8. Natural language processing (NLP): This type of AI is designed to understand, interpret, and generate human language.\n",
      "9. Robotics: This type of AI is designed to interact with the physical world, such as robots that can perform tasks like assembly or surgery.\n",
      "10. Expert systems: This type of AI is designed to perform a specific task or set of tasks, such as diagnosing medical conditions or providing financial advice.\n",
      "It's important to note that AI is still a developing technology and there are many challenges and ethical considerations that need to be addressed as it continues to evolve.\n",
      "I hope this helps you understand more about AI! Is there anything else you would like to know?\n"
     ]
    }
   ],
   "source": [
    "with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n",
    "    outputs = model.generate(**inputs)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
