{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning Llama 2\n",
    "\n",
    "* Free Google Colab offers a 15GB Graphics Card (Limited Resources --> Barely enough to store Llama 2–7b’s weights)\n",
    "* We also need to consider the overhead due to optimizer states, gradients, and forward activations\n",
    "* Full fine-tuning is not possible here: we need parameter-efficient fine-tuning (PEFT) techniques like LoRA or QLoRA.\n",
    "* To drastically reduce the VRAM usage, we must fine-tune the model in 4-bit precision, which is why we’ll use QLoRA here.\n",
    "\n",
    "## C++ tools\n",
    "* `llama.cpp's` objective is to run the LLaMA model with 4-bit integer quantization on MacBook. It is a plain C/C++ implementation optimized for Apple silicon and x86 architectures, supporting various integer quantization and BLAS libraries. Originally a web chat example, it now serves as a development playground for ggml library features.\n",
    "* `GGML`, a C library for machine learning, facilitates the distribution of large language models (LLMs). It utilizes quantization to enable efficient LLM execution on consumer hardware. GGML files contain binary-encoded data, including version number, hyperparameters, vocabulary, and weights. The vocabulary comprises tokens for language generation, while the weights determine the LLM's size. Quantization reduces precision to optimize resource usage.\n",
    "\n",
    "The Hugging Face community provides quantized models, which allow us to efficiently and effectively utilize the model on the T4 GPU. It is important to consult reliable sources before using any model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clem/Documents/source/sandbox/finetune-llm/.venv/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/clem/Documents/source/sandbox/finetune-llm/.venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, HfArgumentParser, TrainingArguments\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define and parse arguments.\n",
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    \"\"\"\n",
    "    The name of the Casual LM model we wish to fine with SFTTrainer\n",
    "    \"\"\"\n",
    "\n",
    "    model_name: Optional[str] = field(default=\"facebook/opt-350m\", metadata={\"help\": \"the model name\"})\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=\"timdettmers/openassistant-guanaco\", metadata={\"help\": \"the dataset name\"}\n",
    "    )\n",
    "    dataset_text_field: Optional[str] = field(default=\"text\", metadata={\"help\": \"the text field of the dataset\"})\n",
    "    log_with: Optional[str] = field(default=None, metadata={\"help\": \"use 'wandb' to log with wandb\"})\n",
    "    learning_rate: Optional[float] = field(default=1.41e-5, metadata={\"help\": \"the learning rate\"})\n",
    "    batch_size: Optional[int] = field(default=64, metadata={\"help\": \"the batch size\"})\n",
    "    seq_length: Optional[int] = field(default=512, metadata={\"help\": \"Input sequence length\"})\n",
    "    gradient_accumulation_steps: Optional[int] = field(\n",
    "        default=16, metadata={\"help\": \"the number of gradient accumulation steps\"}\n",
    "    )\n",
    "    load_in_8bit: Optional[bool] = field(default=False, metadata={\"help\": \"load the model in 8 bits precision\"})\n",
    "    load_in_4bit: Optional[bool] = field(default=False, metadata={\"help\": \"load the model in 4 bits precision\"})\n",
    "    use_peft: Optional[bool] = field(default=False, metadata={\"help\": \"Wether to use PEFT or not to train adapters\"})\n",
    "    trust_remote_code: Optional[bool] = field(default=True, metadata={\"help\": \"Enable `trust_remote_code`\"})\n",
    "    output_dir: Optional[str] = field(default=\"output\", metadata={\"help\": \"the output directory\"})\n",
    "    peft_lora_r: Optional[int] = field(default=64, metadata={\"help\": \"the r parameter of the LoRA adapters\"})\n",
    "    peft_lora_alpha: Optional[int] = field(default=16, metadata={\"help\": \"the alpha parameter of the LoRA adapters\"})\n",
    "    logging_steps: Optional[int] = field(default=1, metadata={\"help\": \"the number of logging steps\"})\n",
    "    use_auth_token: Optional[bool] = field(default=True, metadata={\"help\": \"Use HF auth token to access the model\"})\n",
    "    num_train_epochs: Optional[int] = field(default=3, metadata={\"help\": \"the number of training epochs\"})\n",
    "    max_steps: Optional[int] = field(default=-1, metadata={\"help\": \"the number of training steps\"})\n",
    "    save_steps: Optional[int] = field(\n",
    "        default=100, metadata={\"help\": \"Number of updates steps before two checkpoint saves\"}\n",
    "    )\n",
    "    save_total_limit: Optional[int] = field(default=10, metadata={\"help\": \"Limits total number of checkpoints.\"})\n",
    "    push_to_hub: Optional[bool] = field(default=False, metadata={\"help\": \"Push the model to HF Hub\"})\n",
    "    hub_model_id: Optional[str] = field(default=None, metadata={\"help\": \"The name of the model on HF Hub\"})\n",
    "\n",
    "\n",
    "\n",
    "script_args = ScriptArguments()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_args.load_in_4bit = True\n",
    "script_args.load_in_8bit = False\n",
    "script_args.model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "script_args.dataset_name = \"timdettmers/openassistant-guanaco\"\n",
    "\n",
    "# script_args.gradient_accumulation_steps\n",
    "# script_args.batch_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if script_args.load_in_8bit and script_args.load_in_4bit:\n",
    "    raise ValueError(\"You can't load the model in 8 bits and 4 bits at the same time\")\n",
    "elif script_args.load_in_8bit or script_args.load_in_4bit:\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=script_args.load_in_8bit, load_in_4bit=script_args.load_in_4bit\n",
    "    )\n",
    "    # Copy the model to each device\n",
    "    device_map = {\"\": Accelerator().local_process_index}\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    device_map = None\n",
    "    quantization_config = None\n",
    "    torch_dtype = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11.7'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# torch.zeros(1).cuda()\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_init()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "# torch.zeros(1).cuda()\n",
    "torch._C._cuda_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch._C._cuda_getDeviceCount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clem/Documents/source/sandbox/finetune-llm/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:479: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or pip install bitsandbytes` ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m      2\u001b[0m     script_args\u001b[39m.\u001b[39;49mmodel_name,\n\u001b[1;32m      3\u001b[0m     quantization_config\u001b[39m=\u001b[39;49mquantization_config,\n\u001b[1;32m      4\u001b[0m     device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m      5\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49mscript_args\u001b[39m.\u001b[39;49mtrust_remote_code,\n\u001b[1;32m      6\u001b[0m     torch_dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[1;32m      7\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49mscript_args\u001b[39m.\u001b[39;49muse_auth_token,\n\u001b[1;32m      8\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/source/sandbox/finetune-llm/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    564\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    565\u001b[0m     )\n\u001b[1;32m    566\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/source/sandbox/finetune-llm/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:2482\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[39mif\u001b[39;00m load_in_8bit \u001b[39mor\u001b[39;00m load_in_4bit:\n\u001b[1;32m   2481\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_accelerate_available() \u001b[39mand\u001b[39;00m is_bitsandbytes_available()):\n\u001b[0;32m-> 2482\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m   2483\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUsing `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2484\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2485\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m pip install bitsandbytes` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2486\u001b[0m         )\n\u001b[1;32m   2488\u001b[0m     \u001b[39mif\u001b[39;00m torch_dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2489\u001b[0m         \u001b[39m# We force the `dtype` to be float16, this is a requirement from `bitsandbytes`\u001b[39;00m\n\u001b[1;32m   2490\u001b[0m         logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m   2491\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOverriding torch_dtype=\u001b[39m\u001b[39m{\u001b[39;00mtorch_dtype\u001b[39m}\u001b[39;00m\u001b[39m with `torch_dtype=torch.float16` due to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2492\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mrequirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2493\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2494\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m torch_dtype=torch.float16 to remove this warning.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2495\u001b[0m         )\n",
      "\u001b[0;31mImportError\u001b[0m: Using `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or pip install bitsandbytes` "
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    script_args.model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=script_args.trust_remote_code,\n",
    "    torch_dtype=torch_dtype,\n",
    "    use_auth_token=script_args.use_auth_token,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(script_args.dataset_name, split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Define the training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=script_args.output_dir,\n",
    "    per_device_train_batch_size=script_args.batch_size,\n",
    "    gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n",
    "    learning_rate=script_args.learning_rate,\n",
    "    logging_steps=script_args.logging_steps,\n",
    "    num_train_epochs=script_args.num_train_epochs,\n",
    "    max_steps=script_args.max_steps,\n",
    "    report_to=script_args.log_with,\n",
    "    save_steps=script_args.save_steps,\n",
    "    save_total_limit=script_args.save_total_limit,\n",
    "    push_to_hub=script_args.push_to_hub,\n",
    "    hub_model_id=script_args.hub_model_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Define the LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if script_args.use_peft:\n",
    "    peft_config = LoraConfig(\n",
    "        r=script_args.peft_lora_r,\n",
    "        lora_alpha=script_args.peft_lora_alpha,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "else:\n",
    "    peft_config = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Define the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    max_seq_length=script_args.seq_length,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=script_args.dataset_text_field,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(script_args.output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
