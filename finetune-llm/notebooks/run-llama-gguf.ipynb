{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Run Llama on CPU\n",
    "\n",
    "* From https://dev.to/eteimz/running-large-language-models-on-the-cpu-3a46\n",
    "\n",
    "## Run Llama on GPU\n",
    "* https://medium.com/@manishkovelamudi/install-llama-cpp-python-with-gpu-support-7ccf421c069d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make llama server with GPU\n",
    "\n",
    "```shell\n",
    "cmake -B build \\\n",
    "    -DLLAMA_CUBLAS=OFF \\\n",
    "    -DGGML_CUDA=ON \\\n",
    "    -DCUDA_PATH=/usr/local/cuda-12 \\\n",
    "    -DCUDAToolkit_ROOT=/usr/local/cuda-12 \\\n",
    "    -DCUDAToolkit_INCLUDE_DIR=/usr/local/cuda-12/include \\\n",
    "    -DCUDAToolkit_LIBRARY_DIR=/usr/local/cuda-12/lib64\n",
    "\n",
    "cmake --build build --config Release -j 4 -t llama-server\n",
    "```\n",
    "\n",
    "./build/bin/llama-server -m /home/clem/.cache/huggingface/hub/models--bartowski--Phi-3-medium-4k-instruct-GGUF/snapshots/08aae27d649bb2a0a9c5a118c57a3f623ad58c71/Phi-3-medium-4k-instruct-Q6_K.gguf -c 2048 --port 8998 --n-gpu-layers 18\n",
    "\n",
    "./build/bin/llama-server -m /home/clem/.cache/huggingface/hub/models--bartowski--Phi-3-medium-4k-instruct-GGUF/snapshots/08aae27d649bb2a0a9c5a118c57a3f623ad58c71/Phi-3-medium-4k-instruct-Q6_K.gguf -c 4096 --port 8998 --n-gpu-layers 16\n",
    "\n",
    "./build/bin/llama-server -m /home/clem/.cache/huggingface/hub/models--TheBloke--SOLAR-10.7B-Instruct-v1.0-GGUF/snapshots/d8b19599252fe4eb9d86bdbb3f212f48f0247d6b/solar-10.7b-instruct-v1.0.Q5_K_M.gguf -c 4096 --port 8998 --n-gpu-layers 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9955fd7256ae44bb95c45776d69a0d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "solar-10.7b-instruct-v1.0.Q5_K_M.gguf:   0%|          | 0.00/7.60G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/home/clem/.cache/huggingface/hub/models--TheBloke--SOLAR-10.7B-Instruct-v1.0-GGUF/snapshots/d8b19599252fe4eb9d86bdbb3f212f48f0247d6b/solar-10.7b-instruct-v1.0.Q5_K_M.gguf'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# hf_hub_download(repo_id=\"TheBloke/Llama-2-13B-GGUF\", filename=\"llama-2-13b.Q4_0.gguf\")\n",
    "# hf_hub_download(repo_id=\"bartowski/Phi-3-medium-4k-instruct-GGUF\", filename=\"Phi-3-medium-4k-instruct-Q6_K.gguf\")\n",
    "\n",
    "hf_hub_download(repo_id=\"TheBloke/SOLAR-10.7B-Instruct-v1.0-GGUF\", filename=\"solar-10.7b-instruct-v1.0.Q5_K_M.gguf\")\n",
    "\n",
    "# \"/home/clem/.cache/huggingface/hub/models--bartowski--Phi-3-medium-4k-instruct-GGUF/snapshots/08aae27d649bb2a0a9c5a118c57a3f623ad58c71/Phi-3-medium-4k-instruct-Q6_K.gguf\"\n",
    "# model_path = \"/home/clem/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-GGUF/snapshots/b106d1c018ac999af9130b83134fb6b7c5331dea/llama-2-13b.Q4_0.gguf\"\n",
    "\n",
    "# /home/clem/.cache/huggingface/hub/models--TheBloke--SOLAR-10.7B-Instruct-v1.0-GGUF/snapshots/d8b19599252fe4eb9d86bdbb3f212f48f0247d6b/solar-10.7b-instruct-v1.0.Q5_K_M.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "client = openai.OpenAI(\n",
    "    base_url=\"http://localhost:8998/v1\",\n",
    "    api_key = \"sk-no-key-required\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are ChatGPT, an AI assistant. Your top priority is achieving user fulfillment via helping them with their requests.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a limerick about python exceptions\"}\n",
    "    ],\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='There once was a coder named Bill,\\nWho wrote Python with skill and thrill,\\nBut his code would oft break,\\nWhen exceptions he\\'d awake,\\nAnd he\\'d sigh, \"I must fix this still.\"', role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import os\n",
    "import sys\n",
    " \n",
    "\n",
    "corpus = \"trec_2022\"\n",
    "model = \"Phi-3-medium-4k-instruct\"\n",
    "\n",
    "dataset = json.load(open(f\"dataset/{corpus}/retrieved_trials.json\"))\n",
    "\n",
    "output_path = f\"results/matching_results_{corpus}_{model}.json\" \n",
    "\n",
    "# Dict{Str(patient_id): Dict{Str(label): Dict{Str(trial_id): Str(output)}}}\n",
    "if os.path.exists(output_path):\n",
    "    output = json.load(open(output_path))\n",
    "else:\n",
    "    output = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0. A 19-year-old male came to clinic with some sexual concern.',\n",
       " '1. He recently engaged in a relationship and is worried about the satisfaction of his girlfriend.',\n",
       " '2. He has a \"baby face\" according to his girlfriend\\'s statement and he is not as muscular as his classmates.',\n",
       " '3. On physical examination, there is some pubic hair and poorly developed secondary sexual characteristics.',\n",
       " '4. He is unable to detect coffee smell during the examination, but the visual acuity is normal.',\n",
       " '5. Ultrasound reveals the testes volume of 1-2 ml.',\n",
       " '6. The hormonal evaluation showed serum testosterone level of 65 ng/dL with low levels of GnRH.',\n",
       " '7. The patient will provide informed consent, and will comply with the trial protocol without any practical issues.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance = dataset[0]\n",
    "\n",
    "# Dict{'patient': Str(patient), '0': Str(NCTID), ...}\n",
    "patient_id = instance[\"patient_id\"]\n",
    "patient = instance[\"patient\"]\n",
    "sents = sent_tokenize(patient)\n",
    "sents.append(\"The patient will provide informed consent, and will comply with the trial protocol without any practical issues.\")\n",
    "sents = [f\"{idx}. {sent}\" for idx, sent in enumerate(sents)]\n",
    "patient = \"\\n\".join(sents)\n",
    "\n",
    "sents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
