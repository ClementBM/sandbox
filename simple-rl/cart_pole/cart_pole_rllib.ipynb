{"cells":[{"cell_type":"code","execution_count":4,"source":["import gym\n","import ray\n","import ray.rllib.algorithms.ppo as ppo\n","from ray.rllib.algorithms.algorithm import Algorithm\n","import imageio\n","import numpy as np\n","from ray import tune, air\n","from ray.rllib.utils.framework import try_import_torch\n","from ray.rllib.policy.sample_batch import SampleBatch\n","torch, _ = try_import_torch()"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":5,"source":["ray.shutdown()\n","ray.init()\n","lstm_cell_size = 64\n","config = (\n","    ppo.PPOConfig()\n","    .environment(\"CartPole-v1\")\n","    .framework(\"torch\")\n","    .training(model={\"use_lstm\": True, \"lstm_cell_size\": lstm_cell_size})\n",")\n","\n","stop = {\n","    \"training_iteration\": 10,\n","    \"timesteps_total\": 100000,\n","    \"episode_reward_mean\": 200,\n","}\n","\n","tuner = tune.Tuner(\n","    \"PPO\",\n","    param_space=config.to_dict(),\n","    run_config=air.RunConfig(stop=stop),\n",")\n","\n","results = tuner.fit()\n","result = results.get_best_result()\n","algo = Algorithm.from_checkpoint(result.checkpoint)\n","ppo.PPO(config=config, env=\"CartPole-v1\")\n","algo.restore(result.checkpoint)\n","# Perform inference (action computations) based on given env observations\n","\n","env = gym.make(\"CartPole-v1\")\n","# Get the initial observation (some value between -10.0 and 10.0).\n","\n","obs = env.reset()\n","done = False\n","# In case the model needs previous-reward/action inputs, keep track of\n","# these via these variables here (we'll have to pass them into the\n","# compute_actions methods below).\n","# Do we need prev-action/reward as part of the input?\n","\n","init_prev_a = prev_a = None\n","init_prev_r = prev_r = None\n","# range(2) b/c h- and c-states of the LSTM.\n","\n","state = [np.zeros([lstm_cell_size], np.float32) for _ in range(2)]\n","filename = \"testtesttest.mp4\"\n","with imageio.get_writer(filename, fps=30) as video:\n","    while not done:\n","        # Compute an action (`a`).\n","        a, state_out, _ = algo.compute_single_action(\n","            observation=obs,\n","            state=state,\n","            prev_action=prev_a,\n","            prev_reward=prev_r,\n","            explore=False,\n","        )\n","        # Send the computed action `a` to the env.\n","        obs, reward, done, info = env.step(a)\n","        state = state_out\n","        if init_prev_a is not None:\n","            prev_a = a\n","        if init_prev_r is not None:\n","            prev_r = reward\n","        video.append_data(env.render(mode=\"rgb_array\"))"],"outputs":[{"output_type":"stream","name":"stderr","text":["2023-02-01 14:47:48,813\tINFO worker.py:1538 -- Started a local Ray instance.\n"]},{"output_type":"display_data","data":{"text/html":["<div class=\"tuneStatus\">\n","  <div style=\"display: flex;flex-direction: row\">\n","    <div style=\"display: flex;flex-direction: column;\">\n","      <h3>Tune Status</h3>\n","      <table>\n","<tbody>\n","<tr><td>Current time:</td><td>2023-02-01 14:49:54</td></tr>\n","<tr><td>Running for: </td><td>00:02:04.83        </td></tr>\n","<tr><td>Memory:      </td><td>12.6/15.6 GiB      </td></tr>\n","</tbody>\n","</table>\n","    </div>\n","    <div class=\"vDivider\"></div>\n","    <div class=\"systemInfo\">\n","      <h3>System Info</h3>\n","      Using FIFO scheduling algorithm.<br>Resources requested: 0/6 CPUs, 0/1 GPUs, 0.0/2.3 GiB heap, 0.0/1.15 GiB objects (0.0/1.0 accelerator_type:RTX)\n","    </div>\n","    \n","  </div>\n","  <div class=\"hDivider\"></div>\n","  <div class=\"trialStatus\">\n","    <h3>Trial Status</h3>\n","    <table>\n","<thead>\n","<tr><th>Trial name                 </th><th>status    </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>PPO_CartPole-v1_04576_00000</td><td>TERMINATED</td><td>192.168.1.72:47527</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         116.896</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">  185.91</td><td style=\"text-align: right;\">                 473</td><td style=\"text-align: right;\">                  73</td><td style=\"text-align: right;\">            185.91</td></tr>\n","</tbody>\n","</table>\n","  </div>\n","</div>\n","<style>\n",".tuneStatus {\n","  color: var(--jp-ui-font-color1);\n","}\n",".tuneStatus .systemInfo {\n","  display: flex;\n","  flex-direction: column;\n","}\n",".tuneStatus td {\n","  white-space: nowrap;\n","}\n",".tuneStatus .trialStatus {\n","  display: flex;\n","  flex-direction: column;\n","}\n",".tuneStatus h3 {\n","  font-weight: bold;\n","}\n",".tuneStatus .hDivider {\n","  border-bottom-width: var(--jp-border-width);\n","  border-bottom-color: var(--jp-border-color0);\n","  border-bottom-style: solid;\n","}\n",".tuneStatus .vDivider {\n","  border-left-width: var(--jp-border-width);\n","  border-left-color: var(--jp-border-color0);\n","  border-left-style: solid;\n","  margin: 0.5em 1em 0.5em 1em;\n","}\n","</style>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[2m\u001b[36m(PPO pid=47527)\u001b[0m 2023-02-01 14:47:53,523\tWARNING algorithm_config.py:488 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n","\u001b[2m\u001b[36m(PPO pid=47527)\u001b[0m 2023-02-01 14:47:53,667\tINFO algorithm.py:501 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n","\u001b[2m\u001b[36m(PPO pid=47527)\u001b[0m 2023-02-01 14:47:57,572\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n","\u001b[2m\u001b[36m(RolloutWorker pid=47579)\u001b[0m 2023-02-01 14:47:57,515\tWARNING env.py:159 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n"]},{"output_type":"display_data","data":{"text/html":["<div class=\"trialProgress\">\n","  <h3>Trial Progress</h3>\n","  <table>\n","<thead>\n","<tr><th>Trial name                 </th><th style=\"text-align: right;\">  agent_timesteps_total</th><th>counters                                                                                                                            </th><th>custom_metrics  </th><th>date               </th><th>done  </th><th style=\"text-align: right;\">  episode_len_mean</th><th>episode_media  </th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_mean</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episodes_this_iter</th><th style=\"text-align: right;\">  episodes_total</th><th>experiment_id                   </th><th>hostname    </th><th>info                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </th><th style=\"text-align: right;\">  iterations_since_restore</th><th>node_ip     </th><th style=\"text-align: right;\">  num_agent_steps_sampled</th><th style=\"text-align: right;\">  num_agent_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_sampled</th><th style=\"text-align: right;\">  num_env_steps_sampled_this_iter</th><th style=\"text-align: right;\">  num_env_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_trained_this_iter</th><th style=\"text-align: right;\">  num_faulty_episodes</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_in_flight_async_reqs</th><th style=\"text-align: right;\">  num_remote_worker_restarts</th><th style=\"text-align: right;\">  num_steps_trained_this_iter</th><th>perf                                                    </th><th style=\"text-align: right;\">  pid</th><th>policy_reward_max  </th><th>policy_reward_mean  </th><th>policy_reward_min  </th><th>sampler_perf                                                                                                                                                                                                     </th><th>sampler_results                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      </th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th>timers                                                                                                                                                                                 </th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th style=\"text-align: right;\">  timesteps_total</th><th style=\"text-align: right;\">  training_iteration</th><th style=\"text-align: right;\">   trial_id</th><th style=\"text-align: right;\">  warmup_time</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>PPO_CartPole-v1_04576_00000</td><td style=\"text-align: right;\">                  40000</td><td>{&#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 40000, &#x27;num_agent_steps_trained&#x27;: 40000}</td><td>{}              </td><td>2023-02-01_14-49-54</td><td>True  </td><td style=\"text-align: right;\">            185.91</td><td>{}             </td><td style=\"text-align: right;\">                 473</td><td style=\"text-align: right;\">               185.91</td><td style=\"text-align: right;\">                  73</td><td style=\"text-align: right;\">                  16</td><td style=\"text-align: right;\">             615</td><td>bc2a93bcf6614718891b4852b410c7bd</td><td>clem-MS-7B24</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;grad_gnorm&#x27;: 2.3527032748185177, &#x27;cur_kl_coeff&#x27;: 0.22500000000000006, &#x27;cur_lr&#x27;: 5.0000000000000016e-05, &#x27;total_loss&#x27;: 9.599447546723068, &#x27;policy_loss&#x27;: -0.0037448132440688147, &#x27;vf_loss&#x27;: 9.601659614809098, &#x27;vf_explained_var&#x27;: -0.0234281623876223, &#x27;kl&#x27;: 0.006812256550837265, &#x27;entropy&#x27;: 0.5892923270502398, &#x27;entropy_coeff&#x27;: 0.0}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 127.74193548387096, &#x27;num_grad_updates_lifetime&#x27;: 8835.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 464.5}}, &#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 40000, &#x27;num_agent_steps_sampled&#x27;: 40000, &#x27;num_agent_steps_trained&#x27;: 40000}</td><td style=\"text-align: right;\">                        10</td><td>192.168.1.72</td><td style=\"text-align: right;\">                    40000</td><td style=\"text-align: right;\">                    40000</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    2</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                         4000</td><td>{&#x27;cpu_util_percent&#x27;: 31.30625, &#x27;ram_util_percent&#x27;: 81.0}</td><td style=\"text-align: right;\">47527</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.2770729001245672, &#x27;mean_inference_ms&#x27;: 0.9536920048681804, &#x27;mean_action_processing_ms&#x27;: 0.04858773256284574, &#x27;mean_env_wait_ms&#x27;: 0.04717422188142363, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: 473.0, &#x27;episode_reward_min&#x27;: 73.0, &#x27;episode_reward_mean&#x27;: 185.91, &#x27;episode_len_mean&#x27;: 185.91, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 16, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [197.0, 124.0, 73.0, 113.0, 100.0, 135.0, 142.0, 87.0, 152.0, 161.0, 178.0, 123.0, 172.0, 158.0, 126.0, 123.0, 191.0, 167.0, 445.0, 182.0, 163.0, 135.0, 148.0, 192.0, 151.0, 119.0, 284.0, 122.0, 176.0, 226.0, 282.0, 160.0, 166.0, 187.0, 181.0, 172.0, 132.0, 131.0, 130.0, 211.0, 133.0, 202.0, 167.0, 162.0, 145.0, 153.0, 81.0, 184.0, 117.0, 147.0, 170.0, 221.0, 152.0, 105.0, 251.0, 165.0, 214.0, 276.0, 116.0, 202.0, 205.0, 242.0, 125.0, 145.0, 223.0, 284.0, 220.0, 183.0, 171.0, 279.0, 261.0, 147.0, 110.0, 186.0, 162.0, 149.0, 127.0, 224.0, 139.0, 122.0, 250.0, 183.0, 286.0, 110.0, 190.0, 192.0, 199.0, 473.0, 245.0, 350.0, 247.0, 268.0, 159.0, 299.0, 103.0, 274.0, 441.0, 162.0, 192.0, 284.0], &#x27;episode_lengths&#x27;: [197, 124, 73, 113, 100, 135, 142, 87, 152, 161, 178, 123, 172, 158, 126, 123, 191, 167, 445, 182, 163, 135, 148, 192, 151, 119, 284, 122, 176, 226, 282, 160, 166, 187, 181, 172, 132, 131, 130, 211, 133, 202, 167, 162, 145, 153, 81, 184, 117, 147, 170, 221, 152, 105, 251, 165, 214, 276, 116, 202, 205, 242, 125, 145, 223, 284, 220, 183, 171, 279, 261, 147, 110, 186, 162, 149, 127, 224, 139, 122, 250, 183, 286, 110, 190, 192, 199, 473, 245, 350, 247, 268, 159, 299, 103, 274, 441, 162, 192, 284]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.2770729001245672, &#x27;mean_inference_ms&#x27;: 0.9536920048681804, &#x27;mean_action_processing_ms&#x27;: 0.04858773256284574, &#x27;mean_env_wait_ms&#x27;: 0.04717422188142363, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0}</td><td style=\"text-align: right;\">             116.896</td><td style=\"text-align: right;\">           11.3296</td><td style=\"text-align: right;\">       116.896</td><td>{&#x27;training_iteration_time_ms&#x27;: 11686.437, &#x27;load_time_ms&#x27;: 14.615, &#x27;load_throughput&#x27;: 273693.882, &#x27;learn_time_ms&#x27;: 8995.509, &#x27;learn_throughput&#x27;: 444.666, &#x27;synch_weights_time_ms&#x27;: 1.91}</td><td style=\"text-align: right;\"> 1675259394</td><td style=\"text-align: right;\">                        0</td><td style=\"text-align: right;\">            40000</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">04576_00000</td><td style=\"text-align: right;\">      3.90964</td></tr>\n","</tbody>\n","</table>\n","</div>\n","<style>\n",".trialProgress {\n","  display: flex;\n","  flex-direction: column;\n","  color: var(--jp-ui-font-color1);\n","}\n",".trialProgress h3 {\n","  font-weight: bold;\n","}\n",".trialProgress td {\n","  white-space: nowrap;\n","}\n","</style>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2023-02-01 14:49:55,126\tINFO tune.py:762 -- Total run time: 125.32 seconds (124.82 seconds for the tuning loop).\n","2023-02-01 14:49:55,154\tWARNING algorithm_config.py:488 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n","2023-02-01 14:49:55,163\tINFO algorithm.py:501 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n","\u001b[2m\u001b[36m(RolloutWorker pid=47820)\u001b[0m 2023-02-01 14:49:58,365\tWARNING env.py:159 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n","2023-02-01 14:49:58,486\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n","2023-02-01 14:49:58,521\tWARNING deprecation.py:47 -- DeprecationWarning: `algo = Algorithm(env='CartPole-v1', ...)` has been deprecated. Use `algo = AlgorithmConfig().environment('CartPole-v1').build()` instead. This will raise an error in the future!\n","\u001b[2m\u001b[36m(RolloutWorker pid=47909)\u001b[0m 2023-02-01 14:50:01,721\tWARNING env.py:159 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n","2023-02-01 14:50:01,903\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n","2023-02-01 14:50:01,967\tINFO trainable.py:790 -- Restored on 192.168.1.72 from checkpoint: /home/clem/ray_results/PPO/PPO_CartPole-v1_04576_00000_0_2023-02-01_14-47-49/checkpoint_000010\n","2023-02-01 14:50:01,967\tINFO trainable.py:799 -- Current state after restoring: {'_iteration': 10, '_timesteps_total': None, '_time_total': 116.89642643928528, '_episodes_total': 615}\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","[swscaler @ 0x5892f80] Warning: data is not aligned! This can lead to a speed loss\n"]}],"metadata":{}},{"cell_type":"code","execution_count":6,"source":["\n","# tensorboard --logdir /home/clem/ray_results/\n","# rllib evaluate /home/clem/ray_results/PPO/PPO_CryptoEnv_fcdc7_00000_0_2023-01-16_15-56-42/checkpoint_000100 --config \"{\\\"env\\\": \\\"CartPole-v1\\\"}\" --run PPO --steps 300\n",""],"outputs":[],"metadata":{}}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}