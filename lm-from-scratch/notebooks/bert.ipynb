{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from lm_from_scratch.models.bert import BERT\n",
    "from lm_from_scratch.corpus.decision_corpus import DecisionCorpus\n",
    "import pandas as pd\n",
    "from artefacts import TOKENIZER_PATH\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "VOCAB_SIZE = 30000\n",
    "N_SEGMENTS = 2\n",
    "MAX_LEN = 100 # 512 # what is the maximum context length for predictions?\n",
    "EMBED_DIM = 128 # 768\n",
    "N_LAYERS = 3\n",
    "ATTN_HEADS = 4 # 32 * 4 = 128\n",
    "DROPOUT = 0.1\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "EVAL_ITERS = 200\n",
    "MAX_ITERS = 100\n",
    "EVAL_INTERVAL = 10\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "BATCH_SIZE = 4 # how many independent sequences will we process in parallel?\n",
    "\n",
    "MAX_SENTENCE_LEN = MAX_LEN // 2\n",
    "MIN_SENTENCE_LEN = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus and tokenizer setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = DecisionCorpus()\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "trainer = BpeTrainer(vocab_size=VOCAB_SIZE,\n",
    "                     special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "tokenizer.train_from_iterator(corpus.get_text(), trainer)\n",
    "\n",
    "# post-processing to traditional BERT inputs\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# pad the outputs to the longest sentence present\n",
    "tokenizer.enable_padding(pad_id=3, pad_token=\"[PAD]\", length=MAX_LEN)\n",
    "\n",
    "tokenizer.save(str(TOKENIZER_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load sentence pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_pairs = corpus.get_sentence_pairs()\n",
    "\n",
    "df = pd.DataFrame(sentences_pairs, columns=[\"sentence_1\", \"sentence_2_isnext\"])\n",
    "\n",
    "\n",
    "df_filtered = df[(df[\"sentence_1\"].apply(len) < MAX_SENTENCE_LEN)&\n",
    "                 (df[\"sentence_1\"].apply(len) > MIN_SENTENCE_LEN)&\n",
    "                 (df[\"sentence_2_isnext\"].apply(len) < MAX_SENTENCE_LEN)&\n",
    "                 (df[\"sentence_2_isnext\"].apply(len) > MIN_SENTENCE_LEN)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test for single sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " 2. Le 13 avril 2018, MM. [K] [B] et [J] [D] ont déposé plainte à l'encontre de M. [E] [I] du chef de harcèlement moral commis dans le cadre de ses fonctions de président d'université.\n",
      "['[CLS]', '2', '.', 'Le', '13', 'avril', '2018', ',', 'MM', '.', '[', 'K', ']', '[', 'B', ']', 'et', '[', 'J', ']', '[', 'D', ']', 'ont', 'déposé', 'plainte', 'à', 'l', \"'\", 'encontre', 'de', 'M', '.', '[', 'E', ']', '[', 'I', ']', 'du', 'chef', 'de', 'harcèlement', 'moral', 'commis', 'dans', 'le', 'cadre', 'de', 'ses', 'fonctions', 'de', 'président', 'd', \"'\", 'université', '.', '[SEP]']\n",
      "[1, 22, 18, 354, 457, 635, 1329, 16, 834, 18, 61, 45, 62, 61, 36, 62, 166, 61, 44, 62, 61, 38, 62, 424, 1651, 3674, 120, 77, 11, 830, 146, 47, 18, 61, 39, 62, 61, 43, 62, 161, 951, 146, 2547, 2227, 1408, 299, 149, 1661, 146, 396, 2984, 146, 289, 69, 11, 19659, 18, 2]\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.encode(df_filtered.loc[2,0])\n",
    "\n",
    "print(df_filtered.loc[2,0])\n",
    "print(output.tokens)\n",
    "print(output.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test for paired sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    \\n \\n 2. Le 13 avril 2018, MM. [K] [B] et [J] ...\n",
      "1     \\n \\n 3. Une enquête préliminaire a été ouverte.\n",
      "Name: 2, dtype: object\n",
      "['[CLS]', '2', '.', 'Le', '13', 'avril', '2018', ',', 'MM', '.', '[', 'K', ']', '[', 'B', ']', 'et', '[', 'J', ']', '[', 'D', ']', 'ont', 'déposé', 'plainte', 'à', 'l', \"'\", 'encontre', 'de', 'M', '.', '[', 'E', ']', '[', 'I', ']', 'du', 'chef', 'de', 'harcèlement', 'moral', 'commis', 'dans', 'le', 'cadre', 'de', 'ses', 'fonctions', 'de', 'président', 'd', \"'\", 'université', '.', '[SEP]', '3', '.', 'Une', 'enquête', 'préliminaire', 'a', 'été', 'ouverte', '.', '[SEP]']\n",
      "[1, 22, 18, 354, 457, 635, 1329, 16, 834, 18, 61, 45, 62, 61, 36, 62, 166, 61, 44, 62, 61, 38, 62, 424, 1651, 3674, 120, 77, 11, 830, 146, 47, 18, 61, 39, 62, 61, 43, 62, 161, 951, 146, 2547, 2227, 1408, 299, 149, 1661, 146, 396, 2984, 146, 289, 69, 11, 19659, 18, 2, 23, 18, 4013, 3228, 4633, 66, 203, 3910, 18, 2]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.encode(*df_filtered.loc[2,])\n",
    "\n",
    "print(df_filtered.loc[2,])\n",
    "print(output.tokens)\n",
    "print(output.ids)\n",
    "print(output.type_ids) # segment ids\n",
    "print(output.attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Il', 'résulte', 'de', 'l', \"'\", 'arrêt', 'attaqué', '.', '[SEP]', 'Le', '13', 'avril', '2018', '.', '[SEP]']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "['[CLS]', 'Le', '13', 'avril', '2018', '.', '[SEP]', 'Une', 'enquête', 'préliminaire', 'a', 'été', 'ouverte', '.', '[SEP]', '[PAD]']\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.encode_batch([[\"Il résulte de l'arrêt attaqué.\", \"Le 13 avril 2018.\"],\n",
    "                                [\"Le 13 avril 2018.\", \"Une enquête préliminaire a été ouverte.\"]])\n",
    "\n",
    "for out in output:\n",
    "    print(out.tokens)\n",
    "    print(out.type_ids) # segment ids\n",
    "    print(out.attention_mask)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2_isnext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n \\n 6. Les parties civiles ont interjeté ap...</td>\n",
       "      <td>Le ministère public a interjeté appel incident.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>»\\n \\n Réponse de la Cour\\n \\n 16.</td>\n",
       "      <td>Les moyens sont réunis.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>const., 2 mars 2018, décision n° 2017-693 QPC).</td>\n",
       "      <td>\\n \\n 19. La cassation est par conséquent enco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n \\n 19. La cassation est par conséquent enco...</td>\n",
       "      <td>\\n \\n Portée et conséquences de la cassation\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n \\n DIT n'y avoir lieu à renvoi ;</td>\n",
       "      <td>\\n \\n DÉCLARE irrecevable la demande de M. [L] ;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>\\n \\n 12. La cassation est par conséquent enco...</td>\n",
       "      <td>\\n \\n Portée et conséquences de la cassation\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>\\n \\n Réponse de la Cour\\n \\n 10.</td>\n",
       "      <td>Les moyens sont réunis.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>\\n \\n 14. La cassation est par conséquent enco...</td>\n",
       "      <td>\\n \\n Portée et conséquence de la cassation\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>\\n \\n Un mémoire a été produit.</td>\n",
       "      <td>\\n \\n Faits et procédure\\n \\n 1.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>\\n \\n 3. M. [G] a relevé appel de cette décision.</td>\n",
       "      <td>\\n \\n Examen du moyen \\n \\n Enoncé du moyen\\n ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>427 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence_1  \\\n",
       "0     \\n \\n 6. Les parties civiles ont interjeté ap...   \n",
       "1                   »\\n \\n Réponse de la Cour\\n \\n 16.   \n",
       "2      const., 2 mars 2018, décision n° 2017-693 QPC).   \n",
       "3    \\n \\n 19. La cassation est par conséquent enco...   \n",
       "4                  \\n \\n DIT n'y avoir lieu à renvoi ;   \n",
       "..                                                 ...   \n",
       "422  \\n \\n 12. La cassation est par conséquent enco...   \n",
       "423                  \\n \\n Réponse de la Cour\\n \\n 10.   \n",
       "424  \\n \\n 14. La cassation est par conséquent enco...   \n",
       "425                    \\n \\n Un mémoire a été produit.   \n",
       "426  \\n \\n 3. M. [G] a relevé appel de cette décision.   \n",
       "\n",
       "                                     sentence_2_isnext  \n",
       "0      Le ministère public a interjeté appel incident.  \n",
       "1                              Les moyens sont réunis.  \n",
       "2    \\n \\n 19. La cassation est par conséquent enco...  \n",
       "3    \\n \\n Portée et conséquences de la cassation\\n...  \n",
       "4     \\n \\n DÉCLARE irrecevable la demande de M. [L] ;  \n",
       "..                                                 ...  \n",
       "422  \\n \\n Portée et conséquences de la cassation\\n...  \n",
       "423                            Les moyens sont réunis.  \n",
       "424  \\n \\n Portée et conséquence de la cassation\\n ...  \n",
       "425                   \\n \\n Faits et procédure\\n \\n 1.  \n",
       "426  \\n \\n Examen du moyen \\n \\n Enoncé du moyen\\n ...  \n",
       "\n",
       "[427 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13295/1352146235.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"sentence_2_notnext\"] = train_col_1_shuffled.values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and test splits\n",
    "sentence_pair_split = int(0.9*len(df_filtered))\n",
    "\n",
    "df_train = df_filtered[:sentence_pair_split]\n",
    "df_eval = df_filtered[sentence_pair_split:].reset_index(drop=True)\n",
    "\n",
    "train_col_1_shuffled = df_train[\"sentence_2_isnext\"].sample(\n",
    "    frac=1, \n",
    "    random_state=212,\n",
    "    replace=True).reset_index(drop=True)\n",
    "df_train[\"sentence_2_notnext\"] = train_col_1_shuffled.values\n",
    "\n",
    "sum(train_col_1_shuffled == df_train[\"sentence_2_isnext\"]) == 0\n",
    "\n",
    "eval_col_1_shuffled = df_eval[\"sentence_2_isnext\"].sample(\n",
    "    frac=1,\n",
    "    random_state=23,\n",
    "    replace=True).reset_index(drop=True)\n",
    "df_eval[\"sentence_2_notnext\"] = eval_col_1_shuffled.values\n",
    "\n",
    "sum(eval_col_1_shuffled == df_eval[\"sentence_2_isnext\"]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_pair_train_isnext = tokenizer.encode_batch(\n",
    "    df_train[[\"sentence_1\", \"sentence_2_isnext\"]].values)\n",
    "\n",
    "sentence_pair_train_notnext = tokenizer.encode_batch(\n",
    "    df_train[[\"sentence_1\", \"sentence_2_notnext\"]].values)\n",
    "\n",
    "sentence_pair_val_isnext = tokenizer.encode_batch(\n",
    "    df_eval[[\"sentence_1\",\"sentence_2_isnext\"]].values)\n",
    "\n",
    "sentence_pair_val_notnext = tokenizer.encode_batch(\n",
    "    df_eval[[\"sentence_1\",\"sentence_2_isnext\"]].values)\n",
    "\n",
    "sentence_pair_train_data = (sentence_pair_train_isnext, sentence_pair_train_notnext)\n",
    "sentence_pair_val_data = (sentence_pair_val_isnext, sentence_pair_val_notnext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint, random\n",
    "\n",
    "CLS_TOKEN_ID = 0\n",
    "MASK_TOKEN_ID = 4\n",
    "\n",
    "def get_sentence_pair_batch(split, batch_size=BATCH_SIZE):\n",
    "    data = sentence_pair_train_data if split == 'train' else sentence_pair_val_data\n",
    "\n",
    "    data_isnext, data_notnext = data\n",
    "\n",
    "    pair_ix = torch.randint(len(data_isnext), (batch_size,))\n",
    "\n",
    "    max_pred_count = len(data_isnext[0])\n",
    "    \n",
    "    for i, ix  in enumerate(pair_ix):\n",
    "        is_next = i % 2 == 0\n",
    "\n",
    "        sentence_pair = data_isnext[ix] if is_next else data_notnext[ix]\n",
    "\n",
    "        available_mask = np.where(np.array(sentence_pair.special_tokens_mask) == 0)[0]\n",
    "        pred_count = min(max_pred_count, max(1, round(len(available_mask) * 0.15)))\n",
    "        \n",
    "        masked_positions = np.random.choice(available_mask, pred_count, replace=False)\n",
    "        masked_positions.sort()\n",
    "\n",
    "        masked_tokens = np.array(sentence_pair.ids)[masked_positions]\n",
    "\n",
    "        masked_token_ids = sentence_pair.ids.copy()\n",
    "        for masked_position in masked_positions:\n",
    "            if random() < 0.8:  # 80%\n",
    "                masked_token_ids[masked_position] = MASK_TOKEN_ID\n",
    "            elif random() < 0.5:  # 10%\n",
    "                index = randint(5, VOCAB_SIZE - 1) # random index in vocabulary\n",
    "                masked_token_ids[masked_position] = index\n",
    "\n",
    "        mask_padding = max_pred_count - len(masked_positions)\n",
    "    \n",
    "        yield [\n",
    "            sentence_pair.ids,\n",
    "            masked_token_ids,\n",
    "            sentence_pair.type_ids,\n",
    "            np.concatenate([masked_tokens, [CLS_TOKEN_ID] * mask_padding]),\n",
    "            np.concatenate([masked_positions, [CLS_TOKEN_ID] * mask_padding]),\n",
    "            sentence_pair.attention_mask,\n",
    "            is_next,\n",
    "        ]\n",
    "    # x, y = x.to(DEVICE), y.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_ids             \n",
      " tensor([[    1,    37,    17,  4777,    19,   365,    16, 25280,    30,   639,\n",
      "            30,    37,    30,  1630,    30, 12434,    31,     2,    37,   239,\n",
      "          5354,    70,    18,    66,  2385,   501,   742,  1415,    16,   851,\n",
      "            18,     2,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3]]) \n",
      "\n",
      "masked_token_ids_batch\n",
      " tensor([[    1,    37,    17,  4777,    19,   365,    16,     4,    30,   639,\n",
      "            30,    37,    30,  1630,     4, 12434,    31,     2,    37,   239,\n",
      "             4,     4,    18,    66,  2385,   501,   742,  1415,    16,   851,\n",
      "            18,     2,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3]]) \n",
      "\n",
      "segment_ids           \n",
      " tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]]) \n",
      "\n",
      "masked_token_batch    \n",
      " tensor([[25280,    30,  5354,    70,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]) \n",
      "\n",
      "masked_position_batch \n",
      " tensor([[ 7, 14, 20, 21,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0]]) \n",
      "\n",
      "attention_masks       \n",
      " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]]) \n",
      "\n",
      "is_next               \n",
      " tensor([1]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "(token_ids,\n",
    " masked_token_ids_batch,\n",
    " segment_ids,\n",
    " masked_token_batch,\n",
    " masked_position_batch,\n",
    " attention_masks,\n",
    " is_next) = map(\n",
    "    torch.LongTensor, \n",
    "    zip(*get_sentence_pair_batch(\"train\", batch_size=1)))\n",
    "\n",
    "print(\"token_ids             \\n\", token_ids, \"\\n\")\n",
    "print(\"masked_token_ids_batch\\n\", masked_token_ids_batch, \"\\n\")\n",
    "print(\"segment_ids           \\n\", segment_ids, \"\\n\")\n",
    "print(\"masked_token_batch    \\n\", masked_token_batch, \"\\n\")\n",
    "print(\"masked_position_batch \\n\", masked_position_batch, \"\\n\")\n",
    "print(\"attention_masks       \\n\", attention_masks, \"\\n\")\n",
    "print(\"is_next               \\n\", is_next, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    n_segments=N_SEGMENTS,\n",
    "    max_len=MAX_LEN,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_heads=ATTN_HEADS,\n",
    "    dropout=DROPOUT,\n",
    "    n_layers=N_LAYERS,\n",
    ")\n",
    "m = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 0.6983|10.1618,val loss 0.6976|10.1620\n",
      "step 10: train loss 0.6960|4.4415,val loss 0.6960|4.4434\n",
      "step 20: train loss 0.6939|0.3184,val loss 0.6941|0.3213\n",
      "step 30: train loss 0.6947|0.3423,val loss 0.6952|0.3385\n",
      "step 40: train loss 0.6928|0.2692,val loss 0.6939|0.2788\n",
      "step 50: train loss 0.6944|0.2632,val loss 0.6939|0.2637\n",
      "step 60: train loss 0.6912|0.2428,val loss 0.6934|0.2405\n",
      "step 70: train loss 0.6920|0.2200,val loss 0.6918|0.2209\n",
      "step 80: train loss 0.6938|0.2077,val loss 0.6936|0.2030\n",
      "step 90: train loss 0.7076|0.1996,val loss 0.7047|0.1969\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        clsf_losses = torch.zeros(EVAL_ITERS)\n",
    "        lm_losses = torch.zeros(EVAL_ITERS)\n",
    "        for k in range(EVAL_ITERS):\n",
    "            batch = map(\n",
    "                lambda x: torch.tensor(x, device=DEVICE, dtype=torch.long),\n",
    "                zip(*get_sentence_pair_batch(\"train\", BATCH_SIZE)))\n",
    "            _, token_ids, segment_ids, masked_tokens, masked_positions, attention_masks, is_next = batch\n",
    "            logits_lm, logits_clsf = model(token_ids, segment_ids, attention_masks, masked_positions)\n",
    "\n",
    "            loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens) # for masked LM\n",
    "            loss_lm = (loss_lm.float()).mean()\n",
    "\n",
    "            loss_clsf = criterion(logits_clsf, is_next) # for sentence classification\n",
    "            \n",
    "            clsf_losses[k] = loss_clsf.item()\n",
    "            lm_losses[k] = loss_lm.item()\n",
    "\n",
    "        out[split] = (clsf_losses.mean(), lm_losses.mean())\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for iter in range(MAX_ITERS):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % EVAL_INTERVAL == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train'][0]:.4f}|{losses['train'][1]:.4f},\" +\n",
    "              f\"val loss {losses['val'][0]:.4f}|{losses['val'][1]:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    batch = map(\n",
    "        lambda x: torch.tensor(x, device=DEVICE, dtype=torch.long),\n",
    "        zip(*get_sentence_pair_batch(\"train\", BATCH_SIZE)))\n",
    "    _, token_ids, segment_ids, masked_tokens, masked_positions, attention_masks, is_next = batch\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits_lm, logits_clsf = model(token_ids, segment_ids, attention_masks, masked_positions)\n",
    "\n",
    "    loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens) # for masked LM\n",
    "    loss_lm = (loss_lm.float()).mean()\n",
    "    loss_clsf = criterion(logits_clsf, is_next) # for sentence classification\n",
    "    loss = loss_lm + loss_clsf\n",
    "    \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sequence = torch.randint(\n",
    "    high=VOCAB_SIZE,\n",
    "    size=(BATCH_SIZE, MAX_LEN),\n",
    "    dtype=torch.long\n",
    ") # BATCH_SIZE, MAX_LEN\n",
    "\n",
    "sample_segment = torch.randint(\n",
    "    high=N_SEGMENTS,\n",
    "    size=(BATCH_SIZE, MAX_LEN),\n",
    "    dtype=torch.long\n",
    ") # BATCH_SIZE, MAX_LEN\n",
    "\n",
    "attention_masks = torch.randint(\n",
    "    high=2,\n",
    "    size=(BATCH_SIZE, MAX_LEN),\n",
    "    dtype=torch.long\n",
    ") # BATCH_SIZE, MAX_LEN\n",
    "\n",
    "masked_positions = torch.randint(\n",
    "    high=MAX_LEN,\n",
    "    size=(BATCH_SIZE, MAX_LEN),\n",
    "    dtype=torch.long\n",
    ") # BATCH_SIZE, MAX_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n",
      "torch.Size([4, 8, 30000])\n"
     ]
    }
   ],
   "source": [
    "model = BERT(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    n_segments=N_SEGMENTS,\n",
    "    max_len=MAX_LEN,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_heads=ATTN_HEADS,\n",
    "    dropout=DROPOUT,\n",
    "    n_layers=N_LAYERS,\n",
    ")\n",
    "\n",
    "logits_lm, logits_clsf = model(sample_sequence, sample_segment, attention_masks, masked_positions)\n",
    "print(logits_clsf.size())\n",
    "print(logits_lm.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 128])\n"
     ]
    }
   ],
   "source": [
    "from lm_from_scratch.models.bert import BERTEmbedding\n",
    "\n",
    "embedding = BERTEmbedding(VOCAB_SIZE, N_SEGMENTS, MAX_LEN, EMBED_DIM, DROPOUT)\n",
    "embedding_tensor = embedding(sample_sequence, sample_segment)\n",
    "print(embedding_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(MAX_LEN, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "# Example of target with class indices\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randint(5, (3,), dtype=torch.long)\n",
    "\n",
    "print(input.size())\n",
    "print(target.size())\n",
    "\n",
    "loss = F.cross_entropy(input, target)\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5])\n",
      "torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example of target with class probabilities\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5).softmax(dim=1)\n",
    "\n",
    "print(input.size())\n",
    "print(target.size())\n",
    "\n",
    "loss = F.cross_entropy(input, target)\n",
    "loss.backward()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
