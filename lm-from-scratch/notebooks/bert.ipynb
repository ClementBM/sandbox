{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from lm_from_scratch.models.bert import BERT\n",
    "from lm_from_scratch.corpus.decision_corpus import DecisionCorpus\n",
    "import pandas as pd\n",
    "from artifacts import TOKENIZER_PATH\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from random import randint, random\n",
    "\n",
    "VOCAB_SIZE = 15000\n",
    "N_SEGMENTS = 2\n",
    "MAX_LEN = 20# 128 # 512 # what is the maximum context length for predictions?\n",
    "EMBED_DIM = 384 # 768\n",
    "N_LAYERS = 3\n",
    "ATTN_HEADS = 6 # 32 * 4 = 128\n",
    "DROPOUT = 0.1\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "EVAL_ITERS = 10\n",
    "MAX_ITERS = 10000\n",
    "EVAL_INTERVAL = 1000\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "BATCH_SIZE = 64 # how many independent sequences will we process in parallel?\n",
    "\n",
    "MAX_SENTENCE_LEN = MAX_LEN // 2\n",
    "MIN_SENTENCE_LEN = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus and tokenizer setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CLS_TOKEN_ID = 0\n",
    "SEP_TOKEN_ID = 1\n",
    "PAD_TOKEN_ID = 2\n",
    "MASK_TOKEN_ID = 3\n",
    "UNK_TOKEN_ID = 4\n",
    "IGNORE_INDEX = -100\n",
    "\n",
    "corpus = DecisionCorpus()\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "trainer = BpeTrainer(vocab_size=VOCAB_SIZE,\n",
    "                     special_tokens=[\"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\", \"[UNK]\"])\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "tokenizer.train_from_iterator(corpus.get_text(), trainer)\n",
    "\n",
    "# post-processing to traditional BERT inputs\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# pad the outputs to the longest sentence present\n",
    "tokenizer.enable_padding(pad_id=PAD_TOKEN_ID, pad_token=\"[PAD]\", length=MAX_LEN)\n",
    "\n",
    "tokenizer.save(str(TOKENIZER_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load sentence pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162244"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_pairs = corpus.get_sentence_pairs()\n",
    "\n",
    "df = pd.DataFrame(sentences_pairs, columns=[\"sentence_1\", \"sentence_2_isnext\"])\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test for single sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizer.encode(df.loc[2,0])\n",
    "\n",
    "print(df.loc[2,0])\n",
    "print(output.tokens)\n",
    "print(output.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test for paired sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizer.encode(*df.loc[2,])\n",
    "\n",
    "print(df.loc[2,])\n",
    "print(output.tokens)\n",
    "print(output.ids)\n",
    "print(output.type_ids) # segment ids\n",
    "print(output.attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizer.encode_batch([[\"Il résulte de l'arrêt attaqué.\", \"Le 13 avril 2018.\"],\n",
    "                                [\"Le 13 avril 2018.\", \"Une enquête préliminaire a été ouverte.\"]])\n",
    "\n",
    "for out in output:\n",
    "    print(out.tokens)\n",
    "    print(out.type_ids) # segment ids\n",
    "    print(out.attention_mask)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4506/765054496.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"sentence_2_notnext\"] = train_col_1_shuffled.values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and test splits\n",
    "sentence_pair_split = int(0.9*len(df))\n",
    "\n",
    "df_train = df[:sentence_pair_split]\n",
    "df_eval = df[sentence_pair_split:].reset_index(drop=True)\n",
    "\n",
    "train_col_1_shuffled = df_train[\"sentence_2_isnext\"].sample(\n",
    "    frac=1, \n",
    "    random_state=212,\n",
    "    replace=True).reset_index(drop=True)\n",
    "df_train[\"sentence_2_notnext\"] = train_col_1_shuffled.values\n",
    "\n",
    "sum(train_col_1_shuffled == df_train[\"sentence_2_isnext\"]) == 0\n",
    "\n",
    "eval_col_1_shuffled = df_eval[\"sentence_2_isnext\"].sample(\n",
    "    frac=1,\n",
    "    random_state=23,\n",
    "    replace=True).reset_index(drop=True)\n",
    "df_eval[\"sentence_2_notnext\"] = eval_col_1_shuffled.values\n",
    "\n",
    "sum(eval_col_1_shuffled == df_eval[\"sentence_2_isnext\"]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_pair_train_isnext = tokenizer.encode_batch(\n",
    "    df_train[[\"sentence_1\", \"sentence_2_isnext\"]].values)\n",
    "\n",
    "sentence_pair_train_notnext = tokenizer.encode_batch(\n",
    "    df_train[[\"sentence_1\", \"sentence_2_notnext\"]].values)\n",
    "\n",
    "sentence_pair_val_isnext = tokenizer.encode_batch(\n",
    "    df_eval[[\"sentence_1\",\"sentence_2_isnext\"]].values)\n",
    "\n",
    "sentence_pair_val_notnext = tokenizer.encode_batch(\n",
    "    df_eval[[\"sentence_1\",\"sentence_2_isnext\"]].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_sentence_length(sentence_pairs_isnext, sentence_pairs_notnext, max_len):\n",
    "    pairs_isnext = []\n",
    "    pairs_notnext = []\n",
    "\n",
    "    for pair_isnext, pair_notnext in zip(sentence_pairs_isnext, sentence_pairs_notnext):\n",
    "        if len(pair_isnext) > max_len or len(pair_isnext) < MIN_SENTENCE_LEN:\n",
    "            continue\n",
    "        if len(pair_notnext) > max_len or len(pair_notnext) < MIN_SENTENCE_LEN:\n",
    "            continue\n",
    "        pairs_isnext.append(pair_isnext)\n",
    "        pairs_notnext.append(pair_notnext)\n",
    "    \n",
    "    return pairs_isnext, pairs_notnext\n",
    "\n",
    "sentence_pair_train_data = filter_by_sentence_length(\n",
    "                                sentence_pair_train_isnext, \n",
    "                                sentence_pair_train_notnext,\n",
    "                                MAX_LEN)\n",
    "\n",
    "sentence_pair_val_data = filter_by_sentence_length(\n",
    "                                sentence_pair_val_isnext,\n",
    "                                sentence_pair_val_notnext,\n",
    "                                MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_pair_batch(split, batch_size=BATCH_SIZE):\n",
    "    data_isnext, data_notnext = sentence_pair_train_data if split == 'train' else sentence_pair_val_data\n",
    "    pair_ix = torch.randint(len(data_isnext), (batch_size,))\n",
    "    max_pred_count = len(data_isnext[0])\n",
    "    \n",
    "    for i, ix  in enumerate(pair_ix):\n",
    "        is_next = i % 2 == 0\n",
    "\n",
    "        sentence_pair = data_isnext[ix] if is_next else data_notnext[ix]\n",
    "\n",
    "        available_mask = np.where(np.array(sentence_pair.special_tokens_mask) == 0)[0]\n",
    "        pred_count = min(max_pred_count, max(1, round(len(available_mask) * 0.15)))\n",
    "        \n",
    "        masked_positions = np.random.choice(available_mask, pred_count, replace=False)\n",
    "        masked_positions.sort()\n",
    "        \n",
    "        masked_token_ids = sentence_pair.ids.copy()\n",
    "        for masked_position in masked_positions:\n",
    "            if random() < 0.8:  # 80%\n",
    "                masked_token_ids[masked_position] = MASK_TOKEN_ID\n",
    "            elif random() < 0.5:  # 10%\n",
    "                index = randint(5, VOCAB_SIZE - 1) # random index in vocabulary\n",
    "                masked_token_ids[masked_position] = index\n",
    "\n",
    "        masked_tokens = np.array(sentence_pair.ids)[masked_positions]\n",
    "        mask_padding = max_pred_count - len(masked_positions)\n",
    "    \n",
    "        yield [\n",
    "            sentence_pair.ids,\n",
    "            masked_token_ids,\n",
    "            sentence_pair.type_ids,\n",
    "            np.concatenate([masked_tokens, [IGNORE_INDEX] * mask_padding]),\n",
    "            np.concatenate([masked_positions, [0] * mask_padding]),\n",
    "            sentence_pair.attention_mask,\n",
    "            is_next,\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_ids          \n",
      " tensor([[  0, 993, 149, 298,  31,   1, 850,  47,  18,  61,  40,  62, 304, 758,\n",
      "          31,   1,   2,   2,   2,   2]], device='cuda:0') \n",
      "\n",
      "masked_token_ids   \n",
      " tensor([[  0, 993, 149, 298,  31,   1, 850,  47,  18,   3,  40,   3, 304, 758,\n",
      "          31,   1,   2,   2,   2,   2]], device='cuda:0') \n",
      "\n",
      "segment_ids        \n",
      " tensor([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]],\n",
      "       device='cuda:0') \n",
      "\n",
      "masked_tokens      \n",
      " tensor([[  61,   62, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100]], device='cuda:0') \n",
      "\n",
      "masked_positions   \n",
      " tensor([[ 9, 11,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0]], device='cuda:0') \n",
      "\n",
      "attention_masks    \n",
      " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]],\n",
      "       device='cuda:0') \n",
      "\n",
      "is_next            \n",
      " tensor([1], device='cuda:0') \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4506/268473973.py:3: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  lambda x: torch.tensor(x, device=DEVICE, dtype=torch.long),\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split, batch_size):\n",
    "    return map(\n",
    "         lambda x: torch.tensor(x, device=DEVICE, dtype=torch.long), \n",
    "         zip(*get_sentence_pair_batch(split, batch_size=batch_size)))\n",
    "\n",
    "(token_ids,\n",
    " masked_token_ids,\n",
    " segment_ids,\n",
    " masked_tokens,\n",
    " masked_positions,\n",
    " attention_masks,\n",
    " is_next) = get_batch(\"train\", batch_size=1)\n",
    "\n",
    "print(\"token_ids          \\n\", token_ids, \"\\n\")\n",
    "print(\"masked_token_ids   \\n\", masked_token_ids, \"\\n\")\n",
    "print(\"segment_ids        \\n\", segment_ids, \"\\n\")\n",
    "print(\"masked_tokens      \\n\", masked_tokens, \"\\n\")\n",
    "print(\"masked_positions   \\n\", masked_positions, \"\\n\")\n",
    "print(\"attention_masks    \\n\", attention_masks, \"\\n\")\n",
    "print(\"is_next            \\n\", is_next, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    n_segments=N_SEGMENTS,\n",
    "    max_len=MAX_LEN,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_heads=ATTN_HEADS,\n",
    "    dropout=DROPOUT,\n",
    "    n_layers=N_LAYERS,\n",
    ")\n",
    "m = model.to(DEVICE)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# `ignore_index`: specifies a target value that is ignored\n",
    "# and does not contribute to the input gradient. When :attr:`size_average` is\n",
    "# `True`, the loss is averaged over non-ignored targets. Note that\n",
    "# Only applicable when the target contains class indices\n",
    "\n",
    "loss_fn_lm = torch.nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX)\n",
    "loss_fn_clsf = torch.nn.CrossEntropyLoss() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 0.7123|9.6134,val loss 0.7121|9.6099\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        clsf_losses = torch.zeros(EVAL_ITERS)\n",
    "        lm_losses = torch.zeros(EVAL_ITERS)\n",
    "        for k in range(EVAL_ITERS):\n",
    "            _, token_ids, segment_ids, masked_tokens, masked_positions, attention_masks, is_next = get_batch(\"train\", BATCH_SIZE)\n",
    "            logits_lm, logits_clsf = model(token_ids, segment_ids, attention_masks, masked_positions)\n",
    "\n",
    "            loss_lm = loss_fn_lm(logits_lm.transpose(-2,-1), masked_tokens)\n",
    "            loss_clsf = loss_fn_clsf(logits_clsf, is_next)\n",
    "            \n",
    "            clsf_losses[k] = loss_clsf.item()\n",
    "            lm_losses[k] = loss_lm.item()\n",
    "\n",
    "        out[split] = (clsf_losses.mean(), lm_losses.mean())\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "for iter in range(MAX_ITERS):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % EVAL_INTERVAL == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train'][0]:.4f}|{losses['train'][1]:.4f},\" +\n",
    "              f\"val loss {losses['val'][0]:.4f}|{losses['val'][1]:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    _, token_ids, segment_ids, masked_tokens, masked_positions, attention_masks, is_next = get_batch(\"train\", BATCH_SIZE)\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits_lm, logits_clsf = model(token_ids, segment_ids, attention_masks, masked_positions)\n",
    "    loss_lm = loss_fn_lm(logits_lm.transpose(-2,-1), masked_tokens) # for masked LM\n",
    "    loss_clsf = loss_fn_clsf(logits_clsf, is_next) # for sentence classification\n",
    "    loss = loss_lm + loss_clsf\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"[CLS]alors qu'il relevait que l'assignation[MASK] [SEP]\"\n",
    "\n",
    "test_ids = tokenizer.encode(test_sentence)\n",
    "\n",
    "test_token_ids = torch.tensor(test_ids.ids, dtype=torch.long, device=DEVICE)\n",
    "test_attn_mask = torch.tensor(test_ids.ids, dtype=torch.long, device=DEVICE) != PAD_TOKEN_ID\n",
    "test_segment = torch.zeros(MAX_LEN, device=DEVICE, dtype=torch.long)\n",
    "test_mask_position = torch.tensor(test_ids.ids, dtype=torch.long, device=DEVICE) == MASK_TOKEN_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_lm, logits_clsf = model(\n",
    "      test_token_ids[None, :],\n",
    "      test_segment[None, :],\n",
    "      test_attn_mask[None, :].long(),\n",
    "      test_mask_position[None, :].long())\n",
    "\n",
    "tokenizer.decode(torch.argmax(logit_lm[0], dim=1).cpu().numpy(), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diligences\n",
      "[CLS] Dit que sur les [MASK] du procureur général près la Cour de cassation , le présent arrêt sera transmis pour être transcrit en marge ou à la suite de la décision cassée ; [SEP] Ainsi fait et jugé par la Cour de cassation , première chambre civile , et prononcé par le président en son audience publique du huit mars deux mille vingt - trois . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "tensor([[   0, 1514,  182,  295,  194,    3,  161,  596,  423,  329,  153,  265,\n",
      "          146,  253,   16,  149,  333,  259, 1296, 1409,  217,  486, 1571,  151,\n",
      "         1364,  211,  120,  153,  888,  146,  153,  360, 8163,   31,    1,  643,\n",
      "          381,  166,  878,  183,  153,  265,  146,  253,   16,  879,  280,  309,\n",
      "           16,  166,  543,  183,  149,  289,  151,  239,  410,  341,  161, 2152,\n",
      "          541,  389,  662,  537,   17,  554,   18,    1,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "(token_ids,\n",
    " masked_token_ids,\n",
    " segment_ids,\n",
    " masked_tokens,\n",
    " masked_positions,\n",
    " attention_masks,\n",
    " is_next) = get_batch(\"test\", batch_size=1)\n",
    "\n",
    "print(tokenizer.decode([token_ids[0, 5]], skip_special_tokens=False))\n",
    "\n",
    "masked_tokens = torch.ones((1, MAX_LEN), dtype=torch.long, device=DEVICE) * -100\n",
    "masked_tokens[0,0] = token_ids[0, 5]\n",
    "token_ids[0, 5] = MASK_TOKEN_ID\n",
    "\n",
    "masked_positions = torch.zeros((1, MAX_LEN), dtype=torch.long, device=DEVICE)\n",
    "masked_positions[0,0] = 5\n",
    "\n",
    "print(tokenizer.decode(token_ids[0].cpu().numpy(), skip_special_tokens=False))\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diligences être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être être\n",
      "[1410  486  486  486  486  486  486  486  486  486  486  486  486  486\n",
      "  486  486  486  486  486  486  486  486  486  486  486  486  486  486\n",
      "  486  486  486  486  486  486  486  486  486  486  486  486  486  486\n",
      "  486  486  486  486  486  486  486  486  486  486  486  486  486  486\n",
      "  486  486  486  486  486  486  486  486  486  486  486  486  486  486\n",
      "  486  486  486  486  486  486  486  486  486  486  486  486  486  486\n",
      "  486  486  486  486  486  486  486  486  486  486  486  486  486  486\n",
      "  486  486  486  486  486  486  486  486  486  486  486  486  486  486\n",
      "  486  486  486  486  486  486  486  486  486  486  486  486  486  486\n",
      "  486  486]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "logit_lm, logits_clsf = model(token_ids, segment_ids, attention_masks, masked_positions)\n",
    "model.train()\n",
    "\n",
    "predicted_masked_tok = torch.argmax(logit_lm, dim=2).cpu().numpy()\n",
    "\n",
    "print(tokenizer.decode(predicted_masked_tok[0], skip_special_tokens=True))\n",
    "print(predicted_masked_tok[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn_lm(logit_lm.view(-1,VOCAB_SIZE) , masked_tokens.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_tokens[masked_tokens == -100] = 0\n",
    "loss_fn_lm(logit_lm.view(-1,VOCAB_SIZE) , masked_tokens.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(logit_lm[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
