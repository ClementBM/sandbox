{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/hkproj/pytorch-transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm_from_scratch.models.transformer import build_transformer\n",
    "from lm_from_scratch.bilingual_dataset import BilingualDataset, causal_mask\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Huggingface datasets and tokenizers\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "import torchmetrics\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    return {\n",
    "        \"batch_size\": 8,\n",
    "        \"num_epochs\": 5,\n",
    "        \"lr\": 10**-4,\n",
    "        \"seq_len\": 350,\n",
    "        \"d_model\": 512,\n",
    "        \"datasource\": 'opus_books',\n",
    "        \"lang_src\": \"en\",\n",
    "        \"lang_tgt\": \"it\",\n",
    "        \"model_folder\": \"weights\",\n",
    "        \"model_basename\": \"tmodel_\",\n",
    "        \"preload\": \"latest\",\n",
    "        \"tokenizer_file\": \"tokenizer_{0}.json\",\n",
    "        \"experiment_name\": \"runs/tmodel\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "    # Precompute the encoder output and reuse it for every step\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "    # Initialize the decoder input with the sos token\n",
    "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "\n",
    "        # build mask for target\n",
    "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "\n",
    "        # calculate output\n",
    "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "        # get next token\n",
    "        prob = model.project(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        decoder_input = torch.cat(\n",
    "            [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
    "        )\n",
    "\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "\n",
    "    return decoder_input.squeeze(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_step, writer, num_examples=2):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "\n",
    "    source_texts = []\n",
    "    expected = []\n",
    "    predicted = []\n",
    "\n",
    "    try:\n",
    "        # get the console window width\n",
    "        with os.popen('stty size', 'r') as console:\n",
    "            _, console_width = console.read().split()\n",
    "            console_width = int(console_width)\n",
    "    except:\n",
    "        # If we can't get the console width, use 80 as default\n",
    "        console_width = 80\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_ds:\n",
    "            count += 1\n",
    "            encoder_input = batch[\"encoder_input\"].to(device) # (b, seq_len)\n",
    "            encoder_mask = batch[\"encoder_mask\"].to(device) # (b, 1, 1, seq_len)\n",
    "\n",
    "            # check that the batch size is 1\n",
    "            assert encoder_input.size(\n",
    "                0) == 1, \"Batch size must be 1 for validation\"\n",
    "\n",
    "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "\n",
    "            source_text = batch[\"src_text\"][0]\n",
    "            target_text = batch[\"tgt_text\"][0]\n",
    "            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n",
    "\n",
    "            source_texts.append(source_text)\n",
    "            expected.append(target_text)\n",
    "            predicted.append(model_out_text)\n",
    "            \n",
    "            # Print the source, target and model output\n",
    "            print_msg('-'*console_width)\n",
    "            print_msg(f\"{f'SOURCE: ':>12}{source_text}\")\n",
    "            print_msg(f\"{f'TARGET: ':>12}{target_text}\")\n",
    "            print_msg(f\"{f'PREDICTED: ':>12}{model_out_text}\")\n",
    "\n",
    "            if count == num_examples:\n",
    "                print_msg('-'*console_width)\n",
    "                break\n",
    "    \n",
    "    if writer:\n",
    "        # Evaluate the character error rate\n",
    "        # Compute the char error rate \n",
    "        metric = torchmetrics.CharErrorRate()\n",
    "        cer = metric(predicted, expected)\n",
    "        writer.add_scalar('validation cer', cer, global_step)\n",
    "        writer.flush()\n",
    "\n",
    "        # Compute the word error rate\n",
    "        metric = torchmetrics.WordErrorRate()\n",
    "        wer = metric(predicted, expected)\n",
    "        writer.add_scalar('validation wer', wer, global_step)\n",
    "        writer.flush()\n",
    "\n",
    "        # Compute the BLEU metric\n",
    "        metric = torchmetrics.BLEUScore()\n",
    "        bleu = metric(predicted, expected)\n",
    "        writer.add_scalar('validation BLEU', bleu, global_step)\n",
    "        writer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(ds, lang):\n",
    "    for item in ds:\n",
    "        yield item['translation'][lang]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_build_tokenizer(config, ds, lang):\n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        # Most code taken from: https://huggingface.co/docs/tokenizers/quicktour\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds(config):\n",
    "    # It only has the train split, so we divide it overselves\n",
    "    ds_raw = load_dataset(f\"{config['datasource']}\", f\"{config['lang_src']}-{config['lang_tgt']}\", split='train')\n",
    "\n",
    "    # Build tokenizers\n",
    "    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config['lang_src'])\n",
    "    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config['lang_tgt'])\n",
    "\n",
    "    # Keep 90% for training, 10% for validation\n",
    "    train_ds_size = int(0.9 * len(ds_raw))\n",
    "    val_ds_size = len(ds_raw) - train_ds_size\n",
    "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
    "\n",
    "    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "\n",
    "    # Find the maximum length of each sentence in the source and target sentence\n",
    "    max_len_src = 0\n",
    "    max_len_tgt = 0\n",
    "\n",
    "    for item in ds_raw:\n",
    "        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n",
    "        tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids\n",
    "        max_len_src = max(max_len_src, len(src_ids))\n",
    "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "\n",
    "    print(f'Max length of source sentence: {max_len_src}')\n",
    "    print(f'Max length of target sentence: {max_len_tgt}')\n",
    "    \n",
    "\n",
    "    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
    "\n",
    "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
    "    model = build_transformer(vocab_src_len, vocab_tgt_len, config[\"seq_len\"], config['seq_len'], d_model=config['d_model'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n",
    "device = torch.device(device)\n",
    "\n",
    "config = get_config()\n",
    "\n",
    "# Make sure the weights folder exists\n",
    "Path(f\"{config['datasource']}_{config['model_folder']}\").mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of source sentence: 309\n",
      "Max length of target sentence: 274\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
    "model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "# Tensorboard\n",
    "writer = SummaryWriter(config['experiment_name'])\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
    "\n",
    "# If the user specified a model to preload before training, load it\n",
    "initial_epoch = 0\n",
    "global_step = 0\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 00: 100%|██████████| 3638/3638 [21:50<00:00,  2.78it/s, loss=6.393]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: It was an undignified position for him. A rude boy on the bank immediately yelled out to a lagging chum to \"hurry up and see real monkey on a stick.\"\n",
      "    TARGET: Un monello ch’era sulla riva, immediatamente strillò a un compagno, che lo seguiva, di «correre a vedere una scimmia aggrappata a un bastone».\n",
      " PREDICTED: Ma il signor Rochester , che la sua vita , e il signor Rochester , e , e .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clem/Source/sandbox/lm-from-scratch/.venv/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `CharErrorRate` from `torchmetrics` was deprecated and will be removed in 2.0. Import `CharErrorRate` from `torchmetrics.text` instead.\n",
      "  _future_warning(\n",
      "/home/clem/Source/sandbox/lm-from-scratch/.venv/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `WordErrorRate` from `torchmetrics` was deprecated and will be removed in 2.0. Import `WordErrorRate` from `torchmetrics.text` instead.\n",
      "  _future_warning(\n",
      "/home/clem/Source/sandbox/lm-from-scratch/.venv/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `BLEUScore` from `torchmetrics` was deprecated and will be removed in 2.0. Import `BLEUScore` from `torchmetrics.text` instead.\n",
      "  _future_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Dolly, hearing their screams, ran up to the nursery and found them in a dreadful state. Tanya was holding Grisha by the hair, and he, his face distorted with anger, was hitting her at random with his fists.\n",
      "    TARGET: Dar’ja Aleksandrovna, sentendo gridare nella camera dei bambini, era accorsa e li aveva trovati avvinti in modo orribile: Tanja aveva afferrato Griša per i capelli e questi, col volto mostruoso di cattiveria, tirava pugni dove capitava.\n",
      " PREDICTED: Levin si mise a sé , e si mise a sé , e che si mise a sé , e che si mise a sé , e che si , e si , e si .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 01: 100%|██████████| 3638/3638 [21:29<00:00,  2.82it/s, loss=3.730]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Lastly, I saw Mr. Mason was submissive to Mr. Rochester; that the impetuous will of the latter held complete sway over the inertness of the former: the few words which had passed between them assured me of this. It was evident that in their former intercourse, the passive disposition of the one had been habitually influenced by the active energy of the other: whence then had arisen Mr. Rochester's dismay when he heard of Mr. Mason's arrival?\n",
      "    TARGET: Avevo veduto il signor Mason sottomettersi alla volontà imperiosa del signor Rochester, le poche parole che avevano scambiate ne erano una prova; era evidente che nelle loro relazioni precedenti le disposizioni passive di uno avevano subito l'influenza dell'energia attiva dell'altro. Ma perché il signor Rochester si era tanto turbato, sapendo che il signor Mason era giunto?\n",
      " PREDICTED: \" , ma io mi , ma il signor Rochester , che il mio , che il mio , che mi in un momento di , che mi in un momento di , che mi in un momento di .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: CHAPTER V\n",
      "    TARGET: V\n",
      " PREDICTED: CAPITOLO X\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 02: 100%|██████████| 3638/3638 [21:26<00:00,  2.83it/s, loss=4.803]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Harris said he had had quite a fight with these two swans; but courage and skill had prevailed in the end, and he had defeated them.\n",
      "    TARGET: Harris raccontava che aveva sostenuto una vera battaglia coi due cigni, ma che il suo coraggio e la sua abilità erano prevalsi, sbaragliandoli.\n",
      " PREDICTED: Harris disse che era stato un altro che aveva fatto un altro , ma che aveva fatto il tempo di e di .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Two of them were already riding toward the starting-point.\n",
      "    TARGET: Due andavano avanti verso il luogo donde dovevano partire.\n",
      " PREDICTED: Dopo , in due volte , in modo di nuovo la propria situazione .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 03: 100%|██████████| 3638/3638 [21:36<00:00,  2.81it/s, loss=5.035]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Either we must separate or live together.'\n",
      "    TARGET: O ci dobbiamo separare, o vivere insieme.\n",
      " PREDICTED: o no , o no .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: While working he sometimes forgot for some minutes what he was about, and felt quite at ease; then his mowing was nearly as even as that of Titus.\n",
      "    TARGET: Mentre lavorava, aveva dei momenti nei quali dimenticava quello che faceva, si sentiva leggero, e proprio in quei momenti la falciata gli veniva fuori uguale e bella quasi come quella di Tit.\n",
      " PREDICTED: Mentre egli si di nuovo , era stato stato stato un momento , e , per lui , era stato stato stato più forte che era stato stato stato .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 04: 100%|██████████| 3638/3638 [21:20<00:00,  2.84it/s, loss=3.779]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"A good man. Does that mean a respectable well-conducted man of fifty? Or what does it mean?\"\n",
      "    TARGET: — Buono, significa forse un uomo di cinquant'anni, che si conduce bene?\n",
      " PREDICTED: — Un uomo che è vero che sia un uomo di di ?\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: They said they were very sorry, but that they owed it to their families not to be fool-hardy.\n",
      "    TARGET: Dicevano ch’erano dolenti, ma per riguardo alle loro famiglie non potevano esser temerarie.\n",
      " PREDICTED: , ma non si , ma che si a .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(initial_epoch, config['num_epochs']):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.train()\n",
    "    batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
    "    for batch in batch_iterator:\n",
    "        encoder_input = batch['encoder_input'].to(device) # (b, seq_len)\n",
    "        decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n",
    "        encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n",
    "        decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n",
    "\n",
    "        # Run the tensors through the encoder, decoder and the projection layer\n",
    "        encoder_output = model.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n",
    "        decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n",
    "        proj_output = model.project(decoder_output) # (B, seq_len, vocab_size)\n",
    "\n",
    "        # Compare the output with the label\n",
    "        label = batch['label'].to(device) # (B, seq_len)\n",
    "\n",
    "        # Compute the loss using a simple cross entropy\n",
    "        loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
    "        batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "        # Log the loss\n",
    "        writer.add_scalar('train loss', loss.item(), global_step)\n",
    "        writer.flush()\n",
    "\n",
    "        # Backpropagate the loss\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        global_step += 1\n",
    "\n",
    "    # Run validation at the end of every epoch\n",
    "    run_validation(model, val_dataloader,\n",
    "                   tokenizer_src, tokenizer_tgt,\n",
    "                   config['seq_len'], device,\n",
    "                   lambda msg: batch_iterator.write(msg), global_step, writer)\n",
    "\n",
    "    # Save the model at the end of every epoch\n",
    "    # model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
    "    # torch.save({\n",
    "    #     'epoch': epoch,\n",
    "    #     'model_state_dict': model.state_dict(),\n",
    "    #     'optimizer_state_dict': optimizer.state_dict(),\n",
    "    #     'global_step': global_step\n",
    "    # }, model_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
