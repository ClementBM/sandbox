{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm_from_scratch.corpus.decision_corpus import DecisionCorpus\n",
    "from artifacts import DECISION_CORPUS_RAW\n",
    "from transformers import T5Tokenizer\n",
    "from tokenizers import AddedToken\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "VOCAB_SIZE = 900\n",
    "N_SEGMENTS = 2\n",
    "MAX_LEN = 128 # 512 # what is the maximum context length for predictions?\n",
    "\n",
    "BATCH_SIZE = 32 # how many independent sequences will we process in parallel?\n",
    "\n",
    "MAX_SENTENCE_LEN = MAX_LEN // 2\n",
    "MIN_SENTENCE_LEN = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = DecisionCorpus()\n",
    "\n",
    "corpus_df = corpus.df.sample(\n",
    "        frac=1,\n",
    "        random_state=42\n",
    "    ).reset_index(\n",
    "        drop=True)\n",
    "\n",
    "# Train and test splits\n",
    "n = int(0.9*len(corpus_df)) # first 90% will be train, rest val\n",
    "\n",
    "data = corpus.get_text()\n",
    "\n",
    "with open(DECISION_CORPUS_RAW, \"w\", encoding=\"utf-8\") as f:\n",
    "    for d in data:\n",
    "        f.write(d + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /home/clem/Source/sandbox/lm-from-scratch/artifacts/decision-raw.txt\n",
      "  input_format: \n",
      "  model_prefix: sentencepiece_tokenizer\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 900\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 2\n",
      "  bos_id: 3\n",
      "  eos_id: 1\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  â‡ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: /home/clem/Source/sandbox/lm-from-scratch/artifacts/decision-raw.txt\n",
      "trainer_interface.cc(378) LOG(WARNING) Found too long line (4331 > 4192).\n",
      "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 796051 sentences\n",
      "trainer_interface.cc(414) LOG(INFO) Skipped 42 too long sentences.\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=45745011\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9581% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=89\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999581\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 513172 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=29440726\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 192304 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 513172\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 107525\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 107525 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=64446 obj=11.3858 num_tokens=226542 num_tokens/piece=3.51522\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=52924 obj=8.55441 num_tokens=228028 num_tokens/piece=4.30859\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=39691 obj=8.52452 num_tokens=241971 num_tokens/piece=6.09637\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=39684 obj=8.51019 num_tokens=242029 num_tokens/piece=6.09891\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=29762 obj=8.54193 num_tokens=264640 num_tokens/piece=8.89188\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=29761 obj=8.53417 num_tokens=264632 num_tokens/piece=8.89191\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=22320 obj=8.57868 num_tokens=290039 num_tokens/piece=12.9946\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=22320 obj=8.57133 num_tokens=290053 num_tokens/piece=12.9952\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=16740 obj=8.62747 num_tokens=316257 num_tokens/piece=18.8923\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=16740 obj=8.61817 num_tokens=316229 num_tokens/piece=18.8906\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=12555 obj=8.69667 num_tokens=341798 num_tokens/piece=27.2241\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=12555 obj=8.68361 num_tokens=341773 num_tokens/piece=27.2221\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=9416 obj=8.79044 num_tokens=368642 num_tokens/piece=39.1506\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=9416 obj=8.77313 num_tokens=368653 num_tokens/piece=39.1518\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=7062 obj=8.91379 num_tokens=398371 num_tokens/piece=56.4105\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=7062 obj=8.89052 num_tokens=398376 num_tokens/piece=56.4112\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=5296 obj=9.08206 num_tokens=425992 num_tokens/piece=80.4366\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=5296 obj=9.04554 num_tokens=425939 num_tokens/piece=80.4265\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=3972 obj=9.28902 num_tokens=449244 num_tokens/piece=113.103\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=3972 obj=9.24891 num_tokens=449256 num_tokens/piece=113.106\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2979 obj=9.55016 num_tokens=474505 num_tokens/piece=159.283\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2979 obj=9.49921 num_tokens=474505 num_tokens/piece=159.283\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2234 obj=9.88166 num_tokens=504051 num_tokens/piece=225.627\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2234 obj=9.81279 num_tokens=504044 num_tokens/piece=225.624\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=1675 obj=10.2767 num_tokens=535873 num_tokens/piece=319.924\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=1675 obj=10.2069 num_tokens=535868 num_tokens/piece=319.921\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=1256 obj=10.7489 num_tokens=571293 num_tokens/piece=454.851\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=1256 obj=10.6619 num_tokens=571379 num_tokens/piece=454.92\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=990 obj=11.1231 num_tokens=591762 num_tokens/piece=597.739\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=990 obj=11.0455 num_tokens=591782 num_tokens/piece=597.76\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: sentencepiece_tokenizer.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: sentencepiece_tokenizer.vocab\n"
     ]
    }
   ],
   "source": [
    "PAD_TOKEN_ID = 0\n",
    "EOS_TOKEN_ID = 1\n",
    "UNK_TOKEN_ID = 2\n",
    "BOS_TOKEN_ID = 3\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=DECISION_CORPUS_RAW,\n",
    "    model_prefix='sentencepiece_tokenizer',\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    pad_id=PAD_TOKEN_ID,\n",
    "    unk_id=UNK_TOKEN_ID,\n",
    "    eos_id=EOS_TOKEN_ID,\n",
    "    bos_id=BOS_TOKEN_ID,\n",
    "    pad_piece='<pad>',\n",
    "    unk_piece='<unk>',\n",
    "    eos_piece='</s>',\n",
    "    bos_piece='<s>',\n",
    "    model_type='unigram',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer = spm.SentencePieceProcessor(model_file=\"sentencepiece_tokenizer.model\")\n",
    "\n",
    "tokenizer = T5Tokenizer(\"sentencepiece_tokenizer.model\", extra_ids=0)\n",
    "\n",
    "tokenizer.add_special_tokens({\n",
    "    \"additional_special_tokens\": [\n",
    "        AddedToken(content=f\"<extra_id_{i}>\", single_word=False, normalized=False, special=True) for i in range(100)\n",
    "    ]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'pad_token': '<pad>',\n",
       " 'additional_special_tokens': ['<extra_id_39>',\n",
       "  '<extra_id_95>',\n",
       "  '<extra_id_93>',\n",
       "  '<extra_id_79>',\n",
       "  '<extra_id_32>',\n",
       "  '<extra_id_15>',\n",
       "  '<extra_id_73>',\n",
       "  '<extra_id_36>',\n",
       "  '<extra_id_1>',\n",
       "  '<extra_id_19>',\n",
       "  '<extra_id_26>',\n",
       "  '<extra_id_60>',\n",
       "  '<extra_id_51>',\n",
       "  '<extra_id_5>',\n",
       "  '<extra_id_3>',\n",
       "  '<extra_id_31>',\n",
       "  '<extra_id_58>',\n",
       "  '<extra_id_23>',\n",
       "  '<extra_id_50>',\n",
       "  '<extra_id_78>',\n",
       "  '<extra_id_96>',\n",
       "  '<extra_id_72>',\n",
       "  '<extra_id_74>',\n",
       "  '<extra_id_4>',\n",
       "  '<extra_id_48>',\n",
       "  '<extra_id_42>',\n",
       "  '<extra_id_90>',\n",
       "  '<extra_id_82>',\n",
       "  '<extra_id_57>',\n",
       "  '<extra_id_34>',\n",
       "  '<extra_id_16>',\n",
       "  '<extra_id_33>',\n",
       "  '<extra_id_35>',\n",
       "  '<extra_id_69>',\n",
       "  '<extra_id_14>',\n",
       "  '<extra_id_65>',\n",
       "  '<extra_id_88>',\n",
       "  '<extra_id_97>',\n",
       "  '<extra_id_94>',\n",
       "  '<extra_id_27>',\n",
       "  '<extra_id_80>',\n",
       "  '<extra_id_41>',\n",
       "  '<extra_id_55>',\n",
       "  '<extra_id_68>',\n",
       "  '<extra_id_98>',\n",
       "  '<extra_id_24>',\n",
       "  '<extra_id_22>',\n",
       "  '<extra_id_49>',\n",
       "  '<extra_id_91>',\n",
       "  '<extra_id_64>',\n",
       "  '<extra_id_99>',\n",
       "  '<extra_id_13>',\n",
       "  '<extra_id_45>',\n",
       "  '<extra_id_29>',\n",
       "  '<extra_id_11>',\n",
       "  '<extra_id_87>',\n",
       "  '<extra_id_47>',\n",
       "  '<extra_id_37>',\n",
       "  '<extra_id_86>',\n",
       "  '<extra_id_63>',\n",
       "  '<extra_id_43>',\n",
       "  '<extra_id_25>',\n",
       "  '<extra_id_21>',\n",
       "  '<extra_id_7>',\n",
       "  '<extra_id_92>',\n",
       "  '<extra_id_56>',\n",
       "  '<extra_id_17>',\n",
       "  '<extra_id_9>',\n",
       "  '<extra_id_81>',\n",
       "  '<extra_id_20>',\n",
       "  '<extra_id_30>',\n",
       "  '<extra_id_44>',\n",
       "  '<extra_id_77>',\n",
       "  '<extra_id_89>',\n",
       "  '<extra_id_52>',\n",
       "  '<extra_id_83>',\n",
       "  '<extra_id_85>',\n",
       "  '<extra_id_75>',\n",
       "  '<extra_id_0>',\n",
       "  '<extra_id_12>',\n",
       "  '<extra_id_54>',\n",
       "  '<extra_id_62>',\n",
       "  '<extra_id_76>',\n",
       "  '<extra_id_70>',\n",
       "  '<extra_id_28>',\n",
       "  '<extra_id_8>',\n",
       "  '<extra_id_84>',\n",
       "  '<extra_id_2>',\n",
       "  '<extra_id_61>',\n",
       "  '<extra_id_10>',\n",
       "  '<extra_id_53>',\n",
       "  '<extra_id_6>',\n",
       "  '<extra_id_67>',\n",
       "  '<extra_id_71>',\n",
       "  '<extra_id_66>',\n",
       "  '<extra_id_46>',\n",
       "  '<extra_id_18>',\n",
       "  '<extra_id_38>',\n",
       "  '<extra_id_59>',\n",
       "  '<extra_id_40>']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('<pad></s><unk>')\n",
    "tokenizer.decode([0,1,2,3,4,5])\n",
    "list(tokenizer.get_vocab().items())[:10]\n",
    "tokenizer.special_tokens_map\n",
    "\n",
    "# Returns the number of added tokens when encoding a sequence with special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'COUR DE CASSATION, CHAMBRE CRIMINEL'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_test = data[0][280:315].replace(\"\\n\",\"\").replace(\"_\", \"\")\n",
    "text_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[215, 123, 205, 4, 391, 226, 766, 139, 88, 198, 132, 174]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ids = tokenizer.encode(text_test, add_special_tokens=False)\n",
    "text_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'COUR DE CASSATION, CHAMBRE CRIMINEL'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(text_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[215, 123, 205, 4, 391, 226, 766, 139, 88, 198, 132, 174, 1, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]], 'labels': [[215, 123, 205, 4, 391, 226, 766, 139, 88, 198, 132, 174, 1, 0, 0]]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.prepare_seq2seq_batch(\n",
    "    src_texts=[text_test],\n",
    "    tgt_texts=[text_test],\n",
    "    max_length=15,\n",
    "    max_target_length=15,\n",
    "    padding=\"max_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [215, 123, 205, 4, 391, 226, 766, 139, 88, 198, 132, 174, 1, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(text=text_test,\n",
    "          text_target=None,\n",
    "          padding=\"max_length\",\n",
    "          stride=2,\n",
    "          max_length=15,\n",
    "          truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [215, 123, 205, 4, 391, 226, 766, 139, 88, 198, 132, 174, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.prepare_for_model(\n",
    "    text_ids, max_length=20, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare with pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "pretrained_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad></s><unk>X.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_tokenizer.special_tokens_map\n",
    "list(pretrained_tokenizer.get_vocab().items())[:10]\n",
    "pretrained_tokenizer.encode('<pad></s><unk>')\n",
    "pretrained_tokenizer.encode('<s>')\n",
    "pretrained_tokenizer.decode([0,1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3,\n",
       " 6727,\n",
       " 5693,\n",
       " 329,\n",
       " 3,\n",
       " 7874,\n",
       " 276,\n",
       " 12062,\n",
       " 27872,\n",
       " 377,\n",
       " 16375,\n",
       " 2,\n",
       " 25018,\n",
       " 71,\n",
       " 12224,\n",
       " 2,\n",
       " 382,\n",
       " 3396,\n",
       " 5292,\n",
       " 1]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_tokenizer.encode(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AU NOM DU PEUPLE FRAN<unk> AIS ARR<unk> T DE LA</s>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_tokenizer.decode(pretrained_tokenizer.encode(text_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [215, 123, 205, 4, 391, 226, 766, 139, 88, 198, 132, 174, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_tokenizer.prepare_for_model(\n",
    "    text_ids, max_length=20, truncation=True, padding=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
