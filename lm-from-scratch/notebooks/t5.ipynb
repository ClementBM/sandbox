{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from lm_from_scratch.models.t5 import T5\n",
    "from lm_from_scratch.corpus.decision_corpus import DecisionCorpus\n",
    "import pandas as pd\n",
    "from artifacts import DECISION_CORPUS_RAW\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sentencepiece as spm\n",
    "from random import randint, random\n",
    "from transformers import T5Tokenizer\n",
    "from tokenizers import AddedToken\n",
    "\n",
    "VOCAB_SIZE = 15000\n",
    "MAX_LEN = 128 # 512 # what is the maximum context length for predictions?\n",
    "EMBED_DIM = 384 # 768\n",
    "N_LAYERS = 1\n",
    "ATTN_HEADS = 6 # 64 * 6 = 384\n",
    "DROPOUT = 0.0\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "EVAL_ITERS = 10\n",
    "MAX_ITERS = 1000\n",
    "EVAL_INTERVAL = 100\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "BATCH_SIZE = 64 # how many independent sequences will we process in parallel?\n",
    "\n",
    "MAX_SENTENCE_LEN = MAX_LEN // 2\n",
    "MIN_SENTENCE_LEN = 10\n",
    "\n",
    "PAD_TOKEN_ID = 0\n",
    "EOS_TOKEN_ID = 1\n",
    "UNK_TOKEN_ID = 2\n",
    "BOS_TOKEN_ID = 3\n",
    "\n",
    "SENTINEL_TOKEN_COUNT = 200\n",
    "SENTINEL_TOKEN_ID = VOCAB_SIZE - SENTINEL_TOKEN_COUNT\n",
    "IGNORE_INDEX = -100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To specify which task the model should perform, we add a task-specific (text) prefix to the original input sequence before feeding it to the model.\n",
    "\n",
    "See Appendix D for full examples of preprocessed inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = DecisionCorpus()\n",
    "data = corpus.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /home/clem/Source/sandbox/lm-from-scratch/artifacts/decision-raw.txt\n",
      "  input_format: \n",
      "  model_prefix: sentencepiece_tokenizer\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 14800\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 2\n",
      "  bos_id: 3\n",
      "  eos_id: 1\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ��� \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: /home/clem/Source/sandbox/lm-from-scratch/artifacts/decision-raw.txt\n",
      "trainer_interface.cc(378) LOG(WARNING) Found too long line (4331 > 4192).\n",
      "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 796051 sentences\n",
      "trainer_interface.cc(414) LOG(INFO) Skipped 42 too long sentences.\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=45745011\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9581% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=89\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999581\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 513172 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=29440726\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 192304 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 513172\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 107525\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 107525 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=64446 obj=11.3858 num_tokens=226542 num_tokens/piece=3.51522\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=52924 obj=8.55441 num_tokens=228028 num_tokens/piece=4.30859\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=39691 obj=8.52452 num_tokens=241971 num_tokens/piece=6.09637\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=39684 obj=8.51019 num_tokens=242029 num_tokens/piece=6.09891\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=29762 obj=8.54193 num_tokens=264640 num_tokens/piece=8.89188\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=29761 obj=8.53417 num_tokens=264632 num_tokens/piece=8.89191\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=22320 obj=8.57868 num_tokens=290039 num_tokens/piece=12.9946\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=22320 obj=8.57133 num_tokens=290053 num_tokens/piece=12.9952\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=16740 obj=8.62747 num_tokens=316257 num_tokens/piece=18.8923\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=16740 obj=8.61817 num_tokens=316229 num_tokens/piece=18.8906\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=16280 obj=8.62385 num_tokens=318504 num_tokens/piece=19.5641\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=16280 obj=8.6229 num_tokens=318508 num_tokens/piece=19.5644\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: sentencepiece_tokenizer.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: sentencepiece_tokenizer.vocab\n"
     ]
    }
   ],
   "source": [
    "with open(DECISION_CORPUS_RAW, \"w\", encoding=\"utf-8\") as f:\n",
    "    for d in data:\n",
    "        f.write(d + \"\\n\")\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=DECISION_CORPUS_RAW,\n",
    "    model_prefix='sentencepiece_tokenizer',\n",
    "    vocab_size=VOCAB_SIZE - SENTINEL_TOKEN_COUNT,\n",
    "    pad_id=PAD_TOKEN_ID,\n",
    "    unk_id=UNK_TOKEN_ID,\n",
    "    eos_id=EOS_TOKEN_ID,\n",
    "    bos_id=BOS_TOKEN_ID,\n",
    "    pad_piece='<pad>',\n",
    "    unk_piece='<unk>',\n",
    "    eos_piece='</s>',\n",
    "    bos_piece='<s>',\n",
    "    model_type='unigram',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer = spm.SentencePieceProcessor(model_file=\"sentencepiece_tokenizer.model\")\n",
    "tokenizer = T5Tokenizer(\"sentencepiece_tokenizer.model\", extra_ids=0)\n",
    "\n",
    "tokenizer.add_special_tokens({\n",
    "    \"additional_special_tokens\": [\n",
    "        AddedToken(content=f\"<extra_id_{i}>\",\n",
    "                   single_word=False,\n",
    "                   normalized=False,\n",
    "                   special=True) for i in range(SENTINEL_TOKEN_COUNT)\n",
    "    ]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df = corpus.df.sample(\n",
    "        frac=1,\n",
    "        random_state=42\n",
    "    ).reset_index(\n",
    "        drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:04<00:00, 2454.03it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "172244"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = corpus.get_sentences()\n",
    "\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/172244 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 172244/172244 [00:39<00:00, 4320.88it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "148798"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_ids = [tokenizer.encode(sentence, add_special_tokens=False) for sentence in tqdm(sentences)]\n",
    "sentences_ids = [sentence_ids for sentence_ids in sentences_ids if len(sentence_ids) > MIN_SENTENCE_LEN]\n",
    "\n",
    "len(sentences_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "# first 90% will be train, rest val\n",
    "sentence_split = int(0.9*len(sentences_ids))\n",
    "\n",
    "train_data = sentences_ids[:sentence_split]\n",
    "val_data = sentences_ids[sentence_split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_batch(split, batch_size=BATCH_SIZE):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "\n",
    "    sentence_indices = torch.randint(len(data), (batch_size,))\n",
    "    \n",
    "    for sentence_ix  in sentence_indices:\n",
    "        sentence_ids = data[int(sentence_ix)]\n",
    "\n",
    "        ix = torch.randint(max(1, len(sentence_ids) - MAX_LEN), (1,))\n",
    "        sentence_ids = sentence_ids[ix:ix+MAX_LEN]\n",
    "\n",
    "        pred_count = min(MAX_LEN, max(1, round(len(sentence_ids) * 0.15)))\n",
    "        \n",
    "        masked_positions = np.random.choice(range(1, len(sentence_ids)), pred_count, replace=False)\n",
    "        masked_positions.sort()\n",
    "        \n",
    "        masked_token_ids = sentence_ids.copy()\n",
    "        target_token_ids = [PAD_TOKEN_ID]\n",
    "\n",
    "        for sentinel_id, masked_position in enumerate(masked_positions):\n",
    "            target_token_ids.append(SENTINEL_TOKEN_ID + sentinel_id)\n",
    "            target_token_ids.append(masked_token_ids[masked_position])\n",
    "\n",
    "            masked_token_ids[masked_position] = SENTINEL_TOKEN_ID + sentinel_id\n",
    "        \n",
    "        target_token_ids.append(SENTINEL_TOKEN_ID + sentinel_id + 1)\n",
    "        target_token_ids.append(EOS_TOKEN_ID)\n",
    "\n",
    "        mask_padding = MAX_LEN - len(target_token_ids)\n",
    "        target_attn_mask = np.zeros(MAX_LEN)\n",
    "        target_attn_mask[:-mask_padding] = 1\n",
    "\n",
    "        sentence_padding = MAX_LEN - len(sentence_ids)\n",
    "    \n",
    "        yield [\n",
    "            np.concatenate([sentence_ids, [PAD_TOKEN_ID] * sentence_padding]),\n",
    "            np.concatenate([masked_token_ids, [PAD_TOKEN_ID] * sentence_padding]),\n",
    "            np.concatenate([target_token_ids, [PAD_TOKEN_ID] * mask_padding]),\n",
    "            np.concatenate([target_token_ids[1:], [IGNORE_INDEX] * (mask_padding +1)]),\n",
    "            np.concatenate([(np.array(sentence_ids) != PAD_TOKEN_ID) * 1, [0] * sentence_padding]),\n",
    "            target_attn_mask,\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_ids              \n",
      " tensor([[ 383,  459, 1311,    9,    6,  509,   15,    6,   91,   78, 7006,    5,\n",
      "          447,   16,  483,    5,    9,    6, 1190,   30,  151,    5,    7, 1421,\n",
      "          489,   12, 3295,    7, 1598, 1510,    4,   13,  261,  727,   24,    9,\n",
      "            6, 3212,   22,   65,  700,   15,    6,   99, 1280,   51,   21, 1472,\n",
      "            5,   57,  116,   13,  399,  126,  846,   12,   33,    6,   91,  560,\n",
      "           15,    6, 1190,   11,  116,   22,   65, 1854,   13,  316,  126,  846,\n",
      "           19,   57, 1328,   12, 6255,   14,    7,  408,    8,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0]], device='cuda:0') \n",
      "\n",
      "masked_token_ids       \n",
      " tensor([[  383,   459,  1311,     9,     6,   509, 14800, 14801, 14802, 14803,\n",
      "          7006,     5,   447,    16,   483, 14804, 14805,     6,  1190,    30,\n",
      "         14806,     5,     7,  1421,   489,    12,  3295,     7,  1598,  1510,\n",
      "             4,    13,   261,   727,    24,     9,     6,  3212, 14807,    65,\n",
      "           700,    15,     6,    99,  1280,    51,    21,  1472,     5,    57,\n",
      "           116,    13,   399,   126,   846,    12,    33, 14808,    91,   560,\n",
      "            15,     6,  1190,    11,   116,    22,    65, 14809,    13,   316,\n",
      "           126,   846,    19, 14810,  1328, 14811,  6255,    14,     7,   408,\n",
      "             8,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]],\n",
      "       device='cuda:0') \n",
      "\n",
      "target_token_ids       \n",
      " tensor([[    0, 14800,    15, 14801,     6, 14802,    91, 14803,    78, 14804,\n",
      "             5, 14805,     9, 14806,   151, 14807,    22, 14808,     6, 14809,\n",
      "          1854, 14810,    57, 14811,    12, 14812,     1,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]],\n",
      "       device='cuda:0') \n",
      "\n",
      "target_token_loss_ids       \n",
      " tensor([[14800,    15, 14801,     6, 14802,    91, 14803,    78, 14804,     5,\n",
      "         14805,     9, 14806,   151, 14807,    22, 14808,     6, 14809,  1854,\n",
      "         14810,    57, 14811,    12, 14812,     1,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]],\n",
      "       device='cuda:0') \n",
      "\n",
      "src_attention_masks    \n",
      " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0') \n",
      "\n",
      "target_attention_masks \n",
      " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split, batch_size):\n",
    "    return map(\n",
    "         lambda x: torch.tensor(x, device=DEVICE, dtype=torch.long), \n",
    "         zip(*get_sentence_batch(split, batch_size=batch_size)))\n",
    "\n",
    "(token_ids,\n",
    " masked_token_ids,\n",
    " target_token_ids,\n",
    " target_token_loss_ids,\n",
    " src_attention_masks,\n",
    " target_attention_masks) = get_batch(\"train\", batch_size=1)\n",
    "\n",
    "print(\"token_ids              \\n\", token_ids, \"\\n\")\n",
    "print(\"masked_token_ids       \\n\", masked_token_ids, \"\\n\")\n",
    "print(\"target_token_ids       \\n\", target_token_ids, \"\\n\")\n",
    "print(\"target_token_loss_ids       \\n\", target_token_loss_ids, \"\\n\")\n",
    "print(\"src_attention_masks    \\n\", src_attention_masks, \"\\n\")\n",
    "print(\"target_attention_masks \\n\", target_attention_masks, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5(\n",
    "    dim=EMBED_DIM,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    enc_depth=N_LAYERS,\n",
    "    enc_heads=ATTN_HEADS,\n",
    "    enc_dim_head=EMBED_DIM // ATTN_HEADS,\n",
    "    enc_mlp_mult=4,\n",
    "    dec_depth=N_LAYERS,\n",
    "    dec_heads=ATTN_HEADS,\n",
    "    dec_dim_head=EMBED_DIM // ATTN_HEADS,\n",
    "    dec_mlp_mult=4,\n",
    "    dropout=DROPOUT,\n",
    ")\n",
    "m = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.4647, val loss 2.6757\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(EVAL_ITERS)\n",
    "        for k in range(EVAL_ITERS):\n",
    "            (token_ids,\n",
    "            masked_token_ids,\n",
    "            target_token_ids,\n",
    "            target_token_ids_loss,\n",
    "            src_attention_masks,\n",
    "            target_attention_masks) = get_batch(split, BATCH_SIZE)\n",
    "            logits = model(masked_token_ids, target_token_ids,\n",
    "                           src_attention_masks, target_attention_masks)\n",
    "        \n",
    "            loss = F.cross_entropy(logits.transpose(-2,-1), target_token_ids_loss)\n",
    "\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for iter in range(MAX_ITERS):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % EVAL_INTERVAL == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    (token_ids,\n",
    "    masked_token_ids,\n",
    "    target_token_ids,\n",
    "    target_token_ids_loss,\n",
    "    src_attention_masks,\n",
    "    target_attention_masks) = get_batch('train', BATCH_SIZE)\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits = model(masked_token_ids, target_token_ids,\n",
    "                   src_attention_masks, target_attention_masks)\n",
    "    \n",
    "    loss = F.cross_entropy(logits.transpose(-2,-1), target_token_ids_loss)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model visualization\n",
    "\n",
    "https://opendelta.readthedocs.io/en/latest/notes/overview.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">root</span>\n",
       "├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">encoder </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5Encoder)</span>\n",
       "│   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">token_emb </span><span style=\"color: #008000; text-decoration-color: #008000\">(Embedding) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[15000, 384]</span>\n",
       "│   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">layer </span><span style=\"color: #008000; text-decoration-color: #008000\">(ModuleList)</span>\n",
       "│   │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0 </span><span style=\"color: #008000; text-decoration-color: #008000\">(ModuleList)</span>\n",
       "│   │       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Residual)</span>\n",
       "│   │       │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fn </span><span style=\"color: #008000; text-decoration-color: #008000\">(PreNorm)</span>\n",
       "│   │       │       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">norm </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5LayerNorm) </span><span style=\"color: #008080; text-decoration-color: #008080\">gamma:[384]</span>\n",
       "│   │       │       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fn </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5SelfAttention)</span>\n",
       "│   │       │           ├── <span style=\"color: #800000; text-decoration-color: #800000\">to_q,to_k,to_v</span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[384, 384]</span>\n",
       "│   │       │           ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">to_out </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[384, 384] bias:[384]</span>\n",
       "│   │       │           └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">relative_position_bias </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5RelativePositionBias)</span>\n",
       "│   │       │               └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">relative_attention_bias </span><span style=\"color: #008000; text-decoration-color: #008000\">(Embedding) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[32, 6]</span>\n",
       "│   │       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">1 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Residual)</span>\n",
       "│   │           └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fn </span><span style=\"color: #008000; text-decoration-color: #008000\">(PreNorm)</span>\n",
       "│   │               ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">norm </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5LayerNorm) </span><span style=\"color: #008080; text-decoration-color: #008080\">gamma:[384]</span>\n",
       "│   │               └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fn </span><span style=\"color: #008000; text-decoration-color: #008000\">(FeedForward)</span>\n",
       "│   │                   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">net </span><span style=\"color: #008000; text-decoration-color: #008000\">(Sequential)</span>\n",
       "│   │                       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[1536, 384] bias:[1536]</span>\n",
       "│   │                       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">3 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[384, 1536] bias:[384]</span>\n",
       "│   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">final_norm </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5LayerNorm) </span><span style=\"color: #008080; text-decoration-color: #008080\">gamma:[384]</span>\n",
       "├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">decoder </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5Decoder)</span>\n",
       "│   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">token_emb </span><span style=\"color: #008000; text-decoration-color: #008000\">(Embedding) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[15000, 384]</span>\n",
       "│   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">layer </span><span style=\"color: #008000; text-decoration-color: #008000\">(ModuleList)</span>\n",
       "│   │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0 </span><span style=\"color: #008000; text-decoration-color: #008000\">(ModuleList)</span>\n",
       "│   │       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Residual)</span>\n",
       "│   │       │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fn </span><span style=\"color: #008000; text-decoration-color: #008000\">(PreNorm)</span>\n",
       "│   │       │       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">norm </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5LayerNorm) </span><span style=\"color: #008080; text-decoration-color: #008080\">gamma:[384]</span>\n",
       "│   │       │       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fn </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5SelfAttention)</span>\n",
       "│   │       │           ├── <span style=\"color: #800000; text-decoration-color: #800000\">to_q,to_k,to_v</span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[384, 384]</span>\n",
       "│   │       │           ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">to_out </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[384, 384] bias:[384]</span>\n",
       "│   │       │           └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">relative_position_bias </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5RelativePositionBias)</span>\n",
       "│   │       │               └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">relative_attention_bias </span><span style=\"color: #008000; text-decoration-color: #008000\">(Embedding) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[32, 6]</span>\n",
       "│   │       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">1 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Residual)</span>\n",
       "│   │       │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fn </span><span style=\"color: #008000; text-decoration-color: #008000\">(PreNorm)</span>\n",
       "│   │       │       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">norm </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5LayerNorm) </span><span style=\"color: #008080; text-decoration-color: #008080\">gamma:[384]</span>\n",
       "│   │       │       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fn </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5CrossAttention)</span>\n",
       "│   │       │           ├── <span style=\"color: #800000; text-decoration-color: #800000\">to_q,to_k,to_v</span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[384, 384]</span>\n",
       "│   │       │           └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">to_out </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[384, 384] bias:[384]</span>\n",
       "│   │       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">2 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Residual)</span>\n",
       "│   │           └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fn </span><span style=\"color: #008000; text-decoration-color: #008000\">(PreNorm)</span>\n",
       "│   │               ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">norm </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5LayerNorm) </span><span style=\"color: #008080; text-decoration-color: #008080\">gamma:[384]</span>\n",
       "│   │               └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fn </span><span style=\"color: #008000; text-decoration-color: #008000\">(FeedForward)</span>\n",
       "│   │                   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">net </span><span style=\"color: #008000; text-decoration-color: #008000\">(Sequential)</span>\n",
       "│   │                       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[1536, 384] bias:[1536]</span>\n",
       "│   │                       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">3 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[384, 1536] bias:[384]</span>\n",
       "│   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">final_norm </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5LayerNorm) </span><span style=\"color: #008080; text-decoration-color: #008080\">gamma:[384]</span>\n",
       "└── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">to_logits </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[15000, 384] bias:[15000]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37mroot\u001b[0m\n",
       "├── \u001b[37mencoder \u001b[0m\u001b[32m(T5Encoder)\u001b[0m\n",
       "│   ├── \u001b[37mtoken_emb \u001b[0m\u001b[32m(Embedding) \u001b[0m\u001b[36mweight:[15000, 384]\u001b[0m\n",
       "│   ├── \u001b[37mlayer \u001b[0m\u001b[32m(ModuleList)\u001b[0m\n",
       "│   │   └── \u001b[37m0 \u001b[0m\u001b[32m(ModuleList)\u001b[0m\n",
       "│   │       ├── \u001b[37m0 \u001b[0m\u001b[32m(Residual)\u001b[0m\n",
       "│   │       │   └── \u001b[37mfn \u001b[0m\u001b[32m(PreNorm)\u001b[0m\n",
       "│   │       │       ├── \u001b[37mnorm \u001b[0m\u001b[32m(T5LayerNorm) \u001b[0m\u001b[36mgamma:[384]\u001b[0m\n",
       "│   │       │       └── \u001b[37mfn \u001b[0m\u001b[32m(T5SelfAttention)\u001b[0m\n",
       "│   │       │           ├── \u001b[31mto_q,to_k,to_v\u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[384, 384]\u001b[0m\n",
       "│   │       │           ├── \u001b[37mto_out \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[384, 384] \u001b[0m\u001b[36mbias:[384]\u001b[0m\n",
       "│   │       │           └── \u001b[37mrelative_position_bias \u001b[0m\u001b[32m(T5RelativePositionBias)\u001b[0m\n",
       "│   │       │               └── \u001b[37mrelative_attention_bias \u001b[0m\u001b[32m(Embedding) \u001b[0m\u001b[36mweight:[32, 6]\u001b[0m\n",
       "│   │       └── \u001b[37m1 \u001b[0m\u001b[32m(Residual)\u001b[0m\n",
       "│   │           └── \u001b[37mfn \u001b[0m\u001b[32m(PreNorm)\u001b[0m\n",
       "│   │               ├── \u001b[37mnorm \u001b[0m\u001b[32m(T5LayerNorm) \u001b[0m\u001b[36mgamma:[384]\u001b[0m\n",
       "│   │               └── \u001b[37mfn \u001b[0m\u001b[32m(FeedForward)\u001b[0m\n",
       "│   │                   └── \u001b[37mnet \u001b[0m\u001b[32m(Sequential)\u001b[0m\n",
       "│   │                       ├── \u001b[37m0 \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[1536, 384] \u001b[0m\u001b[36mbias:[1536]\u001b[0m\n",
       "│   │                       └── \u001b[37m3 \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[384, 1536] \u001b[0m\u001b[36mbias:[384]\u001b[0m\n",
       "│   └── \u001b[37mfinal_norm \u001b[0m\u001b[32m(T5LayerNorm) \u001b[0m\u001b[36mgamma:[384]\u001b[0m\n",
       "├── \u001b[37mdecoder \u001b[0m\u001b[32m(T5Decoder)\u001b[0m\n",
       "│   ├── \u001b[37mtoken_emb \u001b[0m\u001b[32m(Embedding) \u001b[0m\u001b[36mweight:[15000, 384]\u001b[0m\n",
       "│   ├── \u001b[37mlayer \u001b[0m\u001b[32m(ModuleList)\u001b[0m\n",
       "│   │   └── \u001b[37m0 \u001b[0m\u001b[32m(ModuleList)\u001b[0m\n",
       "│   │       ├── \u001b[37m0 \u001b[0m\u001b[32m(Residual)\u001b[0m\n",
       "│   │       │   └── \u001b[37mfn \u001b[0m\u001b[32m(PreNorm)\u001b[0m\n",
       "│   │       │       ├── \u001b[37mnorm \u001b[0m\u001b[32m(T5LayerNorm) \u001b[0m\u001b[36mgamma:[384]\u001b[0m\n",
       "│   │       │       └── \u001b[37mfn \u001b[0m\u001b[32m(T5SelfAttention)\u001b[0m\n",
       "│   │       │           ├── \u001b[31mto_q,to_k,to_v\u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[384, 384]\u001b[0m\n",
       "│   │       │           ├── \u001b[37mto_out \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[384, 384] \u001b[0m\u001b[36mbias:[384]\u001b[0m\n",
       "│   │       │           └── \u001b[37mrelative_position_bias \u001b[0m\u001b[32m(T5RelativePositionBias)\u001b[0m\n",
       "│   │       │               └── \u001b[37mrelative_attention_bias \u001b[0m\u001b[32m(Embedding) \u001b[0m\u001b[36mweight:[32, 6]\u001b[0m\n",
       "│   │       ├── \u001b[37m1 \u001b[0m\u001b[32m(Residual)\u001b[0m\n",
       "│   │       │   └── \u001b[37mfn \u001b[0m\u001b[32m(PreNorm)\u001b[0m\n",
       "│   │       │       ├── \u001b[37mnorm \u001b[0m\u001b[32m(T5LayerNorm) \u001b[0m\u001b[36mgamma:[384]\u001b[0m\n",
       "│   │       │       └── \u001b[37mfn \u001b[0m\u001b[32m(T5CrossAttention)\u001b[0m\n",
       "│   │       │           ├── \u001b[31mto_q,to_k,to_v\u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[384, 384]\u001b[0m\n",
       "│   │       │           └── \u001b[37mto_out \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[384, 384] \u001b[0m\u001b[36mbias:[384]\u001b[0m\n",
       "│   │       └── \u001b[37m2 \u001b[0m\u001b[32m(Residual)\u001b[0m\n",
       "│   │           └── \u001b[37mfn \u001b[0m\u001b[32m(PreNorm)\u001b[0m\n",
       "│   │               ├── \u001b[37mnorm \u001b[0m\u001b[32m(T5LayerNorm) \u001b[0m\u001b[36mgamma:[384]\u001b[0m\n",
       "│   │               └── \u001b[37mfn \u001b[0m\u001b[32m(FeedForward)\u001b[0m\n",
       "│   │                   └── \u001b[37mnet \u001b[0m\u001b[32m(Sequential)\u001b[0m\n",
       "│   │                       ├── \u001b[37m0 \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[1536, 384] \u001b[0m\u001b[36mbias:[1536]\u001b[0m\n",
       "│   │                       └── \u001b[37m3 \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[384, 1536] \u001b[0m\u001b[36mbias:[384]\u001b[0m\n",
       "│   └── \u001b[37mfinal_norm \u001b[0m\u001b[32m(T5LayerNorm) \u001b[0m\u001b[36mgamma:[384]\u001b[0m\n",
       "└── \u001b[37mto_logits \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[15000, 384] \u001b[0m\u001b[36mbias:[15000]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">root</span>\n",
       "├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">encoder </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5Encoder)</span>\n",
       "│   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">token_emb </span><span style=\"color: #008000; text-decoration-color: #008000\">(Embedding) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[15000, 384]</span>\n",
       "│   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">layer </span><span style=\"color: #008000; text-decoration-color: #008000\">(ModuleList)</span>\n",
       "│   │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0 </span><span style=\"color: #008000; text-decoration-color: #008000\">(ModuleList)</span>\n",
       "│   │       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Residual)</span>\n",
       "│   │       │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fn </span><span style=\"color: #008000; text-decoration-color: #008000\">(PreNorm)</span>\n",
       "│   │       │       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">norm </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5LayerNorm) </span><span style=\"color: #008080; text-decoration-color: #008080\">gamma:[384]</span>\n",
       "│   │       │       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fn </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5SelfAttention)</span>\n",
       "│   │       │           ├── <span style=\"color: #800000; text-decoration-color: #800000\">to_q,to_k,to_v</span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[384, 384]</span>\n",
       "│   │       │           ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">to_out </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[384, 384] bias:[384]</span>\n",
       "│   │       │           └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">relative_position_bias </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5RelativePositionBias)</span>\n",
       "│   │       │               └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">relative_attention_bias </span><span style=\"color: #008000; text-decoration-color: #008000\">(Embedding) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[32, 6]</span>\n",
       "│   │       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">1 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Residual)</span>\n",
       "│   │           └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fn </span><span style=\"color: #008000; text-decoration-color: #008000\">(PreNorm)</span>\n",
       "│   │               ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">norm </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5LayerNorm) </span><span style=\"color: #008080; text-decoration-color: #008080\">gamma:[384]</span>\n",
       "│   │               └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fn </span><span style=\"color: #008000; text-decoration-color: #008000\">(FeedForward)</span>\n",
       "│   │                   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">net </span><span style=\"color: #008000; text-decoration-color: #008000\">(Sequential)</span>\n",
       "│   │                       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[1536, 384] bias:[1536]</span>\n",
       "│   │                       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">3 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[384, 1536] bias:[384]</span>\n",
       "│   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">final_norm </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5LayerNorm) </span><span style=\"color: #008080; text-decoration-color: #008080\">gamma:[384]</span>\n",
       "├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">decoder </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5Decoder)</span>\n",
       "│   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">token_emb </span><span style=\"color: #008000; text-decoration-color: #008000\">(Embedding) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[15000, 384]</span>\n",
       "│   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">layer </span><span style=\"color: #008000; text-decoration-color: #008000\">(ModuleList)</span>\n",
       "│   │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0 </span><span style=\"color: #008000; text-decoration-color: #008000\">(ModuleList)</span>\n",
       "│   │       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Residual)</span>\n",
       "│   │       │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fn </span><span style=\"color: #008000; text-decoration-color: #008000\">(PreNorm)</span>\n",
       "│   │       │       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">norm </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5LayerNorm) </span><span style=\"color: #008080; text-decoration-color: #008080\">gamma:[384]</span>\n",
       "│   │       │       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fn </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5SelfAttention)</span>\n",
       "│   │       │           ├── <span style=\"color: #800000; text-decoration-color: #800000\">to_q,to_k,to_v</span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[384, 384]</span>\n",
       "│   │       │           ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">to_out </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[384, 384] bias:[384]</span>\n",
       "│   │       │           └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">relative_position_bias </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5RelativePositionBias)</span>\n",
       "│   │       │               └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">relative_attention_bias </span><span style=\"color: #008000; text-decoration-color: #008000\">(Embedding) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[32, 6]</span>\n",
       "│   │       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">1 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Residual)</span>\n",
       "│   │       │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fn </span><span style=\"color: #008000; text-decoration-color: #008000\">(PreNorm)</span>\n",
       "│   │       │       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">norm </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5LayerNorm) </span><span style=\"color: #008080; text-decoration-color: #008080\">gamma:[384]</span>\n",
       "│   │       │       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fn </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5CrossAttention)</span>\n",
       "│   │       │           ├── <span style=\"color: #800000; text-decoration-color: #800000\">to_q,to_k,to_v</span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[384, 384]</span>\n",
       "│   │       │           └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">to_out </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[384, 384] bias:[384]</span>\n",
       "│   │       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">2 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Residual)</span>\n",
       "│   │           └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fn </span><span style=\"color: #008000; text-decoration-color: #008000\">(PreNorm)</span>\n",
       "│   │               ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">norm </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5LayerNorm) </span><span style=\"color: #008080; text-decoration-color: #008080\">gamma:[384]</span>\n",
       "│   │               └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fn </span><span style=\"color: #008000; text-decoration-color: #008000\">(FeedForward)</span>\n",
       "│   │                   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">net </span><span style=\"color: #008000; text-decoration-color: #008000\">(Sequential)</span>\n",
       "│   │                       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[1536, 384] bias:[1536]</span>\n",
       "│   │                       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">3 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[384, 1536] bias:[384]</span>\n",
       "│   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">final_norm </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5LayerNorm) </span><span style=\"color: #008080; text-decoration-color: #008080\">gamma:[384]</span>\n",
       "└── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">to_logits </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[15000, 384] bias:[15000]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37mroot\u001b[0m\n",
       "├── \u001b[37mencoder \u001b[0m\u001b[32m(T5Encoder)\u001b[0m\n",
       "│   ├── \u001b[37mtoken_emb \u001b[0m\u001b[32m(Embedding) \u001b[0m\u001b[36mweight:[15000, 384]\u001b[0m\n",
       "│   ├── \u001b[37mlayer \u001b[0m\u001b[32m(ModuleList)\u001b[0m\n",
       "│   │   └── \u001b[37m0 \u001b[0m\u001b[32m(ModuleList)\u001b[0m\n",
       "│   │       ├── \u001b[37m0 \u001b[0m\u001b[32m(Residual)\u001b[0m\n",
       "│   │       │   └── \u001b[37mfn \u001b[0m\u001b[32m(PreNorm)\u001b[0m\n",
       "│   │       │       ├── \u001b[37mnorm \u001b[0m\u001b[32m(T5LayerNorm) \u001b[0m\u001b[36mgamma:[384]\u001b[0m\n",
       "│   │       │       └── \u001b[37mfn \u001b[0m\u001b[32m(T5SelfAttention)\u001b[0m\n",
       "│   │       │           ├── \u001b[31mto_q,to_k,to_v\u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[384, 384]\u001b[0m\n",
       "│   │       │           ├── \u001b[37mto_out \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[384, 384] \u001b[0m\u001b[36mbias:[384]\u001b[0m\n",
       "│   │       │           └── \u001b[37mrelative_position_bias \u001b[0m\u001b[32m(T5RelativePositionBias)\u001b[0m\n",
       "│   │       │               └── \u001b[37mrelative_attention_bias \u001b[0m\u001b[32m(Embedding) \u001b[0m\u001b[36mweight:[32, 6]\u001b[0m\n",
       "│   │       └── \u001b[37m1 \u001b[0m\u001b[32m(Residual)\u001b[0m\n",
       "│   │           └── \u001b[37mfn \u001b[0m\u001b[32m(PreNorm)\u001b[0m\n",
       "│   │               ├── \u001b[37mnorm \u001b[0m\u001b[32m(T5LayerNorm) \u001b[0m\u001b[36mgamma:[384]\u001b[0m\n",
       "│   │               └── \u001b[37mfn \u001b[0m\u001b[32m(FeedForward)\u001b[0m\n",
       "│   │                   └── \u001b[37mnet \u001b[0m\u001b[32m(Sequential)\u001b[0m\n",
       "│   │                       ├── \u001b[37m0 \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[1536, 384] \u001b[0m\u001b[36mbias:[1536]\u001b[0m\n",
       "│   │                       └── \u001b[37m3 \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[384, 1536] \u001b[0m\u001b[36mbias:[384]\u001b[0m\n",
       "│   └── \u001b[37mfinal_norm \u001b[0m\u001b[32m(T5LayerNorm) \u001b[0m\u001b[36mgamma:[384]\u001b[0m\n",
       "├── \u001b[37mdecoder \u001b[0m\u001b[32m(T5Decoder)\u001b[0m\n",
       "│   ├── \u001b[37mtoken_emb \u001b[0m\u001b[32m(Embedding) \u001b[0m\u001b[36mweight:[15000, 384]\u001b[0m\n",
       "│   ├── \u001b[37mlayer \u001b[0m\u001b[32m(ModuleList)\u001b[0m\n",
       "│   │   └── \u001b[37m0 \u001b[0m\u001b[32m(ModuleList)\u001b[0m\n",
       "│   │       ├── \u001b[37m0 \u001b[0m\u001b[32m(Residual)\u001b[0m\n",
       "│   │       │   └── \u001b[37mfn \u001b[0m\u001b[32m(PreNorm)\u001b[0m\n",
       "│   │       │       ├── \u001b[37mnorm \u001b[0m\u001b[32m(T5LayerNorm) \u001b[0m\u001b[36mgamma:[384]\u001b[0m\n",
       "│   │       │       └── \u001b[37mfn \u001b[0m\u001b[32m(T5SelfAttention)\u001b[0m\n",
       "│   │       │           ├── \u001b[31mto_q,to_k,to_v\u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[384, 384]\u001b[0m\n",
       "│   │       │           ├── \u001b[37mto_out \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[384, 384] \u001b[0m\u001b[36mbias:[384]\u001b[0m\n",
       "│   │       │           └── \u001b[37mrelative_position_bias \u001b[0m\u001b[32m(T5RelativePositionBias)\u001b[0m\n",
       "│   │       │               └── \u001b[37mrelative_attention_bias \u001b[0m\u001b[32m(Embedding) \u001b[0m\u001b[36mweight:[32, 6]\u001b[0m\n",
       "│   │       ├── \u001b[37m1 \u001b[0m\u001b[32m(Residual)\u001b[0m\n",
       "│   │       │   └── \u001b[37mfn \u001b[0m\u001b[32m(PreNorm)\u001b[0m\n",
       "│   │       │       ├── \u001b[37mnorm \u001b[0m\u001b[32m(T5LayerNorm) \u001b[0m\u001b[36mgamma:[384]\u001b[0m\n",
       "│   │       │       └── \u001b[37mfn \u001b[0m\u001b[32m(T5CrossAttention)\u001b[0m\n",
       "│   │       │           ├── \u001b[31mto_q,to_k,to_v\u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[384, 384]\u001b[0m\n",
       "│   │       │           └── \u001b[37mto_out \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[384, 384] \u001b[0m\u001b[36mbias:[384]\u001b[0m\n",
       "│   │       └── \u001b[37m2 \u001b[0m\u001b[32m(Residual)\u001b[0m\n",
       "│   │           └── \u001b[37mfn \u001b[0m\u001b[32m(PreNorm)\u001b[0m\n",
       "│   │               ├── \u001b[37mnorm \u001b[0m\u001b[32m(T5LayerNorm) \u001b[0m\u001b[36mgamma:[384]\u001b[0m\n",
       "│   │               └── \u001b[37mfn \u001b[0m\u001b[32m(FeedForward)\u001b[0m\n",
       "│   │                   └── \u001b[37mnet \u001b[0m\u001b[32m(Sequential)\u001b[0m\n",
       "│   │                       ├── \u001b[37m0 \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[1536, 384] \u001b[0m\u001b[36mbias:[1536]\u001b[0m\n",
       "│   │                       └── \u001b[37m3 \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[384, 1536] \u001b[0m\u001b[36mbias:[384]\u001b[0m\n",
       "│   └── \u001b[37mfinal_norm \u001b[0m\u001b[32m(T5LayerNorm) \u001b[0m\u001b[36mgamma:[384]\u001b[0m\n",
       "└── \u001b[37mto_logits \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[15000, 384] \u001b[0m\u001b[36mbias:[15000]\u001b[0m\n"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from opendelta import Visualization\n",
    "model_vis = Visualization(model)\n",
    "model_vis.structure_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "France\n",
      "4. La société RT <extra_id_39> a porté plainte et s'est constituée partie civile du chef de diffamation publique envers un particulier par courrier réceptionné par le service d'accueil unique du justiciable (SAUJ) le 21 janvier 2022, transmis au secrétariat commun de l'instruction, le 25 janvier suivant.<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "sentinel_token = tokenizer.decode(SENTINEL_TOKEN_ID)\n",
    "\n",
    "(token_ids,\n",
    "masked_token_ids,\n",
    "target_token_ids,\n",
    "target_token_ids_loss,\n",
    "src_attention_masks,\n",
    "target_attention_masks) = get_batch(\"train\", batch_size=1)\n",
    "\n",
    "print(tokenizer.decode([token_ids[0, 5]], skip_special_tokens=False))\n",
    "\n",
    "token_ids[0, 5] = SENTINEL_TOKEN_ID\n",
    "\n",
    "print(tokenizer.decode(token_ids[0].cpu().numpy(), skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> <extra_id_39>, <extra_id_95>\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "# generate from the model\n",
    "def generate(model, token_ids, attn_mask, max_new_tokens, max_len=MAX_LEN):\n",
    "\n",
    "    targets = torch.zeros((1, 1), device=DEVICE).long()\n",
    "\n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    for i in range(max_new_tokens):\n",
    "\n",
    "        # get the predictions\n",
    "        model.eval()\n",
    "        logits = model(token_ids, targets[:, -max_len:], attn_mask)\n",
    "        model.train()\n",
    "\n",
    "        # focus only on the last time step\n",
    "        logits = logits[:, -1, :] # becomes (B, C)\n",
    "\n",
    "        # apply softmax to get probabilities\n",
    "        probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "\n",
    "        targets_next = torch.argmax(probs, dim=-1)[None,:]\n",
    "        # targets_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "\n",
    "        # append sampled index to the running sequence\n",
    "        targets = torch.cat((targets, targets_next), dim=1) # (B, T+1)\n",
    "    return targets\n",
    "\n",
    "\n",
    "print(tokenizer.decode(\n",
    "        generate(model,\n",
    "                 token_ids, src_attention_masks,\n",
    "                 max_new_tokens=3)[0].tolist()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
