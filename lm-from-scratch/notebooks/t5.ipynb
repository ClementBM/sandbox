{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from lm_from_scratch.models.t5_small import T5\n",
    "from lm_from_scratch.corpus.decision_corpus import DecisionCorpus\n",
    "from artifacts import DECISION_CORPUS_RAW\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "import sentencepiece as spm\n",
    "from transformers import T5Tokenizer\n",
    "from tokenizers import AddedToken\n",
    "\n",
    "# tokenizer & model parameters\n",
    "VOCAB_SIZE = 15000\n",
    "\n",
    "MAX_LEN = 128 # 512 # what is the maximum context length for predictions?\n",
    "MAX_SENTENCE_LEN = MAX_LEN // 2\n",
    "MIN_SENTENCE_LEN = 100\n",
    "\n",
    "EMBED_DIM = 384 # 768\n",
    "N_LAYERS = 1\n",
    "ATTN_HEADS = 6 # 64 * 6 = 384\n",
    "DROPOUT = 0.0\n",
    "\n",
    "\n",
    "PAD_TOKEN_ID = 0\n",
    "EOS_TOKEN_ID = 1\n",
    "UNK_TOKEN_ID = 2\n",
    "BOS_TOKEN_ID = 3\n",
    "\n",
    "DESOINING_RATE = 0.15\n",
    "SENTINEL_TOKEN_COUNT = math.ceil(MAX_LEN * DESOINING_RATE) + 1\n",
    "SENTINEL_TOKEN_ID = VOCAB_SIZE - SENTINEL_TOKEN_COUNT\n",
    "IGNORE_INDEX = -100\n",
    "\n",
    "\n",
    "# training parameters\n",
    "BATCH_SIZE = 64 # how many independent sequences will we process in parallel?\n",
    "MAX_ITERS = 1000\n",
    "EVAL_INTERVAL = 100\n",
    "LEARNING_RATE = 1e-3\n",
    "EVAL_ITERS = 100\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To specify which task the model should perform, we add a task-specific (text) prefix to the original input sequence before feeding it to the model.\n",
    "\n",
    "See Appendix D for full examples of preprocessed inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = DecisionCorpus()\n",
    "data = corpus.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "pad_token='<pad>'\n",
    "unk_token='<unk>'\n",
    "eos_token='</s>'\n",
    "bos_token='<s>'\n",
    "\n",
    "sentinel_tokens = [f\"<extra_id_{i}>\" for i in range(SENTINEL_TOKEN_COUNT)]\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "trainer = BpeTrainer(vocab_size=VOCAB_SIZE,\n",
    "                     special_tokens=[pad_token, unk_token, eos_token, bos_token] + sentinel_tokens)\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "tokenizer.train_from_iterator(data, trainer)\n",
    "\n",
    "# tokenizer.enable_padding(direction=\"right\", \n",
    "#                          pad_id=0,\n",
    "#                          pad_token=pad_token,\n",
    "#                          length=MAX_LEN)\n",
    "\n",
    "SENTINEL_TOKEN_ID = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence piece tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /home/clem/Source/sandbox/lm-from-scratch/artifacts/decision-raw.txt\n",
      "  input_format: \n",
      "  model_prefix: sentencepiece_tokenizer\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 14979\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 2\n",
      "  bos_id: 3\n",
      "  eos_id: 1\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ��� \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: /home/clem/Source/sandbox/lm-from-scratch/artifacts/decision-raw.txt\n",
      "trainer_interface.cc(378) LOG(WARNING) Found too long line (4331 > 4192).\n",
      "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 796051 sentences\n",
      "trainer_interface.cc(414) LOG(INFO) Skipped 42 too long sentences.\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=45745011\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9581% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=89\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999581\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 513172 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=29440726\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 192304 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 513172\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 107525\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 107525 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=64446 obj=11.3858 num_tokens=226542 num_tokens/piece=3.51522\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=52924 obj=8.55441 num_tokens=228028 num_tokens/piece=4.30859\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=39691 obj=8.52452 num_tokens=241971 num_tokens/piece=6.09637\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=39684 obj=8.51019 num_tokens=242029 num_tokens/piece=6.09891\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=29762 obj=8.54193 num_tokens=264640 num_tokens/piece=8.89188\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=29761 obj=8.53417 num_tokens=264632 num_tokens/piece=8.89191\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=22320 obj=8.57868 num_tokens=290039 num_tokens/piece=12.9946\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=22320 obj=8.57133 num_tokens=290053 num_tokens/piece=12.9952\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=16740 obj=8.62747 num_tokens=316257 num_tokens/piece=18.8923\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=16740 obj=8.61817 num_tokens=316229 num_tokens/piece=18.8906\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=16476 obj=8.62122 num_tokens=317458 num_tokens/piece=19.2679\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=16476 obj=8.62067 num_tokens=317460 num_tokens/piece=19.268\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: sentencepiece_tokenizer.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: sentencepiece_tokenizer.vocab\n"
     ]
    }
   ],
   "source": [
    "with open(DECISION_CORPUS_RAW, \"w\", encoding=\"utf-8\") as f:\n",
    "    for d in data:\n",
    "        f.write(d + \"\\n\")\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=DECISION_CORPUS_RAW,\n",
    "    model_prefix='sentencepiece_tokenizer',\n",
    "    vocab_size=VOCAB_SIZE - SENTINEL_TOKEN_COUNT,\n",
    "    pad_id=PAD_TOKEN_ID,\n",
    "    unk_id=UNK_TOKEN_ID,\n",
    "    eos_id=EOS_TOKEN_ID,\n",
    "    bos_id=BOS_TOKEN_ID,\n",
    "    pad_piece='<pad>',\n",
    "    unk_piece='<unk>',\n",
    "    eos_piece='</s>',\n",
    "    bos_piece='<s>',\n",
    "    model_type='unigram',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer = spm.SentencePieceProcessor(model_file=\"sentencepiece_tokenizer.model\")\n",
    "tokenizer = T5Tokenizer(\"sentencepiece_tokenizer.model\", extra_ids=0)\n",
    "\n",
    "tokenizer.add_special_tokens({\n",
    "    \"additional_special_tokens\": [\n",
    "        AddedToken(content=f\"<extra_id_{i}>\",\n",
    "                   single_word=False,\n",
    "                   normalized=False,\n",
    "                   special=True) for i in range(SENTINEL_TOKEN_COUNT)\n",
    "    ]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:04<00:00, 2458.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "172244"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = corpus.get_sentences()\n",
    "\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 172244/172244 [00:21<00:00, 7931.94it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32821"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_ids = [tokenizer.encode(sentence, add_special_tokens=False).ids for sentence in tqdm(sentences)]\n",
    "sentences_ids = [sentence_ids for sentence_ids in sentences_ids if len(sentence_ids) > MIN_SENTENCE_LEN]\n",
    "\n",
    "len(sentences_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "# first 90% will be train, rest val\n",
    "sentence_split = int(0.9*len(sentences_ids))\n",
    "\n",
    "train_data = sentences_ids[:sentence_split]\n",
    "val_data = sentences_ids[sentence_split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_batch(split, batch_size=BATCH_SIZE):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "\n",
    "    sentence_indices = torch.randint(len(data), (batch_size,))\n",
    "    \n",
    "    for sentence_ix  in sentence_indices:\n",
    "        sentence_ids = data[int(sentence_ix)]\n",
    "\n",
    "        ix = torch.randint(max(1, len(sentence_ids) - MAX_LEN), (1,))\n",
    "        sentence_ids = sentence_ids[ix:ix+MAX_LEN]\n",
    "\n",
    "        pred_count = min(SENTINEL_TOKEN_COUNT - 1, max(1, round(len(sentence_ids) * DESOINING_RATE)))\n",
    "        \n",
    "        masked_positions = np.random.choice(range(1, len(sentence_ids)), pred_count, replace=False)\n",
    "        masked_positions.sort()\n",
    "        \n",
    "        masked_token_ids = sentence_ids.copy()\n",
    "        target_token_ids = [PAD_TOKEN_ID]\n",
    "\n",
    "        for sentinel_id, masked_position in enumerate(masked_positions):\n",
    "            target_token_ids.append(SENTINEL_TOKEN_ID + sentinel_id)\n",
    "            target_token_ids.append(masked_token_ids[masked_position])\n",
    "\n",
    "            masked_token_ids[masked_position] = SENTINEL_TOKEN_ID + sentinel_id\n",
    "        \n",
    "        target_token_ids.append(SENTINEL_TOKEN_ID + sentinel_id + 1)\n",
    "        target_token_ids.append(EOS_TOKEN_ID)\n",
    "\n",
    "        mask_padding = MAX_LEN - len(target_token_ids)\n",
    "        target_attn_mask = np.zeros(MAX_LEN)\n",
    "        target_attn_mask[:-mask_padding] = 1\n",
    "\n",
    "        sentence_padding = MAX_LEN - len(sentence_ids)\n",
    "\n",
    "        target_tokenid_for_loss = np.concatenate([target_token_ids[1:], [IGNORE_INDEX] * (mask_padding +1)])\n",
    "        # target_tokenid_for_loss[target_tokenid_for_loss >= SENTINEL_TOKEN_ID] = IGNORE_INDEX\n",
    "    \n",
    "        yield [\n",
    "            np.concatenate([sentence_ids, [PAD_TOKEN_ID] * sentence_padding]),\n",
    "            np.concatenate([masked_token_ids, [PAD_TOKEN_ID] * sentence_padding]),\n",
    "            np.concatenate([target_token_ids, [PAD_TOKEN_ID] * mask_padding]),\n",
    "            target_tokenid_for_loss,\n",
    "            np.concatenate([(np.array(sentence_ids) != PAD_TOKEN_ID) * 1, [0] * sentence_padding]),\n",
    "            np.array(target_attn_mask),\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_ids              \n",
      " tensor([[   61,    82,    81,    75,   232,   643,    81,   330,    43,   232,\n",
      "            86,   360,   169,   318,    99,   123,    70,   399,    37,   676,\n",
      "            38,  6930,   371,    97,    31,   638,  2204,   169,   535,   655,\n",
      "           575,   203,   169,   482,   166,    97,    31,  4062,   181,  2268,\n",
      "           222,    81,   824,    47,    82,  9107,   195,   966,   720,   166,\n",
      "          4266,   101, 12248,    36,   319,   169,   584,    97,    31,   673,\n",
      "            50,    41,   356,   140,   173,  2542,   222,    81,   824,    44,\n",
      "           232,  2994,   203,   259,  1704,   171,  1390,    36,   643,   171,\n",
      "           553,   716,   171,    97,  3239,   330,    46,   232,    42,   356,\n",
      "           140,   246,    81,    69,    82,    81,    57,   232,   801,    81,\n",
      "           330,    41,   232,    43,   356,   195,  7043,   222,    81,   824,\n",
      "            47,   232,   643,   171,   173, 11271,    36,    81,   330,    42,\n",
      "           232,    44,   356,   195,  2853,   181,  6720,    36]],\n",
      "       device='cuda:0') \n",
      "\n",
      "masked_token_ids       \n",
      " tensor([[   61,    82,    81,     4,   232,   643,    81,   330,    43,     5,\n",
      "            86,   360,   169,   318,    99,   123,     6,   399,    37,     7,\n",
      "            38,  6930,   371,    97,    31,   638,  2204,   169,   535,     8,\n",
      "           575,   203,     9,    10,    11,    97,    31,  4062,   181,  2268,\n",
      "           222,    81,   824,    47,    82,  9107,    12,   966,   720,   166,\n",
      "          4266,   101,    13,    36,   319,   169,   584,    97,    31,   673,\n",
      "            50,    41,   356,   140,   173,  2542,   222,    14,   824,    44,\n",
      "           232,  2994,    15,   259,  1704,   171,  1390,    36,   643,   171,\n",
      "           553,   716,   171,    97,  3239,    16,    46,   232,    42,   356,\n",
      "           140,   246,    17,    69,    82,    18,    57,   232,    19,    81,\n",
      "            20,    41,   232,    21,   356,   195,  7043,   222,    81,   824,\n",
      "            47,   232,   643,   171,   173, 11271,    36,    81,   330,    42,\n",
      "           232,    44,   356,   195,    22,   181,  6720,    36]],\n",
      "       device='cuda:0') \n",
      "\n",
      "target_token_ids       \n",
      " tensor([[    0,     4,    75,     5,   232,     6,    70,     7,   676,     8,\n",
      "           655,     9,   169,    10,   482,    11,   166,    12,   195,    13,\n",
      "         12248,    14,    81,    15,   203,    16,   330,    17,    81,    18,\n",
      "            81,    19,   801,    20,   330,    21,    43,    22,  2853,    23,\n",
      "             1,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]],\n",
      "       device='cuda:0') \n",
      "\n",
      "target_token_loss_ids       \n",
      " tensor([[    4,    75,     5,   232,     6,    70,     7,   676,     8,   655,\n",
      "             9,   169,    10,   482,    11,   166,    12,   195,    13, 12248,\n",
      "            14,    81,    15,   203,    16,   330,    17,    81,    18,    81,\n",
      "            19,   801,    20,   330,    21,    43,    22,  2853,    23,     1,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]],\n",
      "       device='cuda:0') \n",
      "\n",
      "src_attention_masks    \n",
      " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0') \n",
      "\n",
      "target_attention_masks \n",
      " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0') \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16724/1648495566.py:3: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  lambda x: torch.tensor(x, device=DEVICE, dtype=torch.long),\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split, batch_size):\n",
    "    return map(\n",
    "         lambda x: torch.tensor(x, device=DEVICE, dtype=torch.long), \n",
    "         zip(*get_sentence_batch(split, batch_size=batch_size)))\n",
    "\n",
    "(token_ids,\n",
    " masked_token_ids,\n",
    " target_token_ids,\n",
    " target_token_loss_ids,\n",
    " src_attention_masks,\n",
    " target_attention_masks) = get_batch(\"train\", batch_size=1)\n",
    "\n",
    "print(\"token_ids              \\n\", token_ids, \"\\n\")\n",
    "print(\"masked_token_ids       \\n\", masked_token_ids, \"\\n\")\n",
    "print(\"target_token_ids       \\n\", target_token_ids, \"\\n\")\n",
    "print(\"target_token_loss_ids       \\n\", target_token_loss_ids, \"\\n\")\n",
    "print(\"src_attention_masks    \\n\", src_attention_masks, \"\\n\")\n",
    "print(\"target_attention_masks \\n\", target_attention_masks, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "During pre-training, we use an “inverse square root” learning rate schedule: $$1/\\sqrt{max(n, k)}$$\n",
    "\n",
    "where:\n",
    "* n is the current training iteration and\n",
    "* k is the number of warm-up steps (set to $10^4$ in all of our experiments).\n",
    "\n",
    "This sets a constant learning rate of 0.01 for the first $10^4$ steps, then exponentially decays the learning rate until pre-training is over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5(\n",
    "    dim=EMBED_DIM,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    enc_depth=N_LAYERS,\n",
    "    enc_heads=ATTN_HEADS,\n",
    "    enc_dim_head=EMBED_DIM // ATTN_HEADS,\n",
    "    enc_mlp_mult=4,\n",
    "    dec_depth=N_LAYERS,\n",
    "    dec_heads=ATTN_HEADS,\n",
    "    dec_dim_head=EMBED_DIM // ATTN_HEADS,\n",
    "    dec_mlp_mult=4,\n",
    "    dropout=DROPOUT,\n",
    ")\n",
    "m = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(EVAL_ITERS)\n",
    "        for k in range(EVAL_ITERS):\n",
    "            (_,\n",
    "            masked_token_ids,\n",
    "            target_token_ids,\n",
    "            target_token_ids_loss,\n",
    "            src_attention_masks,\n",
    "            target_attention_masks) = get_batch(split, BATCH_SIZE)\n",
    "            \n",
    "            logits = model(masked_token_ids, target_token_ids,\n",
    "                           src_attention_masks, target_attention_masks)\n",
    "            loss = F.cross_entropy(logits.transpose(-2,-1), target_token_ids_loss)\n",
    "\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "# lambda_scheduler = lambda x: 1 / math.sqrt(max(x * 100, 10000))\n",
    "# scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 9.7630, val loss 9.7669\n",
      "step 100: train loss 2.7886, val loss 2.9647\n",
      "step 200: train loss 2.6368, val loss 2.8275\n",
      "step 300: train loss 2.5558, val loss 2.7598\n",
      "step 400: train loss 2.5198, val loss 2.7206\n",
      "step 500: train loss 2.5002, val loss 2.7166\n",
      "step 600: train loss 2.4914, val loss 2.7015\n",
      "step 700: train loss 2.4516, val loss 2.6770\n",
      "step 800: train loss 2.4570, val loss 2.6797\n",
      "step 900: train loss 2.4314, val loss 2.6626\n"
     ]
    }
   ],
   "source": [
    "for iter in range(MAX_ITERS):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % EVAL_INTERVAL == 0:\n",
    "        losses = estimate_loss()\n",
    "        # current_lr = scheduler.get_last_lr()[0] \"lr {current_lr:.4f}\"\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    (_,\n",
    "    masked_token_ids,\n",
    "    target_token_ids,\n",
    "    target_token_ids_loss,\n",
    "    src_attention_masks,\n",
    "    target_attention_masks) = get_batch('train', BATCH_SIZE)\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits = model(masked_token_ids, target_token_ids,\n",
    "                   src_attention_masks, target_attention_masks)\n",
    "    \n",
    "    loss = F.cross_entropy(logits.transpose(-2,-1), target_token_ids_loss)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "# generate from the model\n",
    "def generate(model, token_ids, attn_mask, max_new_tokens, max_len=MAX_LEN):\n",
    "\n",
    "    targets = torch.zeros((1, 1), device=DEVICE).long()\n",
    "\n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    for i in range(max_new_tokens):\n",
    "\n",
    "        # get the predictions\n",
    "        model.eval()\n",
    "        logits = model(token_ids, targets[:, -max_len:], attn_mask)\n",
    "        model.train()\n",
    "\n",
    "        # focus only on the last time step\n",
    "        logits = logits[:, -1, :] # becomes (B, C)\n",
    "\n",
    "        # apply softmax to get probabilities\n",
    "        probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "\n",
    "        targets_next = torch.argmax(probs, dim=-1)[None,:]\n",
    "        # targets_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "\n",
    "        # append sampled index to the running sequence\n",
    "        targets = torch.cat((targets, targets_next), dim=1) # (B, T+1)\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "(token_ids,\n",
    "masked_token_ids,\n",
    "target_token_ids,\n",
    "target_token_ids_loss,\n",
    "src_attention_masks,\n",
    "target_attention_masks) = get_batch(\"train\", batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "les condamner in solidum <extra_id_0> payer à l ' AGS CGEA d '[ Localité 21 ] des dommages - intérêts représentant le montant des avances consenties <extra_id_1> salariés , alors « qu ' en condamnant les <extra_id_2> Bosal holding France et <extra_id_3> Nederland <extra_id_4> à réparer le préjudice subi par l ' AGS CGEA , correspondant <extra_id_5> montant des avances consenties <extra_id_6> salariés <extra_id_7> après avoir pourtant écarté la demande des <extra_id_8> tendant <extra_id_9> voir attribuer la qualité de co employeur à ces deux sociétés <extra_id_10> la cour d ' appel a ainsi fait peser sur les sociétés <extra_id_11> la réparation d ' un préjudice sans lien de <extra_id_12> direct avec les fautes <extra_id_13> <extra_id_14> étaient imputées , et <extra_id_15> violé les articles <extra_id_16> <extra_id_17> 1383 du <extra_id_18>\n",
      "\n",
      "expected\n",
      "\n",
      "<pad> <extra_id_0> à <extra_id_1> aux <extra_id_2> sociétés <extra_id_3> Bosal <extra_id_4> BV <extra_id_5> au <extra_id_6> aux <extra_id_7> , <extra_id_8> salariés <extra_id_9> à <extra_id_10> , <extra_id_11> exposantes <extra_id_12> causalité <extra_id_13> qui <extra_id_14> leur <extra_id_15> a <extra_id_16> 1382 <extra_id_17> et <extra_id_18> code <extra_id_19> <unk> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "actual\n",
      "\n",
      "<pad> <extra_id_0> ' <extra_id_1> de <extra_id_2> de\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(masked_token_ids[0].cpu().numpy(), skip_special_tokens=False))\n",
    "\n",
    "print(\"\\nexpected\\n\")\n",
    "print(tokenizer.decode(target_token_ids[0].cpu().numpy(), skip_special_tokens=False))\n",
    "print(\"\\nactual\\n\")\n",
    "print(tokenizer.decode(\n",
    "        generate(model,\n",
    "                 masked_token_ids, src_attention_masks,\n",
    "                 max_new_tokens=6)[0].tolist(),\n",
    "        skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la\n",
      "Ainsi fait et jugé par <extra_id_2> Cour de cassation, chambre sociale, et prononcé par le président en son audience publique du vingt-deux mars deux mille vingt-trois.<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "<pad> <extra_id_7>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode([token_ids[0, 5]], skip_special_tokens=False))\n",
    "\n",
    "token_ids[0, 5] = SENTINEL_TOKEN_ID\n",
    "\n",
    "print(tokenizer.decode(token_ids[0].cpu().numpy(), skip_special_tokens=False))\n",
    "\n",
    "print(tokenizer.decode(\n",
    "        generate(model,\n",
    "                 token_ids, src_attention_masks,\n",
    "                 max_new_tokens=1)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model visualization\n",
    "\n",
    "https://opendelta.readthedocs.io/en/latest/notes/overview.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">root</span>\n",
       "├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">encoder </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5Encoder)</span>\n",
       "│   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">token_emb </span><span style=\"color: #008000; text-decoration-color: #008000\">(Embedding) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[15000, 384]</span>\n",
       "│   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">layer </span><span style=\"color: #008000; text-decoration-color: #008000\">(ModuleList)</span>\n",
       "│   │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0 </span><span style=\"color: #008000; text-decoration-color: #008000\">(ModuleList)</span>\n",
       "│   │       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Residual)</span>\n",
       "│   │       │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fn </span><span style=\"color: #008000; text-decoration-color: #008000\">(PreNorm)</span>\n",
       "│   │       │       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">norm </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5LayerNorm) </span><span style=\"color: #008080; text-decoration-color: #008080\">gamma:[384]</span>\n",
       "│   │       │       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fn </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5SelfAttention)</span>\n",
       "│   │       │           ├── <span style=\"color: #800000; text-decoration-color: #800000\">to_q,to_k,to_v</span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[384, 384]</span>\n",
       "│   │       │           ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">to_out </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[384, 384] bias:[384]</span>\n",
       "│   │       │           └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">relative_position_bias </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5RelativePositionBias)</span>\n",
       "│   │       │               └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">relative_attention_bias </span><span style=\"color: #008000; text-decoration-color: #008000\">(Embedding) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[32, 6]</span>\n",
       "│   │       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">1 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Residual)</span>\n",
       "│   │           └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fn </span><span style=\"color: #008000; text-decoration-color: #008000\">(PreNorm)</span>\n",
       "│   │               ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">norm </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5LayerNorm) </span><span style=\"color: #008080; text-decoration-color: #008080\">gamma:[384]</span>\n",
       "│   │               └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fn </span><span style=\"color: #008000; text-decoration-color: #008000\">(FeedForward)</span>\n",
       "│   │                   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">net </span><span style=\"color: #008000; text-decoration-color: #008000\">(Sequential)</span>\n",
       "│   │                       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[1536, 384] bias:[1536]</span>\n",
       "│   │                       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">3 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[384, 1536] bias:[384]</span>\n",
       "│   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">final_norm </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5LayerNorm) </span><span style=\"color: #008080; text-decoration-color: #008080\">gamma:[384]</span>\n",
       "├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">decoder </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5Decoder)</span>\n",
       "│   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">token_emb </span><span style=\"color: #008000; text-decoration-color: #008000\">(Embedding) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[15000, 384]</span>\n",
       "│   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">layer </span><span style=\"color: #008000; text-decoration-color: #008000\">(ModuleList)</span>\n",
       "│   │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0 </span><span style=\"color: #008000; text-decoration-color: #008000\">(ModuleList)</span>\n",
       "│   │       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Residual)</span>\n",
       "│   │       │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fn </span><span style=\"color: #008000; text-decoration-color: #008000\">(PreNorm)</span>\n",
       "│   │       │       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">norm </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5LayerNorm) </span><span style=\"color: #008080; text-decoration-color: #008080\">gamma:[384]</span>\n",
       "│   │       │       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fn </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5SelfAttention)</span>\n",
       "│   │       │           ├── <span style=\"color: #800000; text-decoration-color: #800000\">to_q,to_k,to_v</span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[384, 384]</span>\n",
       "│   │       │           ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">to_out </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[384, 384] bias:[384]</span>\n",
       "│   │       │           └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">relative_position_bias </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5RelativePositionBias)</span>\n",
       "│   │       │               └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">relative_attention_bias </span><span style=\"color: #008000; text-decoration-color: #008000\">(Embedding) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[32, 6]</span>\n",
       "│   │       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">1 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Residual)</span>\n",
       "│   │       │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fn </span><span style=\"color: #008000; text-decoration-color: #008000\">(PreNorm)</span>\n",
       "│   │       │       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">norm </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5LayerNorm) </span><span style=\"color: #008080; text-decoration-color: #008080\">gamma:[384]</span>\n",
       "│   │       │       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fn </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5CrossAttention)</span>\n",
       "│   │       │           ├── <span style=\"color: #800000; text-decoration-color: #800000\">to_q,to_k,to_v</span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[384, 384]</span>\n",
       "│   │       │           └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">to_out </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[384, 384] bias:[384]</span>\n",
       "│   │       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">2 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Residual)</span>\n",
       "│   │           └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fn </span><span style=\"color: #008000; text-decoration-color: #008000\">(PreNorm)</span>\n",
       "│   │               ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">norm </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5LayerNorm) </span><span style=\"color: #008080; text-decoration-color: #008080\">gamma:[384]</span>\n",
       "│   │               └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fn </span><span style=\"color: #008000; text-decoration-color: #008000\">(FeedForward)</span>\n",
       "│   │                   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">net </span><span style=\"color: #008000; text-decoration-color: #008000\">(Sequential)</span>\n",
       "│   │                       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[1536, 384] bias:[1536]</span>\n",
       "│   │                       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">3 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[384, 1536] bias:[384]</span>\n",
       "│   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">final_norm </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5LayerNorm) </span><span style=\"color: #008080; text-decoration-color: #008080\">gamma:[384]</span>\n",
       "└── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">to_logits </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[15000, 384] bias:[15000]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37mroot\u001b[0m\n",
       "├── \u001b[37mencoder \u001b[0m\u001b[32m(T5Encoder)\u001b[0m\n",
       "│   ├── \u001b[37mtoken_emb \u001b[0m\u001b[32m(Embedding) \u001b[0m\u001b[36mweight:[15000, 384]\u001b[0m\n",
       "│   ├── \u001b[37mlayer \u001b[0m\u001b[32m(ModuleList)\u001b[0m\n",
       "│   │   └── \u001b[37m0 \u001b[0m\u001b[32m(ModuleList)\u001b[0m\n",
       "│   │       ├── \u001b[37m0 \u001b[0m\u001b[32m(Residual)\u001b[0m\n",
       "│   │       │   └── \u001b[37mfn \u001b[0m\u001b[32m(PreNorm)\u001b[0m\n",
       "│   │       │       ├── \u001b[37mnorm \u001b[0m\u001b[32m(T5LayerNorm) \u001b[0m\u001b[36mgamma:[384]\u001b[0m\n",
       "│   │       │       └── \u001b[37mfn \u001b[0m\u001b[32m(T5SelfAttention)\u001b[0m\n",
       "│   │       │           ├── \u001b[31mto_q,to_k,to_v\u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[384, 384]\u001b[0m\n",
       "│   │       │           ├── \u001b[37mto_out \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[384, 384] \u001b[0m\u001b[36mbias:[384]\u001b[0m\n",
       "│   │       │           └── \u001b[37mrelative_position_bias \u001b[0m\u001b[32m(T5RelativePositionBias)\u001b[0m\n",
       "│   │       │               └── \u001b[37mrelative_attention_bias \u001b[0m\u001b[32m(Embedding) \u001b[0m\u001b[36mweight:[32, 6]\u001b[0m\n",
       "│   │       └── \u001b[37m1 \u001b[0m\u001b[32m(Residual)\u001b[0m\n",
       "│   │           └── \u001b[37mfn \u001b[0m\u001b[32m(PreNorm)\u001b[0m\n",
       "│   │               ├── \u001b[37mnorm \u001b[0m\u001b[32m(T5LayerNorm) \u001b[0m\u001b[36mgamma:[384]\u001b[0m\n",
       "│   │               └── \u001b[37mfn \u001b[0m\u001b[32m(FeedForward)\u001b[0m\n",
       "│   │                   └── \u001b[37mnet \u001b[0m\u001b[32m(Sequential)\u001b[0m\n",
       "│   │                       ├── \u001b[37m0 \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[1536, 384] \u001b[0m\u001b[36mbias:[1536]\u001b[0m\n",
       "│   │                       └── \u001b[37m3 \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[384, 1536] \u001b[0m\u001b[36mbias:[384]\u001b[0m\n",
       "│   └── \u001b[37mfinal_norm \u001b[0m\u001b[32m(T5LayerNorm) \u001b[0m\u001b[36mgamma:[384]\u001b[0m\n",
       "├── \u001b[37mdecoder \u001b[0m\u001b[32m(T5Decoder)\u001b[0m\n",
       "│   ├── \u001b[37mtoken_emb \u001b[0m\u001b[32m(Embedding) \u001b[0m\u001b[36mweight:[15000, 384]\u001b[0m\n",
       "│   ├── \u001b[37mlayer \u001b[0m\u001b[32m(ModuleList)\u001b[0m\n",
       "│   │   └── \u001b[37m0 \u001b[0m\u001b[32m(ModuleList)\u001b[0m\n",
       "│   │       ├── \u001b[37m0 \u001b[0m\u001b[32m(Residual)\u001b[0m\n",
       "│   │       │   └── \u001b[37mfn \u001b[0m\u001b[32m(PreNorm)\u001b[0m\n",
       "│   │       │       ├── \u001b[37mnorm \u001b[0m\u001b[32m(T5LayerNorm) \u001b[0m\u001b[36mgamma:[384]\u001b[0m\n",
       "│   │       │       └── \u001b[37mfn \u001b[0m\u001b[32m(T5SelfAttention)\u001b[0m\n",
       "│   │       │           ├── \u001b[31mto_q,to_k,to_v\u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[384, 384]\u001b[0m\n",
       "│   │       │           ├── \u001b[37mto_out \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[384, 384] \u001b[0m\u001b[36mbias:[384]\u001b[0m\n",
       "│   │       │           └── \u001b[37mrelative_position_bias \u001b[0m\u001b[32m(T5RelativePositionBias)\u001b[0m\n",
       "│   │       │               └── \u001b[37mrelative_attention_bias \u001b[0m\u001b[32m(Embedding) \u001b[0m\u001b[36mweight:[32, 6]\u001b[0m\n",
       "│   │       ├── \u001b[37m1 \u001b[0m\u001b[32m(Residual)\u001b[0m\n",
       "│   │       │   └── \u001b[37mfn \u001b[0m\u001b[32m(PreNorm)\u001b[0m\n",
       "│   │       │       ├── \u001b[37mnorm \u001b[0m\u001b[32m(T5LayerNorm) \u001b[0m\u001b[36mgamma:[384]\u001b[0m\n",
       "│   │       │       └── \u001b[37mfn \u001b[0m\u001b[32m(T5CrossAttention)\u001b[0m\n",
       "│   │       │           ├── \u001b[31mto_q,to_k,to_v\u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[384, 384]\u001b[0m\n",
       "│   │       │           └── \u001b[37mto_out \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[384, 384] \u001b[0m\u001b[36mbias:[384]\u001b[0m\n",
       "│   │       └── \u001b[37m2 \u001b[0m\u001b[32m(Residual)\u001b[0m\n",
       "│   │           └── \u001b[37mfn \u001b[0m\u001b[32m(PreNorm)\u001b[0m\n",
       "│   │               ├── \u001b[37mnorm \u001b[0m\u001b[32m(T5LayerNorm) \u001b[0m\u001b[36mgamma:[384]\u001b[0m\n",
       "│   │               └── \u001b[37mfn \u001b[0m\u001b[32m(FeedForward)\u001b[0m\n",
       "│   │                   └── \u001b[37mnet \u001b[0m\u001b[32m(Sequential)\u001b[0m\n",
       "│   │                       ├── \u001b[37m0 \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[1536, 384] \u001b[0m\u001b[36mbias:[1536]\u001b[0m\n",
       "│   │                       └── \u001b[37m3 \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[384, 1536] \u001b[0m\u001b[36mbias:[384]\u001b[0m\n",
       "│   └── \u001b[37mfinal_norm \u001b[0m\u001b[32m(T5LayerNorm) \u001b[0m\u001b[36mgamma:[384]\u001b[0m\n",
       "└── \u001b[37mto_logits \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[15000, 384] \u001b[0m\u001b[36mbias:[15000]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">root</span>\n",
       "├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">encoder </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5Encoder)</span>\n",
       "│   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">token_emb </span><span style=\"color: #008000; text-decoration-color: #008000\">(Embedding) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[15000, 384]</span>\n",
       "│   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">layer </span><span style=\"color: #008000; text-decoration-color: #008000\">(ModuleList)</span>\n",
       "│   │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0 </span><span style=\"color: #008000; text-decoration-color: #008000\">(ModuleList)</span>\n",
       "│   │       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Residual)</span>\n",
       "│   │       │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fn </span><span style=\"color: #008000; text-decoration-color: #008000\">(PreNorm)</span>\n",
       "│   │       │       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">norm </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5LayerNorm) </span><span style=\"color: #008080; text-decoration-color: #008080\">gamma:[384]</span>\n",
       "│   │       │       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fn </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5SelfAttention)</span>\n",
       "│   │       │           ├── <span style=\"color: #800000; text-decoration-color: #800000\">to_q,to_k,to_v</span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[384, 384]</span>\n",
       "│   │       │           ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">to_out </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[384, 384] bias:[384]</span>\n",
       "│   │       │           └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">relative_position_bias </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5RelativePositionBias)</span>\n",
       "│   │       │               └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">relative_attention_bias </span><span style=\"color: #008000; text-decoration-color: #008000\">(Embedding) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[32, 6]</span>\n",
       "│   │       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">1 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Residual)</span>\n",
       "│   │           └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fn </span><span style=\"color: #008000; text-decoration-color: #008000\">(PreNorm)</span>\n",
       "│   │               ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">norm </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5LayerNorm) </span><span style=\"color: #008080; text-decoration-color: #008080\">gamma:[384]</span>\n",
       "│   │               └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fn </span><span style=\"color: #008000; text-decoration-color: #008000\">(FeedForward)</span>\n",
       "│   │                   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">net </span><span style=\"color: #008000; text-decoration-color: #008000\">(Sequential)</span>\n",
       "│   │                       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[1536, 384] bias:[1536]</span>\n",
       "│   │                       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">3 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[384, 1536] bias:[384]</span>\n",
       "│   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">final_norm </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5LayerNorm) </span><span style=\"color: #008080; text-decoration-color: #008080\">gamma:[384]</span>\n",
       "├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">decoder </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5Decoder)</span>\n",
       "│   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">token_emb </span><span style=\"color: #008000; text-decoration-color: #008000\">(Embedding) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[15000, 384]</span>\n",
       "│   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">layer </span><span style=\"color: #008000; text-decoration-color: #008000\">(ModuleList)</span>\n",
       "│   │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0 </span><span style=\"color: #008000; text-decoration-color: #008000\">(ModuleList)</span>\n",
       "│   │       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Residual)</span>\n",
       "│   │       │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fn </span><span style=\"color: #008000; text-decoration-color: #008000\">(PreNorm)</span>\n",
       "│   │       │       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">norm </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5LayerNorm) </span><span style=\"color: #008080; text-decoration-color: #008080\">gamma:[384]</span>\n",
       "│   │       │       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fn </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5SelfAttention)</span>\n",
       "│   │       │           ├── <span style=\"color: #800000; text-decoration-color: #800000\">to_q,to_k,to_v</span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[384, 384]</span>\n",
       "│   │       │           ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">to_out </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[384, 384] bias:[384]</span>\n",
       "│   │       │           └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">relative_position_bias </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5RelativePositionBias)</span>\n",
       "│   │       │               └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">relative_attention_bias </span><span style=\"color: #008000; text-decoration-color: #008000\">(Embedding) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[32, 6]</span>\n",
       "│   │       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">1 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Residual)</span>\n",
       "│   │       │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fn </span><span style=\"color: #008000; text-decoration-color: #008000\">(PreNorm)</span>\n",
       "│   │       │       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">norm </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5LayerNorm) </span><span style=\"color: #008080; text-decoration-color: #008080\">gamma:[384]</span>\n",
       "│   │       │       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fn </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5CrossAttention)</span>\n",
       "│   │       │           ├── <span style=\"color: #800000; text-decoration-color: #800000\">to_q,to_k,to_v</span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[384, 384]</span>\n",
       "│   │       │           └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">to_out </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[384, 384] bias:[384]</span>\n",
       "│   │       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">2 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Residual)</span>\n",
       "│   │           └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fn </span><span style=\"color: #008000; text-decoration-color: #008000\">(PreNorm)</span>\n",
       "│   │               ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">norm </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5LayerNorm) </span><span style=\"color: #008080; text-decoration-color: #008080\">gamma:[384]</span>\n",
       "│   │               └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fn </span><span style=\"color: #008000; text-decoration-color: #008000\">(FeedForward)</span>\n",
       "│   │                   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">net </span><span style=\"color: #008000; text-decoration-color: #008000\">(Sequential)</span>\n",
       "│   │                       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[1536, 384] bias:[1536]</span>\n",
       "│   │                       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">3 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[384, 1536] bias:[384]</span>\n",
       "│   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">final_norm </span><span style=\"color: #008000; text-decoration-color: #008000\">(T5LayerNorm) </span><span style=\"color: #008080; text-decoration-color: #008080\">gamma:[384]</span>\n",
       "└── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">to_logits </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[15000, 384] bias:[15000]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37mroot\u001b[0m\n",
       "├── \u001b[37mencoder \u001b[0m\u001b[32m(T5Encoder)\u001b[0m\n",
       "│   ├── \u001b[37mtoken_emb \u001b[0m\u001b[32m(Embedding) \u001b[0m\u001b[36mweight:[15000, 384]\u001b[0m\n",
       "│   ├── \u001b[37mlayer \u001b[0m\u001b[32m(ModuleList)\u001b[0m\n",
       "│   │   └── \u001b[37m0 \u001b[0m\u001b[32m(ModuleList)\u001b[0m\n",
       "│   │       ├── \u001b[37m0 \u001b[0m\u001b[32m(Residual)\u001b[0m\n",
       "│   │       │   └── \u001b[37mfn \u001b[0m\u001b[32m(PreNorm)\u001b[0m\n",
       "│   │       │       ├── \u001b[37mnorm \u001b[0m\u001b[32m(T5LayerNorm) \u001b[0m\u001b[36mgamma:[384]\u001b[0m\n",
       "│   │       │       └── \u001b[37mfn \u001b[0m\u001b[32m(T5SelfAttention)\u001b[0m\n",
       "│   │       │           ├── \u001b[31mto_q,to_k,to_v\u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[384, 384]\u001b[0m\n",
       "│   │       │           ├── \u001b[37mto_out \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[384, 384] \u001b[0m\u001b[36mbias:[384]\u001b[0m\n",
       "│   │       │           └── \u001b[37mrelative_position_bias \u001b[0m\u001b[32m(T5RelativePositionBias)\u001b[0m\n",
       "│   │       │               └── \u001b[37mrelative_attention_bias \u001b[0m\u001b[32m(Embedding) \u001b[0m\u001b[36mweight:[32, 6]\u001b[0m\n",
       "│   │       └── \u001b[37m1 \u001b[0m\u001b[32m(Residual)\u001b[0m\n",
       "│   │           └── \u001b[37mfn \u001b[0m\u001b[32m(PreNorm)\u001b[0m\n",
       "│   │               ├── \u001b[37mnorm \u001b[0m\u001b[32m(T5LayerNorm) \u001b[0m\u001b[36mgamma:[384]\u001b[0m\n",
       "│   │               └── \u001b[37mfn \u001b[0m\u001b[32m(FeedForward)\u001b[0m\n",
       "│   │                   └── \u001b[37mnet \u001b[0m\u001b[32m(Sequential)\u001b[0m\n",
       "│   │                       ├── \u001b[37m0 \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[1536, 384] \u001b[0m\u001b[36mbias:[1536]\u001b[0m\n",
       "│   │                       └── \u001b[37m3 \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[384, 1536] \u001b[0m\u001b[36mbias:[384]\u001b[0m\n",
       "│   └── \u001b[37mfinal_norm \u001b[0m\u001b[32m(T5LayerNorm) \u001b[0m\u001b[36mgamma:[384]\u001b[0m\n",
       "├── \u001b[37mdecoder \u001b[0m\u001b[32m(T5Decoder)\u001b[0m\n",
       "│   ├── \u001b[37mtoken_emb \u001b[0m\u001b[32m(Embedding) \u001b[0m\u001b[36mweight:[15000, 384]\u001b[0m\n",
       "│   ├── \u001b[37mlayer \u001b[0m\u001b[32m(ModuleList)\u001b[0m\n",
       "│   │   └── \u001b[37m0 \u001b[0m\u001b[32m(ModuleList)\u001b[0m\n",
       "│   │       ├── \u001b[37m0 \u001b[0m\u001b[32m(Residual)\u001b[0m\n",
       "│   │       │   └── \u001b[37mfn \u001b[0m\u001b[32m(PreNorm)\u001b[0m\n",
       "│   │       │       ├── \u001b[37mnorm \u001b[0m\u001b[32m(T5LayerNorm) \u001b[0m\u001b[36mgamma:[384]\u001b[0m\n",
       "│   │       │       └── \u001b[37mfn \u001b[0m\u001b[32m(T5SelfAttention)\u001b[0m\n",
       "│   │       │           ├── \u001b[31mto_q,to_k,to_v\u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[384, 384]\u001b[0m\n",
       "│   │       │           ├── \u001b[37mto_out \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[384, 384] \u001b[0m\u001b[36mbias:[384]\u001b[0m\n",
       "│   │       │           └── \u001b[37mrelative_position_bias \u001b[0m\u001b[32m(T5RelativePositionBias)\u001b[0m\n",
       "│   │       │               └── \u001b[37mrelative_attention_bias \u001b[0m\u001b[32m(Embedding) \u001b[0m\u001b[36mweight:[32, 6]\u001b[0m\n",
       "│   │       ├── \u001b[37m1 \u001b[0m\u001b[32m(Residual)\u001b[0m\n",
       "│   │       │   └── \u001b[37mfn \u001b[0m\u001b[32m(PreNorm)\u001b[0m\n",
       "│   │       │       ├── \u001b[37mnorm \u001b[0m\u001b[32m(T5LayerNorm) \u001b[0m\u001b[36mgamma:[384]\u001b[0m\n",
       "│   │       │       └── \u001b[37mfn \u001b[0m\u001b[32m(T5CrossAttention)\u001b[0m\n",
       "│   │       │           ├── \u001b[31mto_q,to_k,to_v\u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[384, 384]\u001b[0m\n",
       "│   │       │           └── \u001b[37mto_out \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[384, 384] \u001b[0m\u001b[36mbias:[384]\u001b[0m\n",
       "│   │       └── \u001b[37m2 \u001b[0m\u001b[32m(Residual)\u001b[0m\n",
       "│   │           └── \u001b[37mfn \u001b[0m\u001b[32m(PreNorm)\u001b[0m\n",
       "│   │               ├── \u001b[37mnorm \u001b[0m\u001b[32m(T5LayerNorm) \u001b[0m\u001b[36mgamma:[384]\u001b[0m\n",
       "│   │               └── \u001b[37mfn \u001b[0m\u001b[32m(FeedForward)\u001b[0m\n",
       "│   │                   └── \u001b[37mnet \u001b[0m\u001b[32m(Sequential)\u001b[0m\n",
       "│   │                       ├── \u001b[37m0 \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[1536, 384] \u001b[0m\u001b[36mbias:[1536]\u001b[0m\n",
       "│   │                       └── \u001b[37m3 \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[384, 1536] \u001b[0m\u001b[36mbias:[384]\u001b[0m\n",
       "│   └── \u001b[37mfinal_norm \u001b[0m\u001b[32m(T5LayerNorm) \u001b[0m\u001b[36mgamma:[384]\u001b[0m\n",
       "└── \u001b[37mto_logits \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[15000, 384] \u001b[0m\u001b[36mbias:[15000]\u001b[0m\n"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from opendelta import Visualization\n",
    "model_vis = Visualization(model)\n",
    "model_vis.structure_graph()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
