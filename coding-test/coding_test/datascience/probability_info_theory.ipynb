{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability and Information Theory\n",
    "\n",
    "While probability theory allows us to make uncertain statements and to reason in the presence of uncertainty, information theory enables us to quantify the amount of uncertainty in a probability distribution.\n",
    "\n",
    "**Frequentist probability**: Drawing a certain hand of cards in a poker game, it is repeatable. When we say that an outcome has a probability $p$ of occuring, it means that if we repeated the experiment infinitely many times, then a proportion $p$ of the repetitions would result in that outcome.\n",
    "\n",
    "**Bayesian probability**: In the case of the doctor diagnosing the patient, we use probability to represent a degree of belief, with 1 indicating absolute certainty that the patient has the flu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete Variables and Probability Mass Functions\n",
    "\n",
    "A probability distribution over discrete variables may be described using a PMF, typically denoted as capital P. Often we associate each random variable with a different probability mass function and the reader must infer which PMF to use based on the identity of the random variable, rather than on the name of the function. $P(x)$ is usually not the same as $P(y)$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
