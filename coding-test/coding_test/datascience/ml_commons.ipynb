{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import poly1d\n",
    "import numpy as np\n",
    "from scipy import linalg, sparse, misc\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import neighbors, datasets, preprocessing\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "# Model\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# With naive bayes\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MultinomialNB())])\n",
    "\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "               'tfidf__use_idf': (True, False),\n",
    "               'clf__alpha': (1e-2, 1e-3)}\n",
    "\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "\n",
    "gs_clf = gs_clf.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "gs_clf.best_score_\n",
    "gs_clf.best_params_\n",
    "\n",
    "# With SVM\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-svm', SGDClassifier(loss='hinge', \n",
    "                                                   penalty='l2',\n",
    "                                                   alpha=1e-3, \n",
    "                                                   max_iter=10, \n",
    "                                                   random_state=42))])\n",
    "\n",
    "parameters_svm = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "                  'tfidf__use_idf': (True, False),\n",
    "                  'clf-svm__alpha': (1e-2, 1e-3)}\n",
    "\n",
    "gs_clf_svm = GridSearchCV(text_clf_svm, parameters_svm, n_jobs=-1)\n",
    "gs_clf_svm = gs_clf_svm.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "gs_clf_svm.best_score_\n",
    "gs_clf_svm.best_params_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA: Latent Dirichlet Allocations\n",
    "\n",
    "https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import poly1d\n",
    "import numpy as np\n",
    "from scipy import linalg, sparse, misc\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import neighbors, datasets, preprocessing\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "# Model\n",
    "from sklearn import linear_model\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)\n",
    "\n",
    "# Use only one feature, the third one\n",
    "diabetes_X = diabetes_X[:, np.newaxis, 2]\n",
    "\n",
    "# Split the data into training/testing sets\n",
    "diabetes_X_train = diabetes_X[:-20]\n",
    "diabetes_X_test = diabetes_X[-20:]\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "diabetes_y_train = diabetes_y[:-20]\n",
    "diabetes_y_test = diabetes_y[-20:]\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(diabetes_X_train, diabetes_y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "diabetes_y_pred = regr.predict(diabetes_X_test)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "# The mean squared error\n",
    "print('Mean squared error: %.2f'\n",
    "      % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f'\n",
    "      % r2_score(diabetes_y_test, diabetes_y_pred))\n",
    "\n",
    "# Plot outputs\n",
    "plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\n",
    "plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import poly1d\n",
    "import numpy as np\n",
    "from scipy import linalg, sparse, misc\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import neighbors, datasets, preprocessing\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "# Model\n",
    "from sklearn import linear_model\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data[:, :2]  # we only take the first two features.\n",
    "\n",
    "Y = iris.target\n",
    "\n",
    "# C : float, default=1.0 Inverse of regularization strength; \n",
    "# must be a positive float. Like in support vector machines, \n",
    "# smaller values specify stronger regularization.\n",
    "logreg = linear_model.LogisticRegression(C=1e5)\n",
    "\n",
    "# Create an instance of Logistic Regression Classifier and fit the data.\n",
    "logreg.fit(X, Y)\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "h = .02  # step size in the mesh\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1, figsize=(4, 3))\n",
    "plt.pcolormesh(xx, yy, Z, cmap = plt.cm.Paired)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap = plt.cm.Paired)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import poly1d\n",
    "import numpy as np\n",
    "from scipy import linalg, sparse, misc\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import neighbors, datasets, preprocessing\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "# Model\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# loading the training data\n",
    "twenty_train = datasets.fetch_20newsgroups(subset='train', shuffle=True)\n",
    "\n",
    "# prints all the categories\n",
    "twenty_train.target_names\n",
    "\n",
    "# prints first\n",
    "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3]))\n",
    "\n",
    "# using bag of words model\n",
    "# segment each text file into words \n",
    "# (for English splitting by space), \n",
    "# and count # of times each word occurs in each document \n",
    "# and finally assign each word an integer id.\n",
    "# Each unique word in our dictionary will correspond to a feature (descriptive feature).\n",
    "\n",
    "# create feature vectors\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape\n",
    "\n",
    "# \n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape\n",
    "\n",
    "# Model\n",
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)\n",
    "\n",
    "# Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MultinomialNB())])\n",
    "\n",
    "# Stop words\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english')),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MultinomialNB())])\n",
    "\n",
    "# MultinomialNB(FitPrior=False), a uniform prior will be used. \n",
    "\n",
    "\n",
    "# Train\n",
    "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "# Test\n",
    "twenty_test = datasets.fetch_20newsgroups(subset='test', shuffle=True)\n",
    "predicted = text_clf.predict(twenty_test.data)\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def train_and_predict(train_input_features, train_outputs, prediction_features):\n",
    "    \"\"\"\n",
    "    :param train_input_features: (numpy.array) A two-dimensional NumPy array where each element\n",
    "                        is an array that contains two numerical features\n",
    "    :param train_outputs: (numpy.array) A one-dimensional NumPy array where each element\n",
    "                        is the value associated with the same row of train_input_features\n",
    "    :param prediction_features: (numpy.array) A two-dimensional NumPy array where each element\n",
    "                        is an array that contains two numerical features\n",
    "    :returns: (list) The function should return an iterable (like list or numpy.ndarray) of predictions,\n",
    "                        one for each item in prediction_features\n",
    "    \"\"\"   \n",
    "    \n",
    "    parameters = {'poly__degree': (np.arange(4))}\n",
    "\n",
    "    pipeline = Pipeline([('scale', StandardScaler()),\n",
    "                        ('poly', PolynomialFeatures(degree=3)),\n",
    "                        ('reg', LinearRegression())])\n",
    "    \n",
    "    \n",
    "    # Grid Search to find the best polynomial degree although it was already specified in the question\n",
    "    # gs_clf = GridSearchCV(pipeline, parameters, n_jobs=-1)\n",
    "    # gs_clf = gs_clf.fit(train_input_features, train_outputs)    \n",
    "    # return gs_clf.predict(prediction_features)\n",
    "    \n",
    "    model = pipeline.fit(train_input_features, train_outputs)\n",
    "    return model.predict(prediction_features)\n",
    "\n",
    "#Example case\n",
    "np.random.seed(1)\n",
    "data = np.random.normal(size=(200, 2))\n",
    "result = 2 * data[:, 0] ** 3 + 4 * data[:, 1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, result,\n",
    "                                                    test_size=0.3, random_state=0)\n",
    "\n",
    "y_pred = train_and_predict(X_train, y_train, X_test)\n",
    "if y_pred is not None:\n",
    "    print(metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM: Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import poly1d\n",
    "import numpy as np\n",
    "from scipy import linalg, sparse, misc\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import neighbors, datasets, preprocessing\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "# Model\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "twenty_train = datasets.fetch_20newsgroups(subset='train', shuffle=True)\n",
    "\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-svm', SGDClassifier(loss='hinge', \n",
    "                                                   penalty='l2',\n",
    "                                                   alpha=1e-3, \n",
    "                                                   max_iter=10, \n",
    "                                                   random_state=42))])\n",
    "\n",
    "_ = text_clf_svm.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "predicted_svm = text_clf_svm.predict(twenty_test.data)\n",
    "\n",
    "np.mean(predicted_svm == twenty_test.target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "You want to create a machine learning algorithm that finds the top 100 people who have shared photographs of themselves on social media. What is the best machine learning method to use ?\n",
    "* [ ] K-nearest neighbor\n",
    "* [ ] binary classification\n",
    "* [ ] unsupervised learning\n",
    "* [ ] reinforcement learning\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "\n",
    "Which pattern exhibits the following figure of the fit line and data?\n",
    "![alt](figure-1.png)\n",
    "\n",
    "* [ ] high bias, low variance\n",
    "* [ ] high bias, high variance\n",
    "* [ ] low bias, low variance\n",
    "* [ ] low bias, high variance\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "\n",
    "You want to create a machine learning algorithm to identify food recipes on the web. To do this, you create an algorithm that looks at different conditional probabilities. So if the post includes the word *flour*, it has a slightly stronger probability of being a recipe. If it contains both *flour* and *sugar*, is is even more likely a recipe. What type of algorithm are you using?\n",
    "* [ ] K-nearest neighbor\n",
    "* [ ] naive Bayes classifier\n",
    "* [ ] multiclass classification\n",
    "* [ ] decision tree\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "In statistics, what is defined as the probability of a hypothesis test of finding an effect, if there is an effect to be found?\n",
    "* [ ] significance\n",
    "* [ ] alpha\n",
    "* [ ] power\n",
    "* [ ] confidence\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "You create a decision tree to show whether someone decides to go to the beach. There are three factors in this decision: rainy, overcast and sunny. What are these three factors called?\n",
    "* [ ] predictors\n",
    "* [ ] tree nodes\n",
    "* [ ] deciders\n",
    "* [ ] root nodes\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "You are part of a data science team that is working for a national fast-food chain. You create a simple report that shows a trend: Customers who visit the store more often and buy smaller meals spend more than customers who visit less frequently and buy larger meals. What is the most likely diagram that your team created?\n",
    "\n",
    "* [ ] multiclass classification diagram\n",
    "* [ ] linear regression and scatter plots\n",
    "* [ ] K-means cluster diagram\n",
    "* [ ] pivot table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
