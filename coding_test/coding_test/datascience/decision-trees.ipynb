{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "Train, regularize, vizualize and make predictions with Decision Trees on regression task. \n",
    "\n",
    "> Decision Trees require little data preparation, they don't require feature scaling or centering at all.\n",
    "\n",
    "Scikit-Learn uses the CART algorithm which produces only binary trees. Other algorithms such as ID3 can produce Decision Trees with nodes that have more than two children.\n",
    "\n",
    "> Decision Trees are intuitive, their decisions are easy to interpret. Such models are often called white box models. In contrast, Random Forests or neural network are generally considered black box models.\n",
    "\n",
    "Decision Trees provide a simple classification rules that can be applied manually.\n",
    "\n",
    "However, they do have a few limitations. First, Decision Trees love orthogonal decision boundaries, all splits are perpendicular to an axis, which makes them sensitive to training set rotation. One way to limit this problem is to use PCA which often results in a better orientation of the training data.\n",
    "\n",
    "More generally, the main issue is that they are very sensitive to small variation in the training data. Actually, since the training algorithm used by Scikit-Learn is stochastic (randomly selects the set of features to evaluate at each node), you may get very different models even on the same training data, unless you set the `random_state` hyperparameter.\n",
    "\n",
    "Random Forests can limit this instability by averaging predictions over many trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(max_depth=2)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, 2:] # petal length and width\n",
    "y = iris.target\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
    "tree_clf.fit(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vizualize the trained Decision Tree using `export_graphviz()` method to output a graph definition file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(tree_clf, \n",
    "                out_file=\"iris_tree.dot\",\n",
    "                feature_names=iris.feature_names[2:],\n",
    "                class_names=iris.target_names,\n",
    "                rounded=True,\n",
    "                filled=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Graphviz package to convert from .dot file to .png:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "! dot -Tpng iris_tree.dot -o iris_tree.png"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A node's gini attribute measures its impurity\n",
    "* if gini=0, the node is pure, all training instances belong to the same class\n",
    "\n",
    "To compute the gini score $G_i$ of the $i^{th}$ note:\n",
    "$$\n",
    "G_i = 1 - \\sum_{k=1}^n p_{i,k}^2\n",
    "$$\n",
    "\n",
    "* $p_{i,k}$ is the ratio of class $k$ instances among the training instances in the $i^{th}$ node"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating Class Probabilities\n",
    "\n",
    "A Decision Tree can estimate the probability that an instance belongs to a particular class k.\n",
    "\n",
    "First it traverses the tree to find the leaf node for this instance, and then it returns the ratio fo training instances of class k in this node.\n",
    "\n",
    "The estimated probabilities would be identical anywhere else in the bottom-right leaf node, if the petals were 6cm long and 1.5cm wide (or 5cm long and 1.5cm wide), even though it seems obvious that it would most likely be an Iris virginica in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict_proba([[5,1.5]])\n",
    "\n",
    "tree_clf.predict([[5,1.5]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CART Training Algorithm\n",
    "Scikit-Learn uses the Classification and Regression Tree (CART) algorithm to train Decision Trees (also called \"growing\" trees). The algorithm works by first splitting the training set into two subsets using a single feature k and threshold $t_k$ (eg petal length <= 2.45cm). How does it choose $k$ and $t_k$? It searches for the pair $(k, t_k)$ that produces the purest subsets, weighted by their size.\n",
    "\n",
    "$$\n",
    "J(k, t_k) = { m_{left} \\over m } G_{left} + { m_{right} \\over m } G_{right}\n",
    "$$\n",
    "\n",
    "where:\n",
    "* $G_{left/right}$ measures the impurity of the left/right subset,\n",
    "* $m_{left/right}$ is the number of instances in the left/right subset.\n",
    "\n",
    "Once the CART algorithm has successfully split the training set in two, it splits the subsets using the same logic, then the sub-subsets, and so on, recursively. It stops recursing once it reaches the maximum depth, defined by the `max_depth` hyperparameters, or if it cannot find a split that will reduce impurity.\n",
    "\n",
    "The CART algorithm is a greedy algorithm, it greedily searches for an optimum split at the top level, then repeats the process at each subsequent level. It does not check wether or not the split will lead to the lowest possible impurity several levels down. A greedy algorithm often produces a solution that's reasonably good but not guaranted to be optimal.\n",
    "\n",
    "Finding the optimal tree is known to be an $NP-Complete$ problem: it requires O(exp(m)) time, making the problem intractable even for small training sets.\n",
    "\n",
    "## Computational Complexity\n",
    "\n",
    "Making prediction requires traversing the Decision Tree from the root to a leaf. Decision Trees generally are approximately balanced, so traversing the DT requires going through roughly O(log(m)) nodes. Since each node only requires checking the value of one feature, the overall prediction complexity is 0(log(m)), independent of the number of features.\n",
    "\n",
    "The training algorithm compares all features (or less if max_features is set) on all samples at each node. Comparing all features on all samples at each node results in a training complexity of O(n m log(m)). For small training sets, less than a few thousand instances, Scikit-Learn can speed up training by presorting the data, (set presort=True), but doing that slows down considerably for larger training set.\n",
    "\n",
    "## Gini Impurity or Entropy?\n",
    "\n",
    "In Machine Learning entropy is frequently used as an impurity measure: a set's entropy is zero when it contains instances of only one class.\n",
    "\n",
    "$$\n",
    "H_i = - \\sum_{k=1}^n p_{i,k} \\log_2 (p_{i,k})\n",
    "$$\n",
    "\n",
    "with $p_{i,k} \\ne 0$, for the $i^{th}$ node\n",
    "\n",
    "Giny impurity is slightly faster to compute, so it is a good default. Generaly they lead to similar trees, but when they differ, Gini impurity tends to isolate the most frequent class in its own branch of the tree, while entropy tends to produce slightly more balanced trees.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization Hyperparameters\n",
    "\n",
    "Decision Trees make very few assumptions about the training data, as opposed to linear models, which assume that the data is linear for example. If left unconstrained the tree structure will adapt itself to the training data, fitting it very closelt, most likely overfitting it.\n",
    "\n",
    "Such a model is called a **nonparametric model** not because it does not have any parameters, but because the number of paraemters is not determined prior to training, so the model structure is free to stick closely to the data.\n",
    "\n",
    "In contrast **parametric model** such as linear model, has a predetermined number of parameters, so its degree of freedom is limiter, reducing the risk of overfitting, but increasing the risk of underfitting.\n",
    "\n",
    "To avoid overfitting the training data, you need to restrict the Decision Tree's freedom during training. The regularization hyperparameters depend on the algorithm usedn but generally you can at least restrict the maximum depth of the Decision Tree. In Scikit-learn, this is controlled by the `max_depth` hyperparameter, the default is `None` which means unlimited. Reducing `max_depth` will regularize the model and thus reduce the risk of overfitting.\n",
    "\n",
    "The `DecisionTreeClassifier` class has a few other parameters that similarly restrict the shape of the Decision Tree:\n",
    "* `min_saples_split`: the minimum number of samples a node must have before it can be split\n",
    "* `min_sample_leaf`: the minimum number of samples a leaf node must have\n",
    "* `min_weight_fraction_leaf`: fraction of the total number of weighted instances\n",
    "* `max_leaf_nodes`: the maximum number of leaf nodes\n",
    "* `max_features`: the maximum number of features that are evaluated for splitting at each node\n",
    "\n",
    "Increasing `min_*` hyperparameters or reducing `max_*` hyperparameters will regularize the model.\n",
    "\n",
    "> Other algorithms work by first training the Decision Tree without restrictions, then pruning (deleting) unnecessary nodes. A node whose children are all leaf nodes is considered unnecessary if the purity improvement it provides is not statistically significance.\n",
    "> \n",
    "> Standard statistical tests, such as $\\chi^2$ test, are used to estimate the probability that the improvement is purely the result of chance, which is called the null hypothesis. If this probability, called p-value, is higher than a given threshold, typically 5%, controlled by a hyperparameter, then the node is considered unnecessary and its children are deleted. The pruning continus until all unnecessary nodes have been pruned."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating qualitative variable\n",
    "\n",
    "Decision Trees are also capable of performing regression tasks.\n",
    "\n",
    "The CART algorithm works mostly the same way as earlier, except that instead of trying to split the training set in a way that minimizes impurity, it now tries to split the training set in a way that minimizes the MSE.\n",
    "\n",
    "$$\n",
    "J(k, t_k) = { m_{left} \\over m } MSE_{left} + { m_{right} \\over m } MSE_{right}\n",
    "$$\n",
    "\n",
    "where:\n",
    "* $MSE_{node} = \\sum_{i \\in node} (\\hat{y}_{node} -y^{(i)})^2$\n",
    "* $ \\hat{y}_{node} = { 1 \\over m_{node} }  \\sum_{i \\in node} y^{(i)}$\n",
    "\n",
    "Like for classification, Decision Trees are prone to overfitting when dealing with regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg.fit(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
