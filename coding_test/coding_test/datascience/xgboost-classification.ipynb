{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest consists of a large number of decision trees and works as an ensemble technique.\n",
    "\n",
    "Random Forests works on the principle of bagging, wherein each decision tree is build on a sample of the training data set with replacement. The results from these multiple decision trees are then aggregated to come up with the final forecast. For the purposes of classification, the mode of all the predictions is used, and for regression the mean of all predicitions is deemed as the final output."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Random Forests works well with both categorical and numerical data. No scaling or transformation of variables is usually necessary.\n",
    "* Random Forests implicitly perform feature selection and generate uncorrelated decision trees. It does this by choosing a random set of features to build each decision tree. This also makes it a great model when you have to work with a high number of features in the data.\n",
    "* Random Forests are not influenced by outliers to a fair degree. It does this by binning the variables.\n",
    "* Random Forests can handle linear and non-linear relationships well.\n",
    "* Random Forests generally provide a high accuracy and balance the bias-variance trade off well. Since the model's principle is to average the results across the multiple decision trees it builds, it averages the variance as well."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Random Forests are great in a number of applications, there are certain places where they may not be an ideal choice. For instance:\n",
    "* Random Forests aren't easily interpretable. Although they provide feature importance, they do not provide complete visibility into the coefficients like a linear regression does.\n",
    "* Random Forests can be computationally intensive for large datasets.\n",
    "\n",
    "XGBoost improves upon the capabilities that Random Forest has by making use of the gradient descent framework. It also has the ability to build trees in parallel and optimizes hardware as it does so. XGBoost has the in-build capability to penalize complex models by using regularization techniques. It also comes with in-built cross validation that can be used to determine the number of boosting iterations required in a run."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "While it might be more comfortable to use the Sklearn API at first, later, you’ll realize that the native API of XGBoost contains some excellent features that the former doesn’t support.\n",
    "\n",
    "Diamonds dataset throughout the tutorial. It is built into the Seaborn library. It has a nice combination of numeric and categorical features and over 50k observations that we can comfortably showcase all the advantages of XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'posix'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "diamonds = sns.load_dataset(\"diamonds\")\n",
    "diamonds.shape\n",
    "diamonds.head()\n",
    "diamonds.describe()\n",
    "diamonds.describe(exclude=np.number)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two most popular classification objectives are\n",
    "\n",
    "* binary:logistic - binary classification (the target contains only two classes, i.e., cat or dog)\n",
    "* multi:softprob - multi-class classification (more than two classes in the target, i.e., apple/orange/banana)\n",
    "\n",
    "Performing binary and multi-class classification in XGBoost is almost identical, so we will go with the latter\n",
    "\n",
    "We want to predict the cut quality of diamonds given their price and their physical measurements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "X, y = diamonds.drop(\"cut\", axis=1), diamonds[['cut']]\n",
    "\n",
    "# Encode y to numeric\n",
    "y_encoded = OrdinalEncoder().fit_transform(y)\n",
    "\n",
    "# Extract text features\n",
    "cats = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "# Convert to pd.Categorical\n",
    "for col in cats:\n",
    "   X[col] = X[col].astype('category')\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, random_state=1, stratify=y_encoded)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost DMatrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classification matrices\n",
    "dtrain_clf = xgb.DMatrix(X_train, y_train, enable_categorical=True)\n",
    "dtest_clf = xgb.DMatrix(X_test, y_test, enable_categorical=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Classification\n",
    "\n",
    "During cross-validation, we are asking XGBoost to watch three classification metrics which report model performance from three different angles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"objective\": \"multi:softprob\", \"tree_method\": \"gpu_hist\", \"num_class\": 5}\n",
    "n = 1000\n",
    "\n",
    "results = xgb.cv(\n",
    "   params, dtrain_clf,\n",
    "   num_boost_round=n,\n",
    "   nfold=5,\n",
    "   metrics=[\"mlogloss\", \"auc\", \"merror\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.keys()\n",
    "\n",
    "# the best AUC score, we take the maximum of test-auc-mean column:\n",
    "results['test-auc-mean'].max()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "During the boosting rounds, the model object has learned all the patterns of the training set it possibly can. Now, we must measure its performance by testing it on unseen data. That's where our `dtest_reg` DMatrix comes into play:\n",
    "\n",
    "This step of the process is called model evaluation (or inference). Once you generate predictions with predict, you pass them inside mean_squared_error function of Sklearn to compare against y_test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "preds = model.predict(dtest_reg)\n",
    "\n",
    "rmse = mean_squared_error(y_test, preds, squared=False)\n",
    "\n",
    "print(f\"RMSE of the base model: {rmse:.3f}\")\n",
    "# RMSE of the base model: 543.203"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
