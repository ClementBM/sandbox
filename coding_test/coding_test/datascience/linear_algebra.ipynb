{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orthogonal Matrix\n",
    "\n",
    "An orthogonal matric is a square matric whose rows are mutually orthonormal and whose columns are mututally orthonormal\n",
    "\n",
    "$$\n",
    "A^{\\text{T}}A = AA^{\\text{T}} = I\n",
    "$$\n",
    "\n",
    "This implies that\n",
    "\n",
    "$$\n",
    "A^{\\text{-1}}=A^{\\text{T}}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigen Decomposition\n",
    "\n",
    "An eigenvector of a square matric A is a nonzero vector $v$ such that multiplication by $A$ alters only the scale of $v$\n",
    "\n",
    "$$\n",
    "Av = \\lambda v\n",
    "$$\n",
    "\n",
    "The scalar $\\lambda$ is known as the eigenvalue corresponding to this eigenvector.\n",
    "\n",
    "$$\n",
    "A = V diag(\\lambda) V^{-1}\n",
    "$$\n",
    "\n",
    "Not every matrix can be decomposed into eigenvalues and eigenvectors. In some cases, the decompositio exists but involves complex rather than real number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition\n",
    "\n",
    "The SVD provides another way to factorize a matrix, into singular vectors and singular values. Every real matrix has a singular value decomposition, but the same is not true of the eigenvalue decomposition. For example, if a matrix is not square, the eigendecomposition is not defined.\n",
    "\n",
    "$$\n",
    "A = U D V^{\\text{T}}\n",
    "$$\n",
    "\n",
    "* A is $m \\times n$\n",
    "* U is $m \\times m$, is orthogonal, columns of $U$ are left-singular vectors\n",
    "* D is $m \\times n$, is diagonal, not necessarily square\n",
    "* V is $n \\times n$, is orthogonal, columns of $V$ are right-singular vectors\n",
    "\n",
    "The left-singular vectors of A are the eigenvectors of $AA^{\\text{T}}$\n",
    "\n",
    "The right-singular vectors of A are the eigenvectors of $A^{\\text{T}}A$\n",
    "\n",
    "The nonzero singular values of A are the square roots of the eigenvalues of $A^{\\text{T}}A$ and $AA^{\\text{T}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Moore-Penrose Pseudoinverse\n",
    "\n",
    "$$\n",
    "Ax = y\\\\\n",
    "BAx = By\\\\\n",
    "x = By\n",
    "$$\n",
    "\n",
    "Depending on the structure of the problem, it may not be possible to design a unique mapping from A to B\n",
    "* if A is taller than it is wide: it is possible for this equation to have no solution\n",
    "* if A is wider than it is tall: it is possible for this equation to have multiple solutions\n",
    "\n",
    "The pseudoinverse of A is defined as:\n",
    "$$\n",
    "A^{\\text{+}} = \\lim_{\\alpha \\rightarrow 0}(A^{\\text{T}}A + \\alpha I)^{\\text{-1}} A^{\\text{T}}\n",
    "$$\n",
    "\n",
    "Practical algorithms written in the form\n",
    "$$\n",
    "A^{\\text{+}} = V D^{\\text{+}} U^{\\text{T}}\n",
    "$$\n",
    "\n",
    "TODO ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The determinant\n",
    "\n",
    "The determinant of a square matrix, $det(A)$ is a function that maps matrics to real scalars.\n",
    "\n",
    "The determinant is equal to the product of all the eigenvalues of the matrix.\n",
    "\n",
    "The absolute value of the determinant can be thought of as a measure of how much multiplication by the matrix expands or contracts space.\n",
    "\n",
    "* If the determinant is 0, then space is contracted completely along at least one dimension, causing it to lose all its volume\n",
    "* if the determinant is 1, then the transformation preserves volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Components Analysis\n",
    "\n",
    "Suppose we have a collection of $m$ points $\\{ x^{(1)}, ..., x^{(m)} \\}$ in $\\mathcal{R}^n$, we want to apply lossy compression to these points. Lossy compression means storing the points in a way that requires less memory but may lose some precision. \n",
    "\n",
    "$$\n",
    "f(x) = c\\\\\n",
    "x = g(f(x))\n",
    "$$\n",
    "\n",
    "For each point $x^{(i)} \\in \\mathcal{R}^n$, find a corresponding code vector $c^{(i)} \\in \\mathcal{R}^l$, with $l \\lt n$\n",
    "\n",
    "Specificaly with PCA, we choose to use matrix multiplication\n",
    "\n",
    "$$\n",
    "g(c) = Dc\n",
    "$$\n",
    "where $D \\in \\mathcal{R}^{n \\times l}$\n",
    "\n",
    "To make it easy, PCA constraints the columns of D to be orthogonal to each other, and constrain all columns of D to have unit norm.\n",
    "\n",
    "TODO ...."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
