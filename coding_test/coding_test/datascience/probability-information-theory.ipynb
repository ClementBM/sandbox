{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability and Information Theory\n",
    "\n",
    "First, the laws of probability tell us how AI systems should reason, se we design our algorithms to compute or approximate various expressions derived using probability theory.\n",
    "\n",
    "Second, we can use probability and statistics to theorically analyze the behavior of proposed AI systems."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability\n",
    "Machine learning must always deal with uncertain quantities and sometimes stochastic quantities.\n",
    "\n",
    "There are three possible sources of uncertainty:\n",
    "* Inherent stochasticity in the system being modeled. For instance, most interpretations of quantum mechanics describe the dynamics of subatomic particles as beign probabilistic.\n",
    "* Incomplete observability. Even deterministic systems can appear stochastic when we cannot observe all the variables that drive the behavior of the system.\n",
    "* Incomplete modeling. When we use a model that must discard some of the information we have observed, the discarded information results in uncertainty in the model's prediction.\n",
    "\n",
    "While it should be clear that we need a means of representing and reasoning about uncertainty, it is not immediately obvious that probability theory can provide all the tools we want for AI applications. Probability theory was orginally developed to analyze the frequencies of (repeatable) events. However in the case of a doctor diagnosing the patient, we use probability to represent a **degree of belief**.\n",
    "\n",
    "The probability direcly related to the rates at which events occur, is known as **Frequentist probability**, while the one related to qualitative levels of certainty is known as **Bayesian probability**.\n",
    "\n",
    "Probability can be seens as the extension of logic to deal with uncertainty. Logic provides a set of formal rules for determining what propositions are implied to be true or false given the assumption that some other set of propositions is true or false. Probability theory provides a set of formal rules for determining the likelihood of a proposition being true given the likelihood of other propositions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability distributions\n",
    "\n",
    "A probability distribution is a description of how likely a random variable or set of random variables is to take on each of its possible states.\n",
    "A random variable is a variable that can take on different value randomly.\n",
    "\n",
    "### Discrete Variables and Probability Mass Functions\n",
    "The probability mass function (PMF) maps from a state of a random variable to the probability of that random variable taking on that state.\n",
    "\n",
    "The probability that $\\text{x} = x $ is denoted as $P(x)$ or $P(\\text{x} = x)$ or $\\text{x} \\sim P(x)$\n",
    "\n",
    "Probability mass functions can act on many variabes at the same time. They're called **joint probability distribution**.\n",
    "$P(\\text{x}=x,\\text{y}=y)$ denotes the probability that $\\text{x}=x$ and $\\text{y}=y$ simultaneously.\n",
    "\n",
    "To be a PMF on a random variable x, a function $P$ must satisfy the following properties:\n",
    "* the domain of $P$ must be the set of all possible states of x\n",
    "* $\\forall x \\in, 0 \\le P(x) \\le 1$. An impossible event has a probability 0, and no state can be less probable than that. Likewise, an event that is garanted to happen has a probability of 1, and no state can have a greater chance of occuring.\n",
    "* $\\sum_{x \\in \\text{x}} P(x) =1$. We refer to this property as being **normalized**. Without this property, we could obtain probabilities greater than one by computing the probability of one of many events occurring.\n",
    "\n",
    "### Continuous Variables and Probability Density Functions\n",
    "\n",
    "To be a probability density function (PDF), a function $p$ must satisfy the following properties:\n",
    "* The domain of $p$ must be the set of all possible states of x\n",
    "* $\\forall x \\in \\text{x}, p(x) \\ge 0$. Note that we do not require $p(x) \\le 1$\n",
    "* $\\int p(x) dx = 1$\n",
    "\n",
    "A probability density function $p(x)$ does not give the probability of a specific state directly, instead the probability of landing an infinitesimal region with volume $\\delta x$ is given by $\\int_{[a,b]} p(x)dx$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marginal Probability\n",
    "\n",
    "Sometimes we know the probability distribution over a set of variables and we want to know the probability distribution over just a subset of them. The probability distribution over the subset is known as the **marginal probability distribution**.\n",
    "\n",
    "For example, suppose we have discrete random variables x and y, and we know $P(\\text{x,y})$. We can find $P(\\text{x})$ with the **sum rule**:\n",
    "\n",
    "$$\n",
    "\\forall x \\in \\text{x}, P(\\text{x},x) = \\sum_y P(\\text{x}=x, \\text{y}=y)\n",
    "$$\n",
    "\n",
    "THe name **marginal probability** comes from the process of computing marginal probabilities on paper. WHen the values of $P(\\text{x,y})$ are written in a grid with different values of $x$ in rows and different values of $y$ in columns, it is natural to sum across a row of the grid, then write $P(x)$ in the margin of the paper just to the right of the row.\n",
    "\n",
    "For continuous variables, we need to use integration instead of summation:\n",
    "\n",
    "$$\n",
    "p(x) = \\int p(x,y)dy\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([1, np.nan])\n",
    "np.array_equal(a,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.empty([3,4])\n",
    "\n",
    "np.array_equal(np.empty([3,4]),np.empty([3,4]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
